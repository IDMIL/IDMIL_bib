
@book{burtner,
	title = {Metasaxophone {Systems}: {Evolving} {New} {Music} and {Technologies} for the {Saxophone}},
	url = {https://ccrma.stanford.edu/ mburtner/metasax.html#Overview},
	language = {English},
	urldate = {2019-12-05},
	author = {Burtner, Matthew},
}

@inproceedings{bowers2005,
	address = {Vancouver, BC, Canada},
	title = {Not {Hyper}, {Not} {Meta}, {Not} {Cyber} but {Infra}-{Instruments}},
	url = {https://www.nime.org/proceedings/2005/nime2005_005.pdf},
	language = {English},
	urldate = {2020-04-20},
	author = {Bowers, John and Archer, Phil},
	year = {2005},
	pages = {5--10},
	file = {Full Text:/Users/Vincent/Zotero/storage/LAXTIIID/Bowers and Archer - 2005 - Not Hyper, Not Meta, Not Cyber but Infra-Instrumen.pdf:application/pdf},
}

@inproceedings{machover1989,
	address = {San Francisco, CA},
	title = {Hyperinstruments: {Musically} {Intelligent} and {Interactive} {Performance} and {Creativity} {Systems}},
	language = {English},
	author = {Machover, Tod and Chung, Joe},
	year = {1989},
	pages = {186--190},
}

@article{burtner2002,
	title = {The {Metasaxophone}: {Concept}, implementation, and mapping strategies for a new computer music instrument},
	volume = {7},
	issn = {1355-7718; 1469-8153 (electronic)},
	url = {https://proxy.library.mcgill.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=rft&AN=A110582&scope=site},
	abstract = {The Metasaxophone is an acoustic tenor saxophone retrofitted with an onboard computer microprocessor and an array of sensors that convert performance data into MIDI control messages. The instrument has additionally been outfitted with a unique microphone system that allows for detailed control of the amplified sound. While maintaining full acoustic functionality it is also a versatile MIDI controller and an electric instrument. A primary motivation behind the Metasaxophone is to put signal processing under direct expressive control of the performer. Through the combination of gestural and audio performance control, employing both discrete and continuous multilayered mapping strategies, the Metasaxophone can be adapted for a wide range of musical purposes. The artistic and technical development of the instrument, as well as new conceptions of musical mappings arising from the enhanced interface, are explored.},
	number = {2},
	journal = {Organised sound: An international journal of music technology. VII/2 (August 2002): Mapping strategies in realtime computer music},
	author = {Burtner, Matthew},
	month = aug,
	year = {2002},
	keywords = {48: Sound sources – Electrophones (synthesized sound), electronic sound generation – mapping strategies – Metasaxophone, instruments—wind (woodwind, reed family) – saxophone – Metasaxophone},
	pages = {201--213},
	annote = {Accession Number: 2002-04233. Record Type: Main Record Document Type: Article in a periodical. Physical Medium: print; online. Pagination: 201-213. Includes: illustrations, bibliography, charts, diagrams, sound recording. Language: English. Language of Summary: English.},
	annote = {Place: Cambridge Publisher: Cambridge University Press},
}

@inproceedings{palacio-quintin2003,
	address = {Montreal, Canada},
	title = {The {Hyper}-{Flute}},
	language = {English},
	booktitle = {Proceedings of the 2003 {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Palacio-Quintin, Cléo},
	year = {2003},
	pages = {206--207},
}

@inproceedings{aska2014,
	title = {The {Black} {Swan}: {Probable} and {Improbable} {Communcation} {Over} {Local} and {Geographically} {Displaced} {Networked} {Connections} as a {Musical} {Performance} {System}},
	shorttitle = {The {Black} {Swan}},
	abstract = {The Black Swan is a networked performance system for two groups of non-specific performers. The work derives its title and inspiration from Nicolas Taleb's description of extreme and catastrophic events. These “black swan” events are characterized as being outliers, unpredictable, and yet completely explainable when viewed in retrospect. The Black Swan uses this concept in performance; throughout the piece a group of instrumentalists is solely responsible for interpreting the score while a group of motion-tracked performers advance the score. However, when the “Black Swan” occurs, the motion-tracked group begins to generate sound, an event that the instrumentalists could not have anticipated. A third party is responsible for distributing instructions to each performance group over the network during the performance. Therefore, The Black Swan explores the way networked performers communicate with each other as well as the dramaturgy between ensemble members in a networked setting.},
	booktitle = {{ICMC}},
	author = {Aska, A.},
	year = {2014},
}

@book{aska2016,
	title = {{IMPROVISATION} {AND} {GESTURE} {AS} {FORM} {DETERMINANTS} {IN} {WORKS} {WITH} {ELECTRONICS}},
	abstract = {This paper examines several examples that use electronics as form determinants in works with some degree of structured improvisation. Three works created by the author are discussed, each of which uses gestural controller input to realize an indeterminate form in some way. The application of such principles and systems to venues such as networked performance is explored. While each of these works contains an improvisatory and/or aleatoric element, much of their content is composed, which brings the role of the composer into question. The “improviser”, who in these works advances the work temporally and determines the overall form, is actually taking on the more familiar role of the conductor. Therefore, these works also bring up important conversation topics regarding performance practice in works that contain electronics and how they are realized.},
	language = {en},
	author = {Aska, A.},
	year = {2016},
	annote = {Published: /paper/IMPROVISATION-AND-GESTURE-AS-FORM-DETERMINANTS-IN-Aska/4ab44cb441fb197366b6e97b2e365caff5e71e46},
}

@book{aspromallis2016,
	title = {Form-{Aware}, {Real}-{Time} {Adaptive} {Music} {Generation} for {Interactive} {Experiences} {Data}},
	abstract = {This archive contains data in support of the Sound and Music Computing 2016 paper: “FORM-AWARE, REAL-TIME ADAPTIVE MUSIC GENERATION FOR INTERACTIVE EXPERIENCES”.},
	language = {en},
	author = {Aspromallis, C. and Gold, Ne},
	year = {2016},
	annote = {Publication Title: undefined Published: /paper/Form-Aware\%2C-Real-Time-Adaptive-Music-Generation-for-Aspromallis-Gold/9e6c2ee5e224956790063e4e94829e42d83caaab},
}

@inproceedings{assayag2010,
	title = {Interaction with {Machine} {Improvisation}},
	doi = {10.1007/978-3-642-12337-5_10},
	abstract = {We describe two multi-agent architectures for an improvisation oriented musician-machine interaction systems that learn in real time from human performers. The improvisation kernel is based on sequence modeling and statistical learning. We present two frameworks of interaction with this kernel. In the first, the stylistic interaction is guided by a human operator in front of an interactive computer environment. In the second framework, the stylistic interaction is delegated to machine intelligence and therefore, knowledge propagation and decision are taken care of by the computer alone. The first framework involves a hybrid architecture using two popular composition/performance environments, Max and OpenMusic, that are put to work and communicate together, each one handling the process at a different time/memory scale. The second framework shares the same representational schemes with the first but uses an Active Learning architecture based on collaborative, competitive and memory-based learning to handle stylistic interactions. Both systems are capable of processing real-time audio/video as well as MIDI. After discussing the general cognitive background of improvisation practices, the statistical modelling tools and the concurrent agent architecture are presented. Then, an Active Learning scheme is described and considered in terms of using different improvisation regimes for improvisation planning. Finally, we provide more details about the different system implementations and describe several performances with the system.},
	booktitle = {The {Structure} of {Style}},
	author = {Assayag, G. and Bloch, G. and Cont, A. and Dubnov, S.},
	year = {2010},
	file = {Full Text:/Users/Vincent/Zotero/storage/46EM3K5S/Assayag et al. - 2010 - Interaction with Machine Improvisation.pdf:application/pdf},
}

@inproceedings{caetano2017,
	title = {A {Sensor}-{Augmented} {Saxophone} {Mouthpiece} for {Unveiling} the {Mechanics} of {Saxophone} {Tone} {Formation}},
	abstract = {This paper presents a sensor-augmented saxophone mouthpiece which promotes data collection from musicians. The collected data aims to unveil the mechanics of saxophone tone formation from the em-bouchure and air flow control— for which only limited knowledge exists due to its occlusion concealed by the players' mouth. Ultimately, by grasping and modeling how musicians play, we aim to develop intelligent music tutoring systems. Towards this aim, we provide the first steps of a sensor-augmented mouthpiece from which we compute air pressure, air flow, and mouthpiece-reed tip opening indicators of a performance. Di\${\textbackslash}textbackslashcarriagereturn\$erent strategies to capture these indicators have been studied under controlled lab conditions as a basis to develop a prototype mouthpiece which is evaluated by an expert saxophonist.},
	author = {Caetano, Gustavo and Bernardes, Gilberto and Twillert, Henk and Pais Clemente, Miguel and Mendes, Joaquim},
	month = sep,
	year = {2017},
}

@inproceedings{carey2012,
	address = {Ann Arbor, Michigan},
	title = {Designing for {Cumulative} {Interactivity}: {The} \_derivations {System}},
	doi = {10.5281/zenodo.1178227},
	abstract = {This paper presents the author's derivations system, an interactive performance system for solo improvising instrumentalist. The system makes use of a combination of real-time audio analysis, live sampling and spectral re-synthesis to build a vocabulary of possible performative responses to live instrumental input throughout an improvisatory performance. A form of timbral matching is employed to form a link between the live performer and an expanding database of musical materials. In addition, the system takes into account the unique nature of the rehearsal/practice space in musical performance through the implementation of performer-configurable cumulative rehearsal databases into the final design. This paper discusses the system in detail with reference to related work in the field, making specific reference to the system's interactive potential both inside and outside of a real-time performance context.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {University of Michigan},
	author = {Carey, Benjamin},
	year = {2012},
	keywords = {improvisation, Interactivity, performance systems},
	annote = {ISSN: 2220-4806},
	file = {Full Text:/Users/Vincent/Zotero/storage/D4R2BDE5/Carey - 2012 - Designing for Cumulative Interactivity The _deriv.pdf:application/pdf},
}

@inproceedings{carey2013,
	address = {Sydney Australia},
	title = {\_derivations: {Improvisation} for {Tenor} {Saxophone} and {Interactive} {Performance} {System}},
	isbn = {978-1-4503-2150-1},
	shorttitle = {\_derivations},
	doi = {10.1145/2466627.2481226},
	language = {en},
	booktitle = {Proceedings of the 9th {ACM} {Conference} on {Creativity} \& {Cognition}},
	publisher = {ACM},
	author = {Carey, Benjamin},
	month = jun,
	year = {2013},
	pages = {411--412},
}

@book{carey2016,
	title = {\_derivations and the {Performer}-{Developer} : {Co}-{Evolving} {Digital} {Artefacts} and {Human}-{Machine} {Performance} {Practices}},
	shorttitle = {\_derivations and the {Performer}-{Developer}},
	abstract = {.................................................................................................................. ix Introduction .......................................................................................... 1 Chapter 1. 1.},
	language = {en},
	author = {Carey, Benjamin Leigh},
	year = {2016},
	annote = {Publication Title: undefined Published: /paper/\_derivations-and-the-performer-developer-\%3A-digital-Carey/4dbe784f3384739738b29191e0159e6fe98e6b0b},
}

@article{ciufo2005,
	title = {Beginner's {Mind}: {An} {Environment} for {Sonic} {Improvisation}},
	abstract = {Beginner's Mind is the newest work in the ongoing Sonic Improvisation Series. This performance system is a combination of software and hardware designed for real-time sonic exploration. This paper will describe the primary design strategies employed in this system, and discuss key aesthetic concerns.},
	language = {en},
	author = {Ciufo, Thomas},
	year = {2005},
	pages = {4},
}

@book{davis2013,
	title = {Human-{Computer} {Co}-{Creativity}: {Blending} {Human} and {Computational} {Creativity}},
	shorttitle = {Human-{Computer} {Co}-{Creativity}},
	abstract = {This paper describes a thesis exploring how computer programs can collaborate as equals in the artistic creative process. The proposed system, CoCo Sketch, encodes some rudimentary stylistic rules of abstract sketching and music theory to contribute supplemental lines and music while the user sketches. We describe a three-part research method that includes defining rudimentary stylistic rules for abstract line drawing, exploring the interaction design for artistic improvisation with a computer, and evaluating how CoCo Sketch affects the artistic creative process. We report on the initial results of early investigations into artistic style that describe cognitive, perceptual, and behavioral processes used in abstract artists making.},
	language = {en},
	author = {Davis, N.},
	year = {2013},
	annote = {Publication Title: undefined Published: /paper/Human-Computer-Co-Creativity\%3A-Blending-Human-and-Davis/565b4abb93a932a400c6263dc0e805b6e9e620b0},
}

@article{dudas2010,
	title = {Comprovisation: {The} {Various} {Facets} of {Composed} {Improvisation} within {Interactive} {Performance} {Systems}},
	shorttitle = {Comprovisation},
	doi = {10.1162/LMJ_a_00009},
	abstract = {ABSTRACT This article discusses the balance between composition and improvisation with respect to interactive performance using electronic and computer-based music systems. The author uses his own experience in this domain in the roles of both collaborator and composer as a point of reference to look at general trends in composed improvisation within the electronic and computer music community. Specifically, the intention is to uncover the limits and limitations of improvisation and its relationship to both composition and composed instruments within the world of interactive electronic musical performance.},
	journal = {Leonardo Music Journal},
	author = {Dudas, R.},
	year = {2010},
}

@article{eerola2018,
	title = {Shared {Periodic} {Performer} {Movements} {Coordinate} {Interactions} in {Duo} {Improvisations}},
	doi = {10.1098/rsos.171520},
	abstract = {Human interaction involves the exchange of temporally coordinated, multimodal cues. Our work focused on interaction in the visual domain, using music performance as a case for analysis due to its temporally diverse and hierarchical structures. We made use of two improvising duo datasets— (i) performances of a jazz standard with a regular pulse and (ii) non-pulsed, free improvizations— to investigate whether human judgements of moments of interaction between co-performers are influenced by body movement coordination at multiple timescales. Bouts of interaction in the performances were manually annotated by experts and the performers' movements were quantified using computer vision techniques. The annotated interaction bouts were then predicted using several quantitative movement and audio features. Over 80\% of the interaction bouts were successfully predicted by a broadband measure of the energy of the cross-wavelet transform of the co-performers' movements in non-pulsed duos. A more complex model, with multiple predictors that captured more specific, interacting features of the movements, was needed to explain a significant amount of variance in the pulsed duos. The methods developed here have key implications for future work on measuring visual coordination in musical ensemble performances, and can be easily adapted to other musical contexts, ensemble types and traditions.},
	journal = {Royal Society Open Science},
	author = {Eerola, T. and Jakubowski, K. and Moran, Nikki and Keller, P. and Clayton, M.},
	year = {2018},
	file = {Full Text:/Users/Vincent/Zotero/storage/2RW6WKD4/Eerola et al. - 2018 - Shared Periodic Performer Movements Coordinate Int.pdf:application/pdf},
}

@inproceedings{flores2019,
	address = {Porto Alegre, Brazil},
	title = {{HypeSax}: {Saxophone} {Acoustic} {Augmentation}},
	doi = {10.5281/zenodo.3672996},
	abstract = {New interfaces allow performers to access new possibilities of musical expression. Even though interfaces are often designed to be adaptable to different software, most of them rely on external speakers or similar transducers. This often results on disembodiment and acoustic disengagement from the interface, and in the case of augmented instruments, from the instruments themselves. This paper describes a project in which a hybrid system allows an acoustic integration between the sound of acoustic saxophone and electronics.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {UFRGS},
	author = {Flores, Cristohper Ramos and Murphy, Jim and Norris, Michael},
	editor = {Queiroz, Marcelo and Sedó, Anna Xambó},
	month = jun,
	year = {2019},
	pages = {365--370},
	annote = {ISSN: 2220-4806},
	file = {Full Text:/Users/Vincent/Zotero/storage/F4PRFYSV/Flores et al. - 2019 - HypeSax Saxophone Acoustic Augmentation.pdf:application/pdf},
}

@article{herremans2017,
	title = {A {Functional} {Taxonomy} of {Music} {Generation} {Systems}},
	doi = {10.1145/3108242},
	abstract = {Digital advances have transformed the face of automatic music generation since its beginnings at the dawn of computing. Despite the many breakthroughs, issues such as the musical tasks targeted by different machines and the degree to which they succeed remain open questions. We present a functional taxonomy for music generation systems with reference to existing systems. The taxonomy organizes systems according to the purposes for which they were designed. It also reveals the inter-relatedness amongst the systems. This design-centered approach contrasts with predominant methods-based surveys and facilitates the identification of grand challenges to set the stage for new breakthroughs.},
	journal = {ACM Comput. Surv.},
	author = {Herremans, Dorien and Chuan, C. and Chew, Elaine},
	year = {2017},
	file = {Submitted Version:/Users/Vincent/Zotero/storage/4F5WIHS3/Herremans et al. - 2017 - A Functional Taxonomy of Music Generation Systems.pdf:application/pdf},
}

@inproceedings{hsu2006,
	address = {Paris, France},
	title = {Managing {Gesture} and {Timbre} for {Analysis} and {Instrument} {Control} in an {Interactive} {Environment}},
	doi = {10.5281/zenodo.1176927},
	abstract = {This paper describes recent enhancements in an interactive system designed to improvise with saxophonist John Butcher [1]. In addition to musical parameters such as pitch and loudness, our system is able to analyze timbral characteristics of the saxophone tone in real-time, and use timbral information to guide the generation of response material. We capture each saxophone gesture on the fly, extract a set of gestural and timbral contours, and store them in a repository. Improvising agents can consult the repository when generating responses. The gestural or timbral progression of a saxophone phrase can be remapped or transformed; this enables a variety of response material that also references audible contours of the original saxophone gestures. A single simple framework is used to manage gestural and timbral information extracted from analysis, and for expressive control of virtual instruments in a free improvisation context.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Hsu, William},
	year = {2006},
	keywords = {instrument control., Interactive music systems, timbre analysis},
	pages = {376--379},
	annote = {ISSN: 2220-4806},
	file = {Full Text:/Users/Vincent/Zotero/storage/7B9HDFJS/Hsu - 2006 - Managing Gesture and Timbre for Analysis and Instr.pdf:application/pdf},
}

@inproceedings{leeuw2009,
	address = {Pittsburgh, PA, United States},
	title = {The {Electrumpet} , a {Hybrid} {Electro}-{Acoustic} {Instrument}},
	doi = {10.5281/zenodo.1177613},
	abstract = {The Electrumpet is an enhancement of a normal trumpet with a variety of electronic sensors and buttons. It is a new hybrid instrument that facilitates simultaneous acoustic and electronic playing. The normal playing skills of a trumpet player apply to the new instrument. The placing of the buttons and sensors is not a hindrance to acoustic use of the instrument and they are conveniently located. The device can be easily attached to and detached from a normal Bb-trumpet. The device has a wireless connection with the computer through Bluetooth-serial (Arduino). Audio and data processing in the computer is effected by three separate instances of MAX/MSP connected through OSC (controller data) and Soundflower (sound data). The current prototype consists of 7 analogue sensors (4 valve-like potentiometers, 2 pressure sensors, 1 "Ribbon" controller) and 9 digital switches. An LCD screen that is controlled by a separate Arduino (mini) is attached to the trumpet and displays the current controller settings that are sent through a serial connection.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Leeuw, Hans},
	year = {2009},
	keywords = {Bluetooth, LCD, low latency, MAX/MSP., multiple Arduinos, OSC, Trumpet},
	pages = {193--198},
	annote = {ISSN: 2220-4806},
	file = {Full Text:/Users/Vincent/Zotero/storage/XTV7V3TB/Leeuw - 2009 - The Electrumpet , a Hybrid Electro-Acoustic Instru.pdf:application/pdf},
}

@book{lippe2002,
	title = {Real - {Time} {Interaction} {Among} {Composers}, {Performers}, and {Computer} {Systems}},
	abstract = {As the computer becomes more and more ubiquitous in society, the term “interactive” has become a widely used, but misunderstood term. In this paper, I discuss definitions of interactive music and interactive music systems, performance issues in interactive music, and performer/machine relationships that engender interaction in an attempt to explain how and why I pursue this discipline. Furthermore, I describe the function of computers in my compositions, and the manner in which I explore performer/machine interaction. Personal Background Prior to 1980, I was equally active as both an instrumental and electronic music composer. I was originally drawn to the computer because of a strong interest in designing new sounds, something at which computers are quite good, and exploring algorithmic compositional structures, simulation being another particular strength of computers. While new sounds and compositional algorithms can be readily explored in a non-real-time environment, and the tape and instrument paradigm has existed since the beginnings of electronic music, my first experiences with electronic music were with analog synthesizers, and my earliest experiences with computers in the 1970s were predominately with real-time systems. In addition, during the second half of the 1970s, I worked with an improvisational music/dance ensemble exploring live-electronics combined with acoustic instruments. For the past 20 years, I have pursued creative and research interests in interactive computer music involving live instrumentalists and computers in performance situations. The opportunity to combine instruments and computers became increasing practical as real-time digital signal processors became more available in the 1980s. As the level of refinement of real-time control became proportionally greater, it became evident that composed music for instruments and interactive computer systems was a viable medium of expression. And while it is true that interactive computer music is a relatively new area in the electronic music field, developments in the power of desktop computers and the sophistication of real-time software have been responsible for enormous growth of the genre in the last ten years. The challenge of interactive computer music has been to articulate sonic design and compositional structures in some sort of interactive relationship with live performers. While some composers use computers to model or imitate musical instruments, and others are interested in modeling human performance, I am not interested in replacing either instruments or performers. Musicians, with their years of experience playing on instruments which have often developed over centuries, offer rich musical and cultural potential, and are perhaps the ultimate driving force for me as a composer working with computer technology. Interactive Music Robert Rowe, in the seminal book Interactive Music Systems [Rowe, 1993], states: “Interactive music systems are those whose behavior changes in response to musical input”. A dictionary definition of the word interactive states: “capable of acting on or influencing each other”. This would imply that a feedback loop of some sort exists between performer and machine. Indeed, the Australian composer Barry Moon suggests: “levels of interaction can be gauged by the potential for change in the behaviors of computer and performer in their response to each other”. [Moon, 1997]. George Lewis, a pioneer in the field of interactive computer music, has stated that much of what passes for interactive music today is in reality just simple event “triggering”, which does not involve interaction except on the most primitive level. He also states that since (Euro-centric) composers often strive for control over musical structure and sound, this leads many composers to confuse triggering with real interaction. He describes an interactive system as “one in which the structures present as inputs are processed in quite a complex, multi-directional fashion. Often the output behavior is not immediately traceable to any particular input event.” [Rowe et al, 1992-93]. David Rokeby, the Toronto-based interactive artist, states that interaction transcends control, and in a successful interactive environment, direct correspondences between actions and results are not perceivable. In other words, if performers feel they are in control of (or are capable of controlling) an environment, then they cannot be truly interacting, since control is not interaction. [Rokeby, 1997]. Clearly, on a continuum from triggering to Rokeby's non-control there is a great deal of latitude to loosely label a wide range of artistic approaches and human/machine relationships as “interactive”. Fortunately, a composer can assign a variety of roles to a computer in an interactive music environment. The computer can be given the role of instrument, performer, conductor, and/or composer. These roles can exist simultaneously and/or change continually, and it is not necessary to conceive of this continuum horizontally. On a vertical axis everything from simple triggering to Rokeby-like interaction can have the potential to exist simultaneously. If performer and machine are to be equal in an interactive environment, one could argue that the performer should also be offered a similar variety of roles. A performer is already a performer, and already plays an instrument; therefore, a composer can assign the role of conductor and composer to a performer. The conductor role comes quite naturally to a performer in an interactive environment, and is commonly exploited. The composer role is more problematic: composers exploring algorithmic processes are often willing to allow aspects of a composition to be decided via a computer program, as this is inherent in the very nature of certain approaches to algorithmic composition. But some composers question the idea of allowing a performer to take on the role of composer, and usually this involves giving the performer some degree of freedom to improvise. While I am not attempting to equate performer improvisation with algorithmic procedures, there are certainly interesting connections between the two which are outside the scope of this paper. But if we look closely at the various explanations of interactive music, most of them imply or clearly involve some shifting of the composer's “responsibilities” towards the computer system, towards the performer, or both. Interaction is a very complex subject, and we are probably only at the beginnings of a discussion of human/machine relationships, which I suspect will continue to develop over many years. Musical Interactivity While this discussion about interaction is fascinating on aesthetic, philosophical, and humanistic levels, at the practical level of music-making the quantity or quality of human/machine interactivity that takes place is much less important to me than the quality of musical interactivity. Musical interactivity is something that happens when people play music together. Rich musical interactivity exists in music that has no computer or electronic part, and can even be found in instrument/tape pieces (albeit only the musician is actively interacting), so the level of interactivity of a computer system is really a secondary consideration from a musical point of view. While I stated that I am not interested in modeling performers or instruments, I am interested in using the computer to model the relationship that exists between musicians during a performance. This relationship involves a subtle kind of listening while playing in a constantly changing dialogue among players, often centering on expressive timing and/or expressive use of dynamics and articulation. We often refer to this aspect of music as “phrasing”, an elusive, yet highly important part of performance. While concepts of musical interpretation exist in solo performance, and one can discuss a solo performer's “interaction” with a score, the musical interactivity that exists when two or more musicians play together is a more appropriate model for describing interaction between musicians and computers in real-time. Musical Expression in Performance At its core, European music notation has developed as a Cartesian coordinate system in which precise and measurable information about just two parameters, frequency and time, are specified. In a typical performance we assume that frequency will be respected, usually rather precisely, while we assume that time will be respected in a less precise manner. But, within certain boundaries, variants in both frequency and time are expected and considered part of an expressive performance. Vibrato, portamento, rubato, accelerando, ritardando, etc., are all common markings used by composers and are part of the interpretive toolbox which a performer is expected to exploit in transforming frequency and time. Some composers prefer not take responsibility for notating subtle variations in frequency and time beyond the notated pitches and rhythms, so that expressive decisions are left to the performers' discretion. Other composers, who perhaps prefer to rely less on cultural conventions, specify variations of pitch and time in greater detail. Beyond frequency and time notation there exists an enormous variety of imprecise notation that can be found in any glossary of musical terms: staccato, legato, mezzo-forte, crescendo, sul ponticello, etc. Since these notations are not as easily measurable as pitch and time, they are considered more in the domain of performers. More importantly, a performance is judged, not by whether the pitches and rhythms are correct (this is a given), but by how well the performer expressively alters pitches to a small degree, and rhythm to a larger extent, while interpreting expressive markings pertaining to parameters of loudness, timbre, articulation, etc., with a rather large degree of freedom. The interpretation of these parameters is subjective a},
	language = {en},
	author = {Lippe, C.},
	year = {2002},
	annote = {Publication Title: undefined Published: /paper/Real-Time-Interaction-Among-Composers\%2C-Performers\%2C-Lippe/35a3ba8fc107f7d300dda2e5b7e0c8d0420a386a},
}

@phdthesis{masone216,
	type = {{PhD} {Thesis}},
	title = {The {Contemporary} {Bassoonist}: {Music} for {Interactive} {Electroacoustics} and {Bassoon}},
	abstract = {As the bassoon has evolved over time, the music written for the instrument has evolved around it, and was many times the catalyst for its evolution. Bassoon music of the seventeenth through early twentieth centuries has defined much of the curricula for bassoon studies, and has established how we consider and experience the bassoon. We experience, write, and consume music in vastly different ways than just a generation ago. Humans use technology for the most basic of tasks. Composers are using the technology of our generation to compose music that is a reflection of our time. This is a significant aspect of art music today, and bassoonists are barely participating in the creation of this new repertoire. Performance practice often considers only the musical score; interactive electronic music regularly goes beyond that. The combination of technological challenges and inexperience can make approaching electroacoustic music a daunting and inaccessible type of music for bassoonists. These issues require a different language to the performance practice: one that addresses music, amplification, computer software, hardware, the collaboration between performer and technology, and often the performer and composer. The author discusses problems that performers face when rehearsing and performing interactive electroacoustic works for bassoon, and offers some solutions.},
	language = {en},
	author = {Masone, Jolene Karen},
	year = {216},
}

@book{nika2015,
	title = {Guided {Improvisation} as {Dynamic} {Calls} to an {Offline} {Model}},
	abstract = {This paper describes a reactive architecture handling the hybrid temporality of guided human-computer music improvisation. It aims at combining reactivity and anticipation in the music generation processes steered by a \&quot; scenario \&quot;. The machine improvisation takes advantage of the temporal structure of this scenario to generate short-term anticipations ahead of the performance time, and reacts to external controls by refining or rewriting these anticipa-tions over time. To achieve this in the framework of an interactive software, guided improvisation is modeled as embedding a compositional process into a reactive architecture. This architecture is instantiated in the improvisation system ImproteK and implemented in OpenMusic.},
	language = {en},
	author = {Nika, Jérôme and Bouche, Dimitri and Bresson, Jean and Chemillier, M. and Assayag, G.},
	year = {2015},
	annote = {Publication Title: undefined Published: /paper/Guided-improvisation-as-dynamic-calls-to-an-offline-Nika-Bouche/fa2abefe79550a42dea5ad118e9877d687eab414},
}

@inproceedings{nika2016,
	title = {Guiding {Human}-{Computer} {Music} {Improvisation}: {Introducing} {Authoring} and {Control} with {Temporal} {Scenarios}. ({Guider} l'improvisation {Musicale} {Homme}-{Machine} : {Introduire} {Du} {Contrôle} et de {La} {Composition} {Avec} {Des} {Scénarios} {Temporels})},
	shorttitle = {Guiding {Human}-{Computer} {Music} {Improvisation}},
	abstract = {This thesis focuses on the introduction of authoring and controls in human-computer music improvisation through the use of temporal scenarios to guide or compose interactive performances, and addresses the dialectic between planning and reactivity in interactive music systems dedicated to improvisation. An interactive system dedicated to music improvisation generates music “on the fly”, in relation to the musical context of a live performance. This work follows on researches on machine improvisation seen as the navigation through a musical memory: typically the music played by an “analog” musician co-improvising with the system during a performance or an offline corpus. These researches were mainly dedicated to free improvisation, and we focus here on pulsed and “idiomatic” music. Within an idiomatic context, an improviser deals with issues of acceptability regarding the stylistic norms and aesthetic values implicitly carried by the musical idiom. This is also the case for an interactive music system that would like to play jazz, blues, or rock... without being limited to imperative rules that would not allow any kind of transgression or digression. Various repertoires of improvised music rely on a formalized and temporally structured object, for example a harmonic progression in jazz improvisation. The same way, the models and architecture we developed rely on a formal temporal structure. This structure does not carry the narrative dimension of the improvisation, that is its fundamentally aesthetic and non-explicit evolution, but is a sequence of formalized constraints for the machine improvisation. This thesis thus presents: a music generation model guided by a “scenario” introducing mechanisms of anticipation; a framework to compose improvised interactive performances at the “scenario” level; an architecture combining anticipatory behavior with reactivity using mixed static/dynamic scheduling techniques; an audio rendering module to perform live re-injection of captured material in synchrony with a non-metronomic beat; a study carried out with ten musicians through performances, work sessions, listening sessions and interviews. First, we propose a music generation model guided by a formal structure. In this framework “improvising” means navigating through an indexed memory to collect some contiguous or disconnected sequences matching the successive parts of a “scenario” guiding the improvisation (for example a chord progression). The musical purpose of the scenario is to ensure the conformity of the improvisations generated by the machine to the idiom it carries, and to introduce anticipation mechanisms in the generation process, by analogy with a musician anticipating the resolution of a harmonic progression. Using the formal genericity of the couple “scenario / memory”, we sketch a protocol to compose improvisation sessions at the scenario level. Defining scenarios described using audio-musical descriptors or any user-defined alphabet can lead to approach others dimensions of guided interactive improvisation. In this framework, musicians for whom the definition of a musical alphabet and the design of scenarios for improvisation is part of the creative process can be involved upstream, in the “meta-level of composition” consisting in the design of the musical language of the machine. This model can be used in a compositional workflow and is “offline” in the sense that one run produces a whole timed and structured musical gesture satisfying the designed scenario that will then be unfolded through time during performance. We present then a dynamic architecture embedding such generation processes with formal specifications in order to combine anticipation and reactivity in a context of guided improvisation. In this context, a reaction of the system to the external environment, such as control interfaces or live players input, cannot only be seen as a spontaneous instant response. Indeed, it has to take advantage of the knowledge of this temporal structure to benefit from anticipatory behavior. A reaction can be considered as a revision of mid-term anticipations, musical sequences previously generated by the system ahead of the time of the performance, in the light of new events or controls. To cope with the issue of combining long-term planning and reactivity, we therefore propose to model guided improvisation as dynamic calls to “compositional” processes, that it to say to embed intrinsically offline generation models in a reactive architecture. In order to be able to play with the musicians, and with the sound of the musicians, this architecture includes a novel audio rendering module that enables to improvise by re-injecting live audio material (processed and transformed online to match the scenario) in synchrony with a non-metronomic fluctuating pulse. Finally, this work fully integrated the results of frequent interactions with expert musicians to the iterative design of the models and architectures. These latter are implemented in the interactive music system ImproteK, one of the offspring of the OMax system, that was used at various occasions during live performances with improvisers. During these collaborations, work sessions were associated to listening sessions and interviews to gather the evaluations of the musicians on the system in order to validate and refine the scientific and technological choices.},
	author = {Nika, Jérôme},
	year = {2016},
}

@inproceedings{nika2017,
	address = {Shangai, China},
	title = {{DYCI2} {Agents}: {Merging} the ”free”, ”reactive”, and ”scenario-{Based}” {Music} {Generation} {Paradigms}},
	shorttitle = {{DYCI2} {Agents}},
	abstract = {The collaborative research and development project DYCI2, Creative Dynamics of Improvised Interaction, focuses on conceiving, adapting, and bringing into play efficient models of artificial listening, learning, interaction, and generation of musical contents. It aims at developing creative and autonomous digital musical agents able to take part in various human projects in an interactive and artistically credible way; and, in the end, at contributing to the perceptive and communicational skills of embedded artificial intelligence. The concerned areas are live performance, production, pedagogy, and active listening. This paper gives an overview focusing on one of the three main research issues of this project: conceiving multi-agent architectures and models of knowledge and decision in order to explore scenarios of music co-improvisation involving human and digital agents. The objective is to merge the usually exclusive "free" , "reactive", and "scenario-based" paradigms in interactive music generation to adapt to a wide range of musical contexts involving hybrid temporality and multimodal interactions.},
	booktitle = {International {Computer} {Music} {Conference}},
	author = {Nika, Jérôme and Déguernel, Ken and Chemla–Romeu-Santos, Axel and Vincent, Emmanuel and Assayag, Gérard},
	month = oct,
	year = {2017},
}

@inproceedings{pardo2016,
	title = {Expansion of {Musical} {Styles}, {Function} of {Texture}, and {Performing} {Techniques} in {Brian} {Lock}'s {Sonic} {Archaeologies} {No}. 1},
	abstract = {British composer Brian Lock merges the composition styles of Alexander Goehr, Henryk Gorecki and Witold Lutoslawski in his innovative works for instrumental sounds and electronics. His most recent work for flute, Sonic Archaeologies No.1, was premiered at the University of North Texas by Mary Karen Clardy, flute; Brian Lock, piano/electric keyboard; and Daniel Pardo, laptop/live mixing. The purpose of this dissertation is to provide flutists with artistic and technical guidance in preparing this work for flute, prerecorded orchestra, interactive electronics and improvisatory accompaniment. Sonic Archaeologies No. 1, a piece in five movements (Black Rain, Psychomania, Kodo, Susperia, and Deep in the Machine), incorporates contemporary techniques to create sounds other than the Western concert flute, with the use of live reinforcement devices such as microphones and time-based audio effects within a D.A.W. (Digital Audio Workstation.) Reggae, Hip-Hop and cinematic styles are juxtaposed within the work, fusing current genres with traditional rhythmic forms like the ones found in a bourree. As the solo instrument, flute provides more textural than melodic elements, and the performer is required to interact with an unpredictable sonic soundscape as a result of the improvisatory element of the keyboards and computer. The notation of Sonic Archaeologies No.1 invites interpretation blending and altering traditional sounds through microphones and a processed signal flow. The performance guide will address acoustical considerations when the flute sound is being manipulated by dynamic and time-based processors in live performance; the interaction between the flute, electronics and acoustic spaces; the elements of sound production that provide interpretation of contemporary popular styles; and the opportunities for the performer to find, explore and develop artistry beyond the limitations of music notation.},
	author = {Pardo, Daniel},
	year = {2016},
}

@article{pasquier2017,
	title = {An {Introduction} to {Musical} {Metacreation}},
	volume = {14},
	doi = {10.1145/2930672},
	abstract = {Musical metacreation (MuMe), also known as musical computational creativity, is a subfield of computational creativity that focuses on endowing machines with the ability to achieve creative musical tasks, such as composition, interpretation, improvisation, accompaniment, mixing, etc. It covers all dimensions of the theory and practice of computational generative music systems, ranging from purely artistic approaches to purely scientific ones, inclusive of discourses relevant to this topic from the humanities. MuMe systems range from purely generative ones to a variety of interactive systems, such as those for computer-assisted composition and computer-assisted sound design. In order to better appreciate the many dimensions of this interdisciplinary domain and see how it overlaps and differs from research in computer music, this introduction provides a general entry point. After defining and introducing the domain, its context, and some of its terminology, we reflect on some challenges and opportunities for the field as a whole.},
	number = {2},
	journal = {Computers in Entertainment},
	author = {Pasquier, Philippe and Eigenfeldt, Arne and Bown, Oliver and Dubnov, Shlomo},
	year = {2017},
	pages = {2:1--2:14},
}

@book{penny2009,
	title = {The {Extended} {Flautist}: {Techniques}, {Technologies} and {Performer} {Perceptions} in {Music} for {Flute} and {Electronics}},
	shorttitle = {The {Extended} {Flautist}},
	abstract = {............................................................................................................................. ii LIST OF FIGURES................................................................................................................viii LIST OF TABLES .................................................................................................................... x LIST OF MIND MAPS ........................................................................................................... xi ACKNOWLEDGEMENTS.................................................................................................... xii PRELUDE ..................................................................................................................................... 1 PART I: THE PROJECT.......................................................................................2},
	language = {en},
	author = {Penny, M. J.},
	year = {2009},
	annote = {Publication Title: undefined Published: /paper/The-Extended-Flautist\%3A-Techniques\%2C-Technologies-and-Penny/d8fcbbffd40fc6f8119de73e9923165d4a44bc80},
}

@inproceedings{puckette1993,
	title = {Nonobvious {Roles} for {Electronics} in {Performance} {Enhancement}},
	abstract = {A new paradigm is proposed which overcomes certain shortcomings in our current models of performer/computer interaction. The "Standard Model", in which the computer passively waits for performer input can be extended to include closed-loop behavior, both stable and unstable. In the time range typiied by the muscular and auditory subsystems, the existence Cadoz] of approximately linear spatially modelled feedback is emphasized. On a longer time frame, the possibilities are more subtile and powerful. In a well-designed timbre space / control space Wessel] the connection occurs on a more musical level and is evidenced in enhancements in the performer's own apparent output.},
	booktitle = {{ICMC}},
	author = {Puckette, M. and Settel, Zack},
	year = {1993},
}

@book{rocha2009,
	title = {{THE} {HYPER}-{KALIMBA}: {DEVELOPING} {AN} {AUGMENTED} {INSTRUMENT} {FROM} {A} {PERFORMER}'{S} {PERSPECTIVE}},
	shorttitle = {{THE} {HYPER}-{KALIMBA}},
	abstract = {The paper describes the development of the hyper-kalimba, an augmented instrument created by the authors. This development was divided into several phases and was based on constant consideration of technology, performance and compositionalissues. Thebasicgoalwastoextendthesound possibilities of the kalimba, without interfering with any of the original features of the instrument or with the performer's pre-existing skills. In this way performers were able to use all the traditional techniques previously developed, while learning and exploring all the new possibilities added to the instrument.},
	language = {en},
	author = {Rocha, F. and Escolade, M. and Malloch, Joseph W.},
	year = {2009},
	annote = {Publication Title: undefined Published: /paper/THE-HYPER-KALIMBA\%3A-DEVELOPING-AN-AUGMENTED-FROM-A-Rocha-Escolade/784db3affd1a73fd045f9d10356bbc162381baa9},
}

@article{rowe1999,
	title = {The {Aesthetics} of {Interactive} {Music} {Systems}},
	doi = {10.1080/07494469900640361},
	abstract = {Computers in music have made possible new kinds of composition at the same time that they have caused upheaval in the social and cultural practice of music making. Interactive music systems have a particular place in this context in that they explore some highly specific techniques of composition at the same time that they create a novel and engaging form of interaction between humans and computers. In this essay, real-time algorithmic composition in works including improvisation are considered as well as the contrasts between interactive and tape music. The author's composition Maritime for violin and interactive music system is presented as an illustration of the aesthetic viewpoint developed.},
	author = {Rowe, R.},
	year = {1999},
	file = {Full Text:/Users/Vincent/Zotero/storage/JHIFEPLP/Rowe - 1999 - The Aesthetics of Interactive Music Systems.pdf:application/pdf},
}

@inproceedings{summers2018,
	title = {Augmenting an {Improvised} {Practice} on the {Viola} {Da} {Gamba}},
	abstract = {This thesis examines my improvisatory practice on the viola da gamba and its augmentation with mixed-music computer systems. It comprises creative work and an extended written commentary and discussion. My creative work is presented in two albums of music – solo viola da gamba improvisation, and viola da gamba and mixed-music computer systems – and supplementary recorded material. The written commentary looks in depth at the presented creative work. I use the first, solo album to examine my improvisatory practice. To explore augmenting this practice with systems, I look in detail at my performances with gruntCount by Martin Parker, Laminate by myself and derivations by Ben Carey. Examples of these performances are presented in the second album. Scrutiny of these three systems leads to extended discussion of the following topics: 1. Taxonomy: What are these systems? What are the characteristics they display? Do these systems fit into a standard classification scheme? 2. Ontology: Do performances with these systems instantiate musical works? What are the criteria that would help us to decide? How much of my practice is therefore underpinned by musical works? 3. Copyright: Who is responsible for the musical output with these systems? Who is a legal/musical author in such performances? To conclude, I compare my improvisatory practice with and without systems and identify learnings arising from this research.},
	author = {Summers, M.},
	year = {2018},
}

@article{welch2010,
	title = {Programming {Machines} and {People}: {Techniques} for {Live} {Improvisation} with {Electronics}},
	shorttitle = {Programming {Machines} and {People}},
	doi = {10.1162/LMJ_a_00008},
	abstract = {ABSTRACT Many performers of new music do not come from an improvising tradition, and the addition of live electronics to works written for these performers may be intimidating due to their inexperience with improvising and/or working with technology. Although inexperience may be a problem, it can be overcome. The author describes techniques and strategies for creating rule-based improvisation environments with live electronics.},
	journal = {Leonardo Music Journal},
	author = {Welch, Chapman},
	year = {2010},
}

@article{kimura2003,
	title = {Creative process and performance practice of interactive computer music: a performer's tale},
	volume = {8},
	issn = {1355-7718, 1469-8153},
	shorttitle = {Creative process and performance practice of interactive computer music},
	url = {https://www.cambridge.org/core/product/identifier/S1355771803000268/type/journal_article},
	doi = {10.1017/S1355771803000268},
	abstract = {I have had a major interest in the performance practice issues in electronic and interactive systems over the years (see, for example, Kimura 1996). As a performer/composer often presenting pieces from the classical and other contemporary acoustic violin literature in traditional settings along with electronic works, and also as a teacher of interactive computer music performance at a conservatory where my students include highly trained performers, performance practice issues in computer music come up very frequently in association with the creative process. I tend to focus on creating MaxMSP patches that address a particular musical context or situation, rather than creating an elaborate versatile and reusable MaxMSP patch and then using that patch in a particular way to make music. This paper describes a few examples of my interest in this area: (i) System Aspects: Performance Practice Issues and Room Acoustics; (ii) ‘Pragmatic’ Programming and Performance of Interactive Music; and (iii) Creative Process and Interactive Computer Music.},
	language = {en},
	number = {3},
	urldate = {2021-02-05},
	journal = {Organised Sound},
	author = {Kimura, Mari},
	month = dec,
	year = {2003},
	pages = {289--296},
}

@inproceedings{alexander2014,
	address = {New York, NY, USA},
	series = {{ITS} '14},
	title = {Characterising the {Physicality} of {Everyday} {Buttons}},
	isbn = {978-1-4503-2587-5},
	doi = {10.1145/2669485.2669519},
	abstract = {A significant milestone in the development of physically-dynamic surfaces is the ability for buttons to protrude outwards from any location on a touch-screen. As a first step toward developing interaction requirements for this technology we conducted a survey of 1515 electronic push buttons in everyday home environments. We report a characterisation that describes the features of the data set and discusses important button properties that we expect will inform the design of future physically-dynamic devices and surfaces.},
	booktitle = {Proceedings of the {Ninth} {ACM} {International} {Conference} on {Interactive} {Tabletops} and {Surfaces}},
	publisher = {Association for Computing Machinery},
	author = {Alexander, Jason and Hardy, John and Wattam, Stephen},
	month = nov,
	year = {2014},
	pages = {205--208},
	file = {Alexander et al_2014_Characterising the Physicality of Everyday Buttons.pdf:/Users/Vincent/Zotero/storage/S75A32S2/Alexander et al_2014_Characterising the Physicality of Everyday Buttons.pdf:application/pdf},
}

@article{alling2007,
	title = {Mechanical {Keyboard} for {Concept} {Mock}-{Up}},
	copyright = {info:eu-repo/semantics/closedAccess},
	abstract = {Problem statement This master thesis was initiated since today, when developing a new mobile phone at Sony Ericsson, it is difficult to get a perception of the ergonomics and the tactility of the keyboard before the first design prototype (DP0 and DP1) is built. The SLA block model used today is completely solid and consequently a secured decision of the final design can not be made in this phase. After the first prototype build is done, it is complex and expensive to make major changes in the keyboard design. Before the DP1 is build, designers and engineers still are relatively free to make alterations in the design. Purpose Investigate the possibilities and the best way to implement a mechanical keyboard in the concept mock-up with the purpose to be able to secure the keyboard design in an early stage of the development process. Method The master thesis is conducted via benchmarking, brain-storming, interviews and consulting with people holding specific knowledge in the different areas. With the SLA-machine in the prototype workshop at Sony Ericsson a lot of tests and iterations have been made in order to find out the best practical solution and appropriate tolerances. Conclusions The master thesis has shown that it is possible to implement a mechanical keyboard in the concept mock-up with the SLA-process, using one of the concepts developed in this thesis. In comparison to the solid block-SLA model, the mechanical mock-up supplies a tactile apprehension of the keys, valuable for designers and product planners. At the same time the block model with its uncolored outside, is still just a draft of the final hand set. Alongside this thesis a guideline has been produced based on the results from the tests. This guideline contains the procedure of how to create the mock-ups and certain dimensions and tolerances to consider when creating a mechanical concept mock-up.},
	language = {eng},
	author = {Alling, Erik and Andersson, Ola},
	year = {2007},
}

@article{asundi2011,
	title = {Effects of {Keyboard} {Keyswitch} {Design}: {A} {Review} of the {Current} {Literature}},
	volume = {39},
	doi = {10.3233/WOR-2011-1161},
	abstract = {Objective: This article aims to provide designers and researchers with a summary of the current literature regarding the effects of specific keyswitch design parameters on user preference, performance, and biomechanical outcomes. Methods: Studies which evaluated the effects of keyswitch make force, make travel and over travel on preference, typing speed, errors, keyboard reaction forces and upper extremity EMG are included in the review. Studies which examined possible control strategies employed during keyswitch tapping are also covered. Results: General conclusions that can be drawn from these studies are: greater make forces result in increased keyboard reaction forces and EMG activity, users strike keys with forces 2– 7 times the required make force and that they employ a ballistic finger motion to do so. Furthermore, typists tend to prefer kinesthetic and auditory feedback. Conclusions: Due to the ballistic nature of typing, new keyswitch designs should be aimed at reducing impact forces. Future studies should examine the role of breakaway force and over travel as limited research has been done on the role of these parameters.},
	number = {2},
	journal = {Work},
	author = {Asundi, Krishna and Odell, Dan},
	year = {2011},
	note = {Publisher: IOS Press},
	pages = {151--159},
}

@inproceedings{bell2020,
	address = {Honolulu HI USA},
	title = {{PauseBoard}: {A} {Force}-{Feedback} {Keyboard} for {Unintrusively} {Encouraging} {Regular} {Typing} {Breaks}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{PauseBoard}},
	doi = {10.1145/3334480.3382969},
	abstract = {Maintaining positive digital well-being has become essential as we spend more and more time working at desks in offices, interacting with computers and typing for hours at a time. In this paper we present PauseBoard: a computer keyboard designed to unintrusively encourage users to take regular breaks. Through the use of motorised linear potentiometers, the force required to activate each key is increased towards the end of a set work period, until a maximum level of resistance is reached. Preliminary testing shows that 75 \% of users respond well to this novel gentle encouragement, being reminded to take breaks while still being able to concentrate and finish their current task, especially when the resistance is increased slowly over time.},
	language = {en},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Bell, Lewis and Lees, Jay and Smith, Will and Harding, Charlie and Lee, Ben and Bennett, Daniel},
	month = apr,
	year = {2020},
	pages = {1--8},
	file = {Bell et al_2020_PauseBoard.pdf:/Users/Vincent/Zotero/storage/FX7PJLU6/Bell et al_2020_PauseBoard.pdf:application/pdf},
}

@book{by2016,
	title = {Quickie {USB} {Keyboard} {Device}},
	abstract = {There are a ton of applications that we use that can benefit from keyboard shortcuts, and we use 'em religiously. Indeed, there are some tasks that we do so often that they warrant their own {\textbackslash}ldots},
	language = {en-US},
	author = {{By}},
	month = jun,
	year = {2016},
	note = {Publication Title: Hackaday},
}

@book{by2018,
	title = {Adding {Analog} {Touch} {To} ({Nearly}) {Any} {Mechanical} {Keyboard}},
	abstract = {The new hotness for DIY electronics is mechanical keyboards, and over the past few years we've seen some amazing innovations. This one is something different. It adds an analog sensor to near{\textbackslash}ldots},
	language = {en-US},
	author = {{By}},
	month = sep,
	year = {2018},
	note = {Publication Title: Hackaday},
}

@book{by2019,
	title = {Tokyo {Mechanical} {Keyboard} {Meetup} {Knocks} {Our} {Clacks} {Off}},
	abstract = {Just a few days ago, on the other side of the planet from this author, there was a mechanical keyboard meetup in Tokyo. Fortunately through the magic of the Internet we can all enjoy the impressive{\textbackslash}ldots},
	language = {en-US},
	author = {{By}},
	month = jun,
	year = {2019},
	note = {Publication Title: Hackaday},
}

@book{by2020,
	title = {Clacker {Hacker}: {Hot} {Rod} {Switch} {Mods}},
	shorttitle = {Clacker {Hacker}},
	abstract = {Whether you're a programmer, gamer, writer, or data entry specialist, the keyboard is an extension of your nervous system. It's not so much a tool as it is a medium for flow — for{\textbackslash}ldots},
	language = {en-US},
	author = {{By}},
	month = jul,
	year = {2020},
	note = {Publication Title: Hackaday},
}

@book{by2020a,
	title = {Print {Your} {Way} {To} {Keyboard} {Stability}},
	abstract = {Keyboard key stabilizers, or stabs as they're known in enthusiast circles, do exactly what you'd expect — they stabilize longer keys like the Shifts and the space bar so that they{\textbackslash}ldots},
	language = {en-US},
	author = {{By}},
	month = aug,
	year = {2020},
	note = {Publication Title: Hackaday},
}

@inproceedings{calegario2020,
	address = {Birmingham, UK},
	title = {Probatio 1.0: {Collaborative} {Development} of a {Toolkit} for {Functional} {DMI} {Prototypes}},
	abstract = {Probatio is an open-source toolkit for prototyping new digital musical instruments created in 2016. Based on a morphological chart of postures and controls of musical instruments, it comprises a set of blocks, bases, hubs, and supports that, when combined, allows designers, artists, and musicians to experiment with different input devices for musical interaction in different positions and postures. Several musicians have used the system, and based on these past experiences, we assembled a list of improvements to implement version 1.0 of the toolkit through a unique international partnership between two laboratories in Brazil and Canada. In this paper, we present the original toolkit and its use so far, summarize the main lessons learned from musicians using it, and present the requirements behind, and the final design of, v1.0 of the project. We also detail the work developed in digital fabrication using two different techniques: laser cutting and 3D printing, comparing their pros and cons. We finally discuss the opportunities and challenges of fully sharing the project online and replicating its parts in both countries.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {Birmingham City University},
	author = {Calegario, Filipe and Wanderley, Marcelo and Tragtenberg, João and Meneses, Eduardo and Wang, Johnty and Sullivan, John and Franco, Ivan and Kirkegaard, Mathias S and Bredholt, Mathias and Rohs, Josh},
	editor = {Michon, Romain and Schroeder, Franziska},
	month = jul,
	year = {2020},
	note = {ISSN: 2220-4806},
	pages = {285--290},
	file = {Calegario et al_2020_Probatio 1.pdf:/Users/Vincent/Zotero/storage/W24TD2KW/Calegario et al_2020_Probatio 1.pdf:application/pdf},
}

@book{coolermaster,
	title = {{ControlPad}},
	url = {https://www.coolermaster.com/catalog/peripheral/keyboards/controlpad/?fbclid=IwAR0x8E7UUAOT3uBdG-l7R4gQ5GgNMFs4nmADqt8YdlET_ciyEyli7xRlky8},
	abstract = {Cooler Master is proud to present ControlPad, an innovative new control method for gamers, creative professionals, musicians and everyone in between. Designed as a passion project from the ground up, ControlPad aims to change the way we interact with games and our most used applications by introducing analog control into a mobile keypad.},
	language = {en-001},
	author = {{COOLERMASTER}},
}

@article{dalton2018,
	title = {Ergonomic {Design} of a {Chorded} {Keyboard}},
	volume = {11},
	copyright = {Copyright (c) 2018 The UNSW Canberra at ADFA Journal of Undergraduate Engineering Research},
	abstract = {This report presents the design and prototyping of a functional chorded keyboard that accommodates for user ergonomics and preferences. The focus of this project is on the device as a user interface and a mechanical product. Having previously used a literature review to develop a set of requirements, an iterative design process was followed through from concept to fabrication – with particular emphasis on ergonomics and product design. The result of this process was a mature functional prototype. This device is representative of a mass-market consumer product in all functional aspects and as such is an ideal platform for further testing of this technology. The electronic and software considerations supporting this development are also briefly discussed.},
	language = {en},
	number = {1},
	journal = {The UNSW Canberra at ADFA Journal of Undergraduate Engineering Research},
	author = {Dalton, James Philip},
	month = dec,
	year = {2018},
	file = {Dalton_2018_Ergonomic Design of a Chorded Keyboard.pdf:/Users/Vincent/Zotero/storage/7E6M8AXD/Dalton_2018_Ergonomic Design of a Chorded Keyboard.pdf:application/pdf},
}

@inproceedings{dobrian2006,
	title = {The '{E}' in {NIME}: {Musical} {Expression} with {New} {Computer} {Interfaces}.},
	shorttitle = {The '{E}' in {NIME}},
	author = {Dobrian, Christopher and Koppelman, Daniel},
	month = jan,
	year = {2006},
	pages = {277--282},
	file = {Dobrian_Koppelman_2006_The 'E' in NIME.pdf:/Users/Vincent/Zotero/storage/N3UI3IIX/Dobrian_Koppelman_2006_The 'E' in NIME.pdf:application/pdf},
}

@inproceedings{fiebrink2007,
	address = {New York, NY, USA},
	series = {{NIME} '07},
	title = {Don't {Forget} the {Laptop}: {Using} {Native} {Input} {Capabilities} for {Expressive} {Musical} {Control}},
	isbn = {978-1-4503-7837-6},
	shorttitle = {Don't {Forget} the {Laptop}},
	doi = {10.1145/1279740.1279771},
	abstract = {We draw on our experiences with the Princeton Laptop Orchestra to discuss novel uses of the laptop's native physical inputs for flexible and expressive control. We argue that instruments designed using these built-in inputs offer benefits over custom standalone controllers, particularly in certain group performance settings; creatively thinking about native capabilities can lead to interesting and unique new interfaces. We discuss a variety of example instruments that use the laptop's native capabilities and suggest avenues for future work. We also describe a new toolkit for rapidly experimenting with these capabilities.},
	booktitle = {Proceedings of the 7th {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {Association for Computing Machinery},
	author = {Fiebrink, Rebecca and Wang, Ge and Cook, Perry R.},
	month = jun,
	year = {2007},
	pages = {164--167},
	file = {Fiebrink et al_2007_Don't Forget the Laptop.pdf:/Users/Vincent/Zotero/storage/YFIX54QF/Fiebrink et al_2007_Don't Forget the Laptop.pdf:application/pdf},
}

@article{gerard2002,
	title = {Short {Term} and {Long} {Term} {Effects} of {Enhanced} {Auditory} {Feedback} on {Typing} {Force}, {EMG}, and {Comfort} {While} {Typing}},
	volume = {33},
	issn = {0003-6870},
	doi = {10.1016/S0003-6870(01)00062-X},
	abstract = {Two studies were conducted to determine the effects of enhanced auditory feedback on typing force, electromyography (EMG) and subjective discomfort. The introduction of enhanced auditory feedback caused a 10– 20\% reduction in 90th percentile typing force, finger flexor EMG, and finger extensor EMG. Adaptation to the enhanced auditory feedback occurred in \${\textless}\$3min. After 1 week of intermittent enhanced auditory feedback there were no differences in typing force or EMG while subjects were typing with or without the enhanced auditory feedback. The continued use of auditory feedback did not further reduce the levels of typing force or EMG after 1 or 2 weeks of exposure.},
	language = {en},
	number = {2},
	journal = {Applied Ergonomics},
	author = {Gerard, Michael J and Armstrong, Thomas J and Rempel, David A and Woolley, Chuck},
	month = mar,
	year = {2002},
	pages = {129--138},
	file = {Gerard et al_2002_Short Term and Long Term Effects of Enhanced Auditory Feedback on Typing Force,.pdf:/Users/Vincent/Zotero/storage/DW57RUQS/Gerard et al_2002_Short Term and Long Term Effects of Enhanced Auditory Feedback on Typing Force,.pdf:application/pdf},
}

@book{help-142021,
	title = {Help-14/{Mechanical}-{Keyboard}},
	abstract = {DIY mechanical keyboard and where to find them. Contribute to help-14/mechanical-keyboard development by creating an account on GitHub.},
	author = {{Help-14}},
	month = jan,
	year = {2021},
}

@article{hollinger,
	title = {Evaluation of {Commercial} {Force}-{Sensing} {Resistors}},
	abstract = {Pressure and force touch sensors are pervasive in electronic musical instruments. While there are a variety of ways to sense pressures and forces, many instrument builders tend towards force-sensing resistors (FSR). These sensors are often implemented with only two design parameters in mind: their minimum and maximum resistances. While commercial FSRs are qualitative devices and not meant for accurate force measurement, if we want to design electronic musical instruments and interfaces which can compete in terms of expressivity with acoustic ones, care should be taken in choosing and implementing sensors properly. In an effort to better understand the electrical response of the commercially available sensors, several tests were performed to measure the time-varying response of the device itself, specifically the resistance drift and hysteresis under different forces. We present quantitative results of two tests performed to characterize the behavior of three commercial touch sensors and comment on ways to best use them in interface design. Results show a wide variation in drift and hysteresis characteristics among the different models.},
	language = {en},
	author = {Hollinger, Avrum and Wanderley, Marcelo M},
	pages = {4},
	file = {Hollinger_Wanderley_Evaluation of Commercial Force-Sensing Resistors.pdf:/Users/Vincent/Zotero/storage/WCD8N7WJ/Hollinger_Wanderley_Evaluation of Commercial Force-Sensing Resistors.pdf:application/pdf},
}

@article{hua2017,
	title = {A {Novel} {Thinner} {Mechanical} {Keyboard} {Key} {Based} on the {Slider} {Rocker} {Mechanism}},
	volume = {11},
	doi = {10.2174/1872212111666170512164748},
	abstract = {Background: Patents reveal that a comfortable touch, slim size and light in weight have become a prevailing trend in the design and development of computer keyboard. Current mechanical keyboard has attracted a great attention due to its advanced features especially the need for unique touch and comfort of mechanical key devices in the new era of computer technology. However, there are still shortcomings in the key size, portability and flexibility and also the assembling process. Methods: This paper proposes the novel use of the slider rocker mechanism for the thinner design of the mechanical keyboard key. The mechanism is combined with the snap-in connection to make the key thinner and assembling process simpler. Results: The slider rocker mechanism enables a thinner design of the key by driving the rotational motion through the rotary keycap to initiate the translational motion. This feature also improves the portability and flexibility for the mechanical keyboard. The key assembling process is deemed simpler based on the simple snap-in connection between the switch and the base of the key. The design and operation procedure of this mechanical keyboard key are described in detail. Conclusion: A novel thinner mechanical keyboard key has been developed based on the slider rocker mechanism and snap-in connection between the switch and base of the key to accomplish the reduction of size in the vertical direction of the key, improved portability and flexibility for mechanical keyboard and simpler key assembling process. To our knowledge, this is the first thinner mechanical key in the keyboard based on the new mechanisms of operation.},
	number = {3},
	journal = {Recent Patents on Engineering},
	author = {Hua, Yanbin and Yang, Baotong and Lee, Chew-Tin and Xu, Zhiqiang},
	month = dec,
	year = {2017},
	pages = {227--231},
}

@inproceedings{hwang2019,
	address = {Tempe Arizona USA},
	series = {{TEI} '19},
	title = {Expressive {Tactile} {Controls}},
	isbn = {978-1-4503-6196-5},
	doi = {10.1145/3294109.3300991},
	abstract = {Push buttons, sliders, switches, and dials-we use such controls everyday and everywhere, but we barely notice them. Expressive Tactile Controls is a research experiment with a series of controls that are augmented by giving human personalities. What if each control had a unique personality and they could express their emotion only through haptic feedback? How could our interaction with controls be improved? The research approached the question by constructing a series of button prototypes able to express themselves with varying tactile and kinesthetic feedback according to the interaction between the user and controls.},
	language = {en},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Hwang, Hayeon},
	month = mar,
	year = {2019},
	pages = {223--227},
	file = {Hwang_2019_Expressive Tactile Controls.pdf:/Users/Vincent/Zotero/storage/9EKHHGTN/Hwang_2019_Expressive Tactile Controls.pdf:application/pdf},
}

@inproceedings{kim2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Masking} {Study} of {Key}-{Click} {Feedback} {Signals} on a {Virtual} {Keyboard}},
	isbn = {978-3-642-31401-8},
	doi = {10.1007/978-3-642-31401-8_23},
	abstract = {We study masking of key-click feedback signals on a flat surface for ten-finger touch typing with localized tactile feedback. We hypothesize that people will attribute tactile feedback to the key being pressed, even with global tactile feedback, provided that the tactile signal on other parts of the surface is sufficiently attenuated. To this end, we measure the thresholds at which a tactile signal is barely perceptible to a finger that is resting passively on a surface while another finger actively presses on the surface and receives a key-click feedback signal. Combinations of the index and middle fingers of both hands are tested. The results indicate that the thresholds are independent of the signal amplitude on the active finger. Larger signal attenuation is needed when the index fingers of both hands are involved than when two fingers of the same hand are involved. Future research will extend the current experimental design to ten fingers and typing-based tasks.},
	language = {en},
	booktitle = {Haptics: {Perception}, {Devices}, {Mobility}, and {Communication}},
	publisher = {Springer},
	author = {Kim, Jin Ryong and Dai, Xiaowei and Cao, Xiang and Picciotto, Carl and Tan, Desney and Tan, Hong Z.},
	editor = {Isokoski, Poika and Springare, Jukka},
	year = {2012},
	pages = {247--257},
	file = {Kim et al_2012_A Masking Study of Key-Click Feedback Signals on a Virtual Keyboard.pdf:/Users/Vincent/Zotero/storage/XSH7NJ9V/Kim et al_2012_A Masking Study of Key-Click Feedback Signals on a Virtual Keyboard.pdf:application/pdf},
}

@article{kosaka1993,
	title = {A {Universal} {Keyboard} {Switch} for a {Feeling} {Test}},
	doi = {10.1109/ROMAN.1993.367717},
	abstract = {People sometime can recognize the quality of a computer keyboard by both the feel and/or sound of the key switches when they are pressed. For a good quality keyboard the switches when pressed, should have a feeling that is neither too heavy nor too light, and should also convey a clear confirmation of the switching being made. We focus on the man-machine interface of the keyboard and investigate the human feelings of touch and push, from the engineering point of view, in order to design switches that are comfortable to operate.\${\textless}{\textless}\$ETX\${\textgreater}{\textgreater}\$},
	journal = {Proceedings of 1993 2nd IEEE International Workshop on Robot and Human Communication},
	author = {Kosaka, H. and Serizawa, K. and Watanabe, I.},
	year = {1993},
}

@inproceedings{liao2020a,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '20},
	title = {Press'{Em}: {Simulating} {Varying} {Button} {Tactility} via {FDVV} {Models}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {Press'{Em}},
	doi = {10.1145/3334480.3383161},
	abstract = {Push-buttons provide rich haptic feedback during a press via mechanical structures. While different buttons have varying haptic qualities, few works have attempted to dynamically render such tactility, which limits designers from freely exploring buttons' haptic design. We extend the typical force-displacement (FD) model with vibration (V) and velocity-dependence characteristics (V) to form a novel FDVV model. We then introduce Press'Em, a 3D-printed prototype capable of simulating button tactility based on FDVV models. To drive Press'Em, an end-to-end simulation pipeline is presented that covers (1) capturing any physical buttons, (2) controlling the actuation signals, and (3) simulating the tactility. Our system can go beyond replicating existing buttons to enable designers to emulate and test non-existent ones with desired haptic properties. Press'Em aims to be a tool for future research to better understand and iterate over button designs.},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Yi-Chi and Kim, Sunjun and Lee, Byungjoo and Oulasvirta, Antti},
	month = apr,
	year = {2020},
	pages = {1--4},
	file = {Liao et al_2020_Press'Em.pdf:/Users/Vincent/Zotero/storage/Y5G8M78U/Liao et al_2020_Press'Em.pdf:application/pdf},
}

@inproceedings{liu2018,
	title = {Perceptual {Dimensionality} of {Manual} {Key} {Clicks}},
	doi = {10.1109/HAPTICS.2018.8357162},
	abstract = {The present study investigated the perceptual dimensions associated with manual key clicks, with the goal of developing realistic haptic key-click feedback signals for virtual keys. We first harvested eight adjective pairs for describing the haptic feel of button and key presses from native English speakers. We then conducted the main experiment where participants provided adjective ratings and grouping data for twenty-three buttons and keys. An MDS analysis of the grouping data led to either a 2-D or 3-D solution. By projecting adjective ratings onto the MDS solution spaces, we found the 2-D perceptual space to be an adequate representation of human perception of manual key clicks. The two perceptual dimensions are determined to be shallow-deep and rough-smooth. Future work will explore the physical parameters corresponding to the perceptual dimensions and ways to simulate realistic key clicks by designing feedback signals using the relevant parameters.},
	booktitle = {2018 {IEEE} {Haptics} {Symposium} ({HAPTICS})},
	author = {Liu, Q. and Tan, H. Z. and Jiang, L. and Zhang, Y.},
	month = mar,
	year = {2018},
	note = {ISSN: 2324-7355},
	pages = {112--118},
	file = {Liu et al_2018_Perceptual Dimensionality of Manual Key Clicks.pdf:/Users/Vincent/Zotero/storage/M78QC2FC/Liu et al_2018_Perceptual Dimensionality of Manual Key Clicks.pdf:application/pdf},
}

@incollection{malloch2017,
	title = {Embodied {Cognition} and {Digital} {Musical} {Instruments}: {Design} and {Performance}},
	booktitle = {The {Routledge} {Companion} to {Embodied} {Music} {Interaction}},
	publisher = {Routledge},
	author = {Malloch, Joseph and Wanderley, Marcelo M.},
	editor = {Lesaffre, Micheline and Leman, Marc and Maes, Pieter-Jan},
	year = {2017},
	pages = {440--449},
}

@inproceedings{marier2010,
	address = {Sydney, Australia},
	title = {The {Sponge} a {Flexible} {Interface}},
	doi = {10.5281/zenodo.1177839},
	abstract = {The sponge is an interface that allows a clear link to beestablished between gesture and sound in electroacousticmusic. The goals in developing the sponge were to reintroduce the pleasure of playing and to improve the interaction between the composer/performer and the audience. Ithas been argued that expenditure of effort or energy is required to obtain expressive interfaces. The sponge favors anenergy-sound relationship in two ways : 1) it senses acceleration, which is closely related to energy; and 2) it is madeout of a flexible material (foam) that requires effort to besqueezed or twisted. Some of the mapping strategies usedin a performance context with the sponge are discussed.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Marier, Martin},
	year = {2010},
	note = {ISSN: 2220-4806},
	keywords = {electroacoustic music, expressivity, Interface, mapping, performance},
	pages = {356--359},
	file = {Marier_2010_The Sponge a Flexible Interface.pdf:/Users/Vincent/Zotero/storage/DUTMGDGJ/Marier_2010_The Sponge a Flexible Interface.pdf:application/pdf},
}

@article{mcpherson2015,
	title = {Buttons, {Handles}, and {Keys}: {Advances} in {Continuous}-{Control} {Keyboard} {Instruments}},
	shorttitle = {Buttons, {Handles}, and {Keys}},
	doi = {10.1162/COMJ_a_00297},
	abstract = {The keyboard is one of the most popular and enduring musical interfaces ever created. Today, the keyboard is most closely associated with the acoustic piano and the electronic keyboards inspired by it, which share the essential feature of being discrete: Notes are defined temporally by their onset and release only, with little control over each note beyond velocity and timing. Many keyboard instruments have been invented, however, that let the player continuously shape each note. This article provides a review of keyboards whose keys allow continuous control, from early mechanical origins to the latest digital controllers and augmented instruments. Two of the author's own contributions will be described in detail: a portable optical scanner that can measure continuous key angle on any acoustic piano, and the TouchKeys capacitive multi-touch sensors, which measure the position of fingers on the key surfaces. These two instrument technologies share the trait that they transform the keys of existing keyboards into fully continuous controllers. In addition to their ability to shape the sound of a sustaining note, both technologies also give the keyboardist new dimensions of articulation beyond key velocity. Even in an era of new and imaginative musical interfaces, the keyboard is likely to remain with us for the foreseeable future, and the incorporation of continuous control can bring new levels of richness and nuance to a performance.},
	journal = {Computer Music Journal},
	author = {McPherson, A.},
	year = {2015},
	file = {McPherson_2015_Buttons, Handles, and Keys.pdf:/Users/Vincent/Zotero/storage/FS7WBBCD/McPherson_2015_Buttons, Handles, and Keys.pdf:application/pdf},
}

@article{medeiros2014,
	title = {A {Comprehensive} {Review} of {Sensors} and {Instrumentation} {Methods} in {Devices} for {Musical} {Expression}},
	volume = {14},
	doi = {10.3390/s140813556},
	abstract = {Digital Musical Instruments (DMIs) are musical instruments typically composed of a control surface where user interaction is measured by sensors whose values are mapped to sound synthesis algorithms. These instruments have gained interest among skilled musicians and performers in the last decades leading to artistic practices including musical performance, interactive installations and dance. The creation of DMIs typically involves several areas, among them: arts, design and engineering. The balance between these areas is an essential task in DMI design so that the resulting instruments are aesthetically appealing, robust, and allow responsive, accurate and repeatable sensing. In this paper, we review the use of sensors in the DMI community as manifested in the proceedings of the International Conference on New Interfaces for Musical Expression (NIME 2009-2013). Focusing on the sensor technologies and signal conditioning techniques used by the NIME community. Although it has been claimed that specifications for artistic tools are harder than those for military applications, this study raises a paradox showing that in most of the cases, DMIs are based on a few basic sensors types and unsophisticated engineering solutions, not taking advantage of more advanced sensing, instrumentation and signal processing techniques that could dramatically improve their response. We aim to raise awareness of limitations of any engineering solution and to assert the benefits of advanced electronics instrumentation design in DMIs. For this, we propose the use of specialized sensors such as strain gages, advanced conditioning circuits and signal processing tools such as sensor fusion. We believe that careful electronic instrumentation design may lead to more responsive instruments.},
	journal = {Sensors (Basel, Switzerland)},
	author = {Medeiros, Carolina and Wanderley, Marcelo},
	month = aug,
	year = {2014},
	pages = {13556--91},
	file = {Medeiros_Wanderley_2014_A Comprehensive Review of Sensors and Instrumentation Methods in Devices for.pdf:/Users/Vincent/Zotero/storage/CIQ8VNG5/Medeiros_Wanderley_2014_A Comprehensive Review of Sensors and Instrumentation Methods in Devices for.pdf:application/pdf},
}

@article{moro2017,
	title = {Dynamic {Temporal} {Behaviour} of the {Keyboard} {Action} on the {Hammond} {Organ} and {Its} {Perceptual} {Significance}.},
	doi = {10.1121/1.5003796},
	abstract = {The Hammond organ is one of earliest electronic instruments and is still used widely in contemporary popular music. One of its main sonic features is the "key-click," a transient that occurs upon note onset, caused by the mechanical bouncing of the nine electric contacts actuated during each key press. A study of the dynamic mechanical behaviour of the contact bounces is presented, showing that the velocity, the type of touch and, more in general, the temporal evolution of the key position, all affect different characteristics of the contact bounces. A second study focuses on the listener's perception of the generated sound and finds that listeners can classify sounds produced on the Hammond organ according to the type of touch and velocity used. It is concluded that the Hammond organ is a touch-responsive instrument and that the gesture used to produce a note affects the generated sound across multiple dimensions. The control available at the fingertips of the musician is therefore such that it cannot be easily reduced to a single scalar velocity parameter, as is common practice in modern digital emulations of the instrument.},
	journal = {The Journal of the Acoustical Society of America},
	author = {Moro, G. and McPherson, Andrew P. and Sandler, M.},
	year = {2017},
	file = {Moro et al_2017_Dynamic Temporal Behaviour of the Keyboard Action on the Hammond Organ and Its.pdf:/Users/Vincent/Zotero/storage/59TXKDBN/Moro et al_2017_Dynamic Temporal Behaviour of the Keyboard Action on the Hammond Organ and Its.pdf:application/pdf},
}

@inproceedings{nash2016,
	address = {Brisbane, Australia},
	title = {The 'e' in {QWERTY}: {Musical} {Expression} with {Old} {Computer} {Interfaces}},
	volume = {16},
	isbn = {978-1-925455-13-7},
	doi = {10.5281/zenodo.1176088},
	abstract = {This paper presents a development of the ubiquitous computer keyboard to capture velocity and other continuous musical properties, in order to support more expressive interaction with music software. Building on existing `virtual piano' utilities, the device is designed to provide a richer mechanism for note entry within predominantly non-realtime editing tasks, in applications where keyboard interaction is a central component of the user experience (score editors, sequencers, DAWs, trackers, live coding), and in which users draw on virtuosities in both music and computing. In the keyboard, additional hardware combines existing scan code (key press) data with accelerometer readings to create a secondary USB device, using the same cable but visible to software as a separate USB MIDI device aside existing USB HID functionality. This paper presents and evaluates an initial prototype, developed using an Arduino board and inexpensive sensors, and discusses design considerations and test findings in musical applications, drawing on user studies of keyboard-mediated music interaction. Without challenging more established (and expensive) performance devices; significant benefits are demonstrated in notation-mediated interaction, where the user's focus rests with software.},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {Queensland Conservatorium Griffith University},
	author = {Nash, Chris},
	year = {2016},
	note = {ISSN: 2220-4806},
	pages = {224--229},
	file = {Nash_2016_The 'e' in QWERTY.pdf:/Users/Vincent/Zotero/storage/KJ7JNUVC/Nash_2016_The 'e' in QWERTY.pdf:application/pdf},
}

@inproceedings{ogawa2015,
	address = {Kobe Japan},
	title = {Multiple {Texture} {Button} by {Adding} {Haptic} {Vibration} and {Displacement} {Sensing} to the {Physical} {Button}},
	isbn = {978-1-4503-3932-2},
	doi = {10.1145/2818384.2818394},
	language = {en},
	booktitle = {{SIGGRAPH} {Asia} 2015 {Haptic} {Media} {And} {Contents} {Design}},
	publisher = {ACM},
	author = {Ogawa, Daichi and Yem, Vibol and Hachisu, Taku and Kajimoto, Hiroyuki},
	month = nov,
	year = {2015},
	pages = {1--2},
	file = {Ogawa et al_2015_Multiple Texture Button by Adding Haptic Vibration and Displacement Sensing to.pdf:/Users/Vincent/Zotero/storage/9UKHPZKN/Ogawa et al_2015_Multiple Texture Button by Adding Haptic Vibration and Displacement Sensing to.pdf:application/pdf},
}

@inproceedings{ozawa2019,
	title = {Effects of {Listening} {Attitudes} on {Affective} {Evaluation} of {Switch} {Sounds}},
	doi = {10.5057/ISASE.2019-C000018},
	abstract = {Switch sounds are not always pleasant for passive listeners although they are comfortable for operators to confirm the completion of their operations. An example is how the sounds of keyboard typing of other passengers are deemed noisy in a railway train. This difference in affective evaluation may be due to the attitudes of listeners: the operator is in an active listening attitude with tactile feedback from the pushed switch whereas a usual listener is in passive listening attitude. To separate the effects of tactile information and listening attitudes, we defined an active listener as one in an active listening attitude but without tactile feedback. To examine these effects on affective evaluation of switch sounds, a psychoacoustical experiment was carried out using 15 switches. The sound quality of each switch sound was evaluated by the semantic differential (SD) method using 26 adjective pairs. Eighty-one subjects participated in the experiment as one of the roles of operator, active listener, and passive listener. The results were analyzed using factor analysis; the three factors of activity (brightness), evaluation (aesthetic state), and potency (volume) were extracted. The comparisons of these factor scores among the three roles revealed the following two points. First, the effects of tactile information seem different depending on the switches used. Second, a passive listening attitude results in the negative evaluation of sound: less active, more uncomfortable, and noisier. This finding supports the abovementioned scenario in a train.},
	author = {Ozawa, K. and Yamaji, Kousuke and Shirasaka, Takeshi and Saito, K. and Shimomura, Hisato},
	year = {2019},
	file = {Ozawa et al_2019_Effects of Listening Attitudes on Affective Evaluation of Switch Sounds.pdf:/Users/Vincent/Zotero/storage/W7IVNRZF/Ozawa et al_2019_Effects of Listening Attitudes on Affective Evaluation of Switch Sounds.pdf:application/pdf},
}

@inproceedings{park2020,
	address = {Virtual Event USA},
	title = {Augmenting {Physical} {Buttons} with {Vibrotactile} {Feedback} for {Programmable} {Feels}},
	isbn = {978-1-4503-7514-6},
	doi = {10.1145/3379337.3415837},
	abstract = {Physical buttons provide clear haptic feedback when pressed and released, but their responses are unvarying. Physical buttons can be powered by force actuators to produce unlimited click sensations, but the cost is substantial. An alternative can be augmenting physical buttons with simple and inexpensive vibration actuators. When pushed, an augmented button generates a vibration overlayed on the button's original kinesthetic response, under the general framework of haptic augmented reality. We explore the design space of augmented buttons while changing vibration frequency, amplitude, duration, and envelope. We then visualize the perceptual structure of augmented buttons by estimating a perceptual space for 7 physical buttons and 40 augmented buttons. Their sensations are also assessed against adjectives, and results are mapped into the perceptual space to identify meaningful perceptual dimensions. Our results contribute to understanding the benefts and limitations of programmable vibration-augmented physical buttons with emphasis on their feels.},
	language = {en},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Park, Chaeyong and Yoon, Jinhyuk and Oh, Seungjae and Choi, Seungmoon},
	month = oct,
	year = {2020},
	pages = {924--937},
	file = {Park et al_2020_Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels.pdf:/Users/Vincent/Zotero/storage/2VVCZIIZ/Park et al_2020_Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels.pdf:application/pdf},
}

@article{peery2018,
	title = {{3D} {Printed} {Composite} {Keyboard} {Switches}},
	volume = {17},
	issn = {23519789},
	doi = {10.1016/j.promfg.2018.10.057},
	language = {en},
	journal = {Procedia Manufacturing},
	author = {Peery, Alec and Sormaz, Dušan},
	year = {2018},
	pages = {357--362},
	file = {Peery_Sormaz_2018_3D Printed Composite Keyboard Switches.pdf:/Users/Vincent/Zotero/storage/NIRJZTIE/Peery_Sormaz_2018_3D Printed Composite Keyboard Switches.pdf:application/pdf},
}

@article{pereira2012,
	title = {The {Effect} of {Keyboard} {Key} {Spacing} on {Productivity}, {Usability}, and {Biomechanics} in {Touch} {Typists} with {Large} {Hands}},
	volume = {56},
	issn = {2169-5067},
	doi = {10.1177/1071181312561271},
	abstract = {International standards that specify the spacing between keys on a keyboard have been guided primarily by design convention. Experienced typists (N=37) with large hands typed on five keyboards with different horizontal and vertical key spacing (19x19, 18x19, 17x19, 16x19, and 17x17mm) while productivity, comfort ratings, left and right extensor carpi ulnaris (ECU) and flexor carpi ulnaris (FCU) muscle forces, and right and left wrist extension and ulnar deviation were recorded. Productivity and usability ratings were significantly worse for the 16x19 keyboard. There was a trend for muscle activity to increase in the left forearm and decrease in the right forearm with decreasing horizontal key spacing. There was also a trend for left wrist extension to increase and left ulnar deviation to decrease with decreasing horizontal key spacing. The study findings support key spacing on a keyboard between 17 and 19mm. These findings may influence keyboard standards and design of keyboards.},
	language = {en},
	number = {1},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Pereira, Anna and Lee, David L. and Sadeeshkumar, Harini and Laroche, Charles and Odell, Dan and Rempel, David},
	year = {2012},
	note = {Publisher: SAGE Publications Inc},
	pages = {1872--1876},
	file = {Pereira et al_2012_The Effect of Keyboard Key Spacing on Productivity, Usability, and Biomechanics.pdf:/Users/Vincent/Zotero/storage/NAIQASHA/Pereira et al_2012_The Effect of Keyboard Key Spacing on Productivity, Usability, and Biomechanics.pdf:application/pdf},
}

@article{pham2015,
	title = {Mechanical and {Membrane} {Keyboard} {Typing} {Assessment} {Using} {Surface} {Electromyography} ({sEMG})},
	volume = {59},
	issn = {2169-5067},
	doi = {10.1177/1541931215591268},
	abstract = {There are many different types of keyboards available for use. However, the mechanical keyboard is becoming more popular with enthusiast computer users. Vendors and users alike proclaim that they provide increased typing speeds and that less physical effort is required to activate the keyswitches due to the additional tactile and auditory feedback from the keyswitch design. This study investigates words per minute (WPM), error percentage, and surface electromyography (EMG) of the flexor arm muscle activity during a typing task using a membrane and mechanical keyboard. Results showed statistical significance with both flexor muscles exerting less effort on a mechanical keyboard. Advantages were not limited the mechanical keyboard with WPM revealing greater typing speeds with the standard membrane keyboard.},
	number = {1},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Pham, Tri and Kelling, Nicholas},
	month = sep,
	year = {2015},
	note = {Publisher: SAGE Publications Inc},
	pages = {912--915},
	file = {Pham_Kelling_2015_Mechanical and Membrane Keyboard Typing Assessment Using Surface.pdf:/Users/Vincent/Zotero/storage/BMZDGWJB/Pham_Kelling_2015_Mechanical and Membrane Keyboard Typing Assessment Using Surface.pdf:application/pdf},
}

@inproceedings{sullivan2018,
	title = {Stability, {Reliability}, {Compatibility}: {Reviewing} 40 {Years} of {DMI} {Design}},
	shorttitle = {Stability, {Reliability}, {Compatibility}},
	abstract = {Despite the proliferation of new digital musical instruments (DMIs) coming from a diverse community of designers, researchers and creative practitioners, many of these instruments experience short life cycles and see little actual use in performance. There are a variety of reasons for this, including a lack of established technique and repertoire for new instruments, and the prospect that some designs may be intended for other purposes besides performance. In addition, we propose that many designs may not meet basic functional standards necessary for an instrument to withstand the rigors of real-world performance situations. For active and professional musicians, a DMI might not be viable unless these issues have been specifically addressed in the design process, as much as possible, to ensure trouble-free use during performance. Here we discuss findings from user surveys around the design and use of DMIs in performance, from which we identify primary factors relating to stability, reliability and compatibility that are necessary for their dependable use. We then review the state of the art in new instrument design through 40 years of proceedings from three conferences - ICMC, NIME, and SMC - to see where and how these have been discussed previously. Our review highlights key factors for the design of new instruments to meet the practical demands of real-world use by active musicians.},
	author = {Sullivan, John and Wanderley, Marcelo},
	month = jul,
	year = {2018},
	file = {Sullivan_Wanderley_2018_Stability, Reliability, Compatibility.pdf:/Users/Vincent/Zotero/storage/QSYJXAF3/Sullivan_Wanderley_2018_Stability, Reliability, Compatibility.pdf:application/pdf},
}

@inproceedings{taylor2014,
	address = {New York, NY, USA},
	series = {{CHI} '14},
	title = {Type-{Hover}-{Swipe} in 96 {Bytes}: {A} {Motion} {Sensing} {Mechanical} {Keyboard}},
	isbn = {978-1-4503-2473-1},
	shorttitle = {Type-{Hover}-{Swipe} in 96 {Bytes}},
	doi = {10.1145/2556288.2557030},
	abstract = {We present a new type of augmented mechanical keyboard, capable of sensing rich and expressive motion gestures performed both on and directly above the device. Our hardware comprises of low-resolution matrix of infrared (IR) proximity sensors interspersed between the keys of a regular mechanical keyboard. This results in coarse but high frame-rate motion data. We extend a machine learning algorithm, traditionally used for static classification only, to robustly support dynamic, temporal gestures. We propose the use of motion signatures a technique that utilizes pairs of motion history images and a random forest based classifier to robustly recognize a large set of motion gestures on and directly above the keyboard. Our technique achieves a mean per-frame classification accuracy of 75.6\% in leave-one-subject-out and 89.9\% in half-test/half-training cross-validation. We detail our hardware and gesture recognition algorithm, provide performance and accuracy numbers, and demonstrate a large set of gestures designed to be performed with our device. We conclude with qualitative feedback from users, discussion of limitations and areas for future work.},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Taylor, Stuart and Keskin, Cem and Hilliges, Otmar and Izadi, Shahram and Helmes, John},
	month = apr,
	year = {2014},
	pages = {1695--1704},
}

@article{wanderley2002,
	title = {Evaluation of {Input} {Devices} for {Musical} {Expression}: {Borrowing} {Tools} from {HCI}},
	volume = {26},
	shorttitle = {Evaluation of {Input} {Devices} for {Musical} {Expression}},
	doi = {10.1162/014892602320582981},
	abstract = {Input devices for musical expression were evaluated by drawing parallels to existing research in the field of human computer interaction (HCI). The applications of the knowledge was discussed to the development of interfaces for musical expression. A set of musical tasks was discussed to allow the evaluation of existing input devices. The present evaluation methodology was found useful for designers, composers and performers.},
	journal = {Computer Music Journal - COMPUT MUSIC J},
	author = {Wanderley, Marcelo and Orio, Nicola},
	month = sep,
	year = {2002},
	pages = {62--76},
	file = {Wanderley_Orio_2002_Evaluation of Input Devices for Musical Expression.pdf:/Users/Vincent/Zotero/storage/E34YTJET/Wanderley_Orio_2002_Evaluation of Input Devices for Musical Expression.pdf:application/pdf},
}

@article{wanderley2004,
	title = {Gestural {Control} of {Sound} {Synthesis}},
	volume = {92},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2004.825882},
	abstract = {This paper provides a review of gestural control of sound synthesis in the context of the design and evaluation of digital musical instruments. It discusses research in various areas related to this field and equally focuses on four main topics: analysis of music performers' gestures, gestural capture technologies, real-time sound synthesis methods, and strategies for mapping gesture variables to sound synthesis input parameters. Finally, this approach is illustrated by presenting an application of this research to the control of digital audio effects.},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Wanderley, M. M. and Depalle, P.},
	month = apr,
	year = {2004},
	pages = {632--644},
	file = {Wanderley_Depalle_2004_Gestural Control of Sound Synthesis.pdf:/Users/Vincent/Zotero/storage/VDLLD36A/Wanderley_Depalle_2004_Gestural Control of Sound Synthesis.pdf:application/pdf},
}

@inproceedings{wanderley2006,
	address = {Paris, France},
	title = {{SensorWiki}.{Org}: {A} {Collaborative} {Resource} for {Researchers} and {Interface} {Designers}},
	doi = {10.5281/zenodo.1177015},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Wanderley, Marcelo M. and Birnbaum, David and Malloch, Joseph and Sinyor, Elliot and Boissinot, Julien},
	year = {2006},
	note = {ISSN: 2220-4806},
	keywords = {collaborative website, open content, sensors, Wiki},
	pages = {180--183},
	file = {Wanderley et al_2006_SensorWiki.pdf:/Users/Vincent/Zotero/storage/PXR6UQXH/Wanderley et al_2006_SensorWiki.pdf:application/pdf},
}

@article{watanabe1994,
	title = {Selection and {Design} of {Keyboard} {Switches} {Based} on the {Human} {Feeling}},
	doi = {10.9746/SICETR1965.30.208},
	abstract = {Semantic Scholar extracted view of "Selection and Design of Keyboard Switches Based on the Human Feeling" by K. Watanabe et al.},
	author = {Watanabe, K. and Serizawa, Kazumasa},
	year = {1994},
	file = {Watanabe_Serizawa_1994_Selection and Design of Keyboard Switches Based on the Human Feeling.pdf:/Users/Vincent/Zotero/storage/9CES2RTC/Watanabe_Serizawa_1994_Selection and Design of Keyboard Switches Based on the Human Feeling.pdf:application/pdf},
}

@article{watanabe1995,
	title = {Evaluation of {Keyboard} {Switches} {Based} on {Kansei} ({Human} {Sensitivity}) {Information}},
	doi = {10.1109/ROMAN.1995.531936},
	abstract = {Comfortable man-machine interfaces are required for current advanced systems. This paper aims at investigating the relation between reaction force of keyboard switches and human switch operation feeling. We investigate how each reaction force of switches effects to the touch feeling of keyboard switches via a sensory test. The results of factor analysis for the questionnaire data obtained in the sensory test show that the most substantial factor is the feeling expressed by the words "Clear", "Smooth", "Stiff", and "Clicking". Finally, each feeling described by each word is directly evaluated by reaction force or vice versa on dual scales. The dual scale for reaction forces and degree of feeling can be directly used for designing a comfortable keyboard switch.},
	journal = {Proceedings 4th IEEE International Workshop on Robot and Human Communication},
	author = {Watanabe, K. and Kosaka, H.},
	year = {1995},
}

@book{wolf2021,
	title = {Diimdeep/{Awesome}-{Split}-{Keyboards}},
	abstract = {A collection of ergonomic split keyboards ⌨. Contribute to diimdeep/awesome-split-keyboards development by creating an account on GitHub.},
	author = {Wolf, Dmitry},
	month = jan,
	year = {2021},
}

@article{xiao2019,
	title = {Vidgets: {Modular} {Mechanical} {Widgets} for {Mobile} {Devices}},
	volume = {38},
	issn = {0730-0301},
	shorttitle = {Vidgets},
	doi = {10.1145/3306346.3322943},
	abstract = {We present Vidgets, a family of mechanical widgets, specifically push buttons and rotary knobs that augment mobile devices with tangible user interfaces. When these widgets are attached to a mobile device and a user interacts with them, the widgets' nonlinear mechanical response shifts the device slightly and quickly, and this subtle motion can be detected by the accelerometer commonly equipped on mobile devices. We propose a physics-based model to understand the nonlinear mechanical response of widgets. This understanding enables us to design tactile force profiles of these widgets so that the resulting accelerometer signals become easy to recognize. We then develop a lightweight signal processing algorithm that analyzes the accelerometer signals and recognizes how the user interacts with the widgets in real time. Vidgets widgets are low-cost, compact, reconfigurable, and power efficient. They can form a diverse set of physical interfaces that enrich users' interactions with mobile devices in various practical scenarios. We demonstrate their use in three applications: photo capture with single-handed zoom, control of mobile games, and making a playable mobile music instrument.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Xiao, Chang and Bayer, Karl and Zheng, Changxi and Nayar, Shree K.},
	month = jul,
	year = {2019},
	pages = {100:1--100:12},
	file = {Xiao et al_2019_Vidgets.pdf:/Users/Vincent/Zotero/storage/S4X9N5K4/Xiao et al_2019_Vidgets.pdf:application/pdf},
}

@mastersthesis{zheng2017,
	title = {Enabling {Expressive} {Keyboard} {Interaction} with {Finger}, {Hand}, and {Hand} {Posture} {Identification}},
	abstract = {The input space of conventional physical keyboards is largely limited by the number of keys. To enable more actions than simply entering the symbol represented by a key, standard keyboards use combinations of modifier keys such as command, alternate, or shift to re-purpose the standard text entry behaviour. To explore alternatives to conventional keyboard shortcuts and enable more expressive keyboard interaction, this thesis first presents Finger-Aware Shortcuts, which encode information from finger, hand, and hand posture identification as keyboard shortcuts. By detecting the hand and finger used to press a key, and an open or closed hand posture, a key press can have multiple command mappings. A formative study revealed the performance and preference patterns when using different fingers and postures to press a key. The results were used to develop a computer vision algorithm to identify fingers and hands on a keyboard captured by a built-in laptop camera and a reflector. This algorithm was built into a background service to enable system-wide Finger-Aware Shortcut keys in any application. A controlled experiment used the service to compare the performance of Finger-Aware Shortcuts with existing methods. The results showed that Finger-Aware Shortcuts are comparable with a common class of shortcuts using multiple modifier keys. Several application demonstrations illustrate different use cases and mappings for Finger-Aware Shortcuts. To further explore how introducing finger awareness can help foster the learning and use of keyboard shortcuts, an interview study was conducted with expert computer users to identify the likely causes that hinder the adoption of keyboard shortcuts. Based on this, the concept of Finger-Aware Shortcuts is extended and two guided keyboard shortcut techniques are proposed: FingerArc and FingerChord. The two techniques provide dynamic visual guidance on the screen when users press and hold an alphabetical key semantically related to a set of commands. FingerArc differentiates these commands by examining the angle between the thumb and index finger; FingerChord differentiates these commands by allowing users to press different key areas using a second finger. The thesis contributes comprehensive evaluations of Finger-Aware Shortcuts and proof-of-concept demonstrations of FingerArc and FingerChord. Together, they contribute a novel interaction space that expands the conventional keyboard input space with more expressivity.},
	language = {en},
	school = {University of Waterloo},
	author = {Zheng, Jingjie},
	year = {2017},
	file = {Zheng_2017_Enabling Expressive Keyboard Interaction with Finger, Hand, and Hand Posture.pdf:/Users/Vincent/Zotero/storage/G8ITBIV5/Zheng_2017_Enabling Expressive Keyboard Interaction with Finger, Hand, and Hand Posture.pdf:application/pdf},
}

@inproceedings{zheng2019,
	address = {Tempe Arizona USA},
	title = {Mechamagnets: {Designing} and {Fabricating} {Haptic} and {Functional} {Physical} {Inputs} with {Embedded} {Magnets}},
	isbn = {978-1-4503-6196-5},
	shorttitle = {Mechamagnets},
	doi = {10.1145/3294109.3295622},
	abstract = {We present Mechamagnets, a technique for facilitating the design and fabrication of haptic and functional inputs for physical interfaces. This technique consists of a set of 3D printed spatial constraints which facilitate different physical movements, as well as unpowered haptic profiles created by embedding static magnets in 3D printed parts. We propose the Mechamagnets taxonomy to map the design space of this technique for designers and makers. Furthermore, we leverage the use of magnets by instrumenting these objects with linear Hall effect sensors to create functional digital inputs. We showcase Mechamagnets with a series of novel physical interfaces made with this technique.},
	language = {en},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {ACM},
	author = {Zheng, Clement and Kim, Jeeeun and Leithinger, Daniel and Gross, Mark D. and Do, Ellen Yi-Luen},
	month = mar,
	year = {2019},
	pages = {325--334},
	file = {Zheng et al_2019_Mechamagnets.pdf:/Users/Vincent/Zotero/storage/ZAKST8JI/Zheng et al_2019_Mechamagnets.pdf:application/pdf},
}

@book{zotero-264,
	title = {Mechanical {Keyboard} {Switches}},
	url = {https://mechanicalkeyboards.com/switches/index.php},
}

@inproceedings{xia2014,
	address = {New York, NY, USA},
	series = {{UIST} '14},
	title = {Zero-latency tapping: using hover information to predict touch locations and eliminate touchdown latency},
	isbn = {978-1-4503-3069-5},
	shorttitle = {Zero-latency tapping},
	url = {https://doi.org/10.1145/2642918.2647348},
	doi = {10.1145/2642918.2647348},
	abstract = {A method of reducing the perceived latency of touch input by employing a model to predict touch events before the finger reaches the touch surface is proposed. A corpus of 3D finger movement data was collected, and used to develop a model capable of three granularities at different phases of movement: initial direction, final touch location, time of touchdown. The model is validated for target distances {\textgreater}= 25.5cm, and demonstrated to have a mean accuracy of 1.05cm 128ms before the user touches the screen. Preference study of different levels of latency reveals a strong preference for unperceived latency touchdown feedback. A form of 'soft' feedback, as well as other uses for this prediction to improve performance, is proposed.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 27th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Xia, Haijun and Jota, Ricardo and McCanny, Benjamin and Yu, Zhe and Forlines, Clifton and Singh, Karan and Wigdor, Daniel},
	month = oct,
	year = {2014},
	keywords = {latency, model, pre-touch, prediction},
	pages = {205--214},
}

@article{krause2010,
	title = {Perception in action: {The} impact of sensory information on sensorimotor synchronization in musicians and non-musicians},
	volume = {133},
	issn = {0001-6918},
	shorttitle = {Perception in action},
	url = {https://www.sciencedirect.com/science/article/pii/S0001691809001176},
	doi = {10.1016/j.actpsy.2009.08.003},
	abstract = {The present study aimed at investigating to what extent sensorimotor synchronization is related to (i) musical specialization, (ii) perceptual discrimination, and (iii) the movement’s trajectory. To this end, musicians with different musical expertise (drummers, professional pianists, amateur pianists, singers, and non-musicians) performed an auditory and visual synchronization and a cross-modal temporal discrimination task. During auditory synchronization drummers performed less variably than amateur pianists, singers and non-musicians. In the cross-modal discrimination task drummers showed superior discrimination abilities which were correlated with synchronization variability as well as with the trajectory. These data suggest that (i) the type of specialized musical instrument affects synchronization abilities and (ii) synchronization accuracy is related to perceptual discrimination abilities as well as to (iii) the movement’s trajectory. Since particularly synchronization variability was affected by musical expertise, the present data imply that the type of instrument improves accuracy of timekeeping mechanisms.},
	language = {en},
	number = {1},
	urldate = {2021-04-05},
	journal = {Acta Psychologica},
	author = {Krause, Vanessa and Pollok, Bettina and Schnitzler, Alfons},
	month = jan,
	year = {2010},
	keywords = {Cross-modal temporal discrimination, Drummers, Musicians, Sensorimotor synchronization, Timing},
	pages = {28--37},
	file = {ScienceDirect Snapshot:/Users/Vincent/Zotero/storage/QZUESQRZ/S0001691809001176.html:text/html},
}

@inproceedings{lee2016,
	address = {San Jose California USA},
	title = {Modelling {Error} {Rates} in {Temporal} {Pointing}},
	isbn = {978-1-4503-3362-7},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858143},
	doi = {10.1145/2858036.2858143},
	abstract = {We present a novel model to predict error rates in temporal pointing. With temporal pointing, a target is about to appear within a limited time window for selection. Unlike in spatial pointing, there is no movement to control in the temporal domain; the user can only determine when to launch the response. Although this task is common in interactions requiring temporal precision, rhythm, or synchrony, no previous HCI model predicts error rates as a function of task properties. Our model assumes that users have an implicit point of aim but their ability to elicit the input event at that time is hampered by variability in three processes: 1) an internal time-keeping process, 2) a response-execution stage, and 3) input processing in the computer. We derive a mathematical model with two parameters from these assumptions. High ﬁt is shown for user performance with two task types, including a rapidly paced game. The model can explain previous ﬁndings showing that touchscreens are much worse in temporal pointing than physical input devices. It also has novel implications for design that extend beyond the conventional wisdom of minimising latency.},
	language = {en},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lee, Byungjoo and Oulasvirta, Antti},
	month = may,
	year = {2016},
	pages = {1857--1868},
	file = {Lee_Oulasvirta_2016_Modelling Error Rates in Temporal Pointing.pdf:/Users/Vincent/Zotero/storage/E2FKQWKX/Lee_Oulasvirta_2016_Modelling Error Rates in Temporal Pointing.pdf:application/pdf},
}

@inproceedings{park2018,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '18},
	title = {Button++: {Designing} {Risk}-aware {Smart} {Buttons}},
	isbn = {978-1-4503-5621-3},
	shorttitle = {Button++},
	url = {https://doi.org/10.1145/3170427.3188645},
	doi = {10.1145/3170427.3188645},
	abstract = {Buttons are the most commonly used input devices. So far the goal of the designers was to provide a passive button that can accept user input as easily as possible. Therefore, based on Fitts' law, they maximize the size of the button and make the distance closer. This paper proposes Button++, a novel method to design smart buttons that actively judge user's movement risk and selectively trigger input. Based on the latest model of moving target selection, Button++ tracks the user's submovement just before the click and infers the expected error rate that can occur if the user repeatedly clicks with the same movement. This allows designers to make buttons that actively respond to the amount of risk in the user's input movement.},
	urldate = {2021-04-05},
	booktitle = {Extended {Abstracts} of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Park, Eunji and Kim, Hyunju and Lee, Byungjoo},
	month = apr,
	year = {2018},
	keywords = {button design, fitts' law, moving target selection, pointing, smart buttons, submovements, temporal pointing},
	pages = {1--6},
	file = {Park et al_2018_Button++.pdf:/Users/Vincent/Zotero/storage/LZ25T7IW/Park et al_2018_Button++.pdf:application/pdf},
}

@inproceedings{kim2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {Impact {Activation} {Improves} {Rapid} {Button} {Pressing}},
	isbn = {978-1-4503-5620-6},
	url = {https://doi.org/10.1145/3173574.3174145},
	doi = {10.1145/3173574.3174145},
	abstract = {The activation point of a button is defined as the depth at which it invokes a make signal. Regular buttons are activated during the downward stroke, which occurs within the first 20 ms of a press. The remaining portion, which can be as long as 80 ms, has not been examined for button activation for reason of mechanical limitations. The paper presents a technique and empirical evidence for an activation technique called Impact Activation, where the button is activated at its maximal impact point. We argue that this technique is advantageous particularly in rapid, repetitive button pressing, which is common in gaming and music applications. We report on a study of rapid button pressing, wherein users' timing accuracy improved significantly with use of Impact Activation. The technique can be implemented for modern push-buttons and capacitive sensors that generate a continuous signal.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Sunjun and Lee, Byungjoo and Oulasvirta, Antti},
	month = apr,
	year = {2018},
	keywords = {button design, temporal pointing, activation point, impact activation, rapid tapping},
	pages = {1--8},
	file = {Kim et al_2018_Impact Activation Improves Rapid Button Pressing.pdf:/Users/Vincent/Zotero/storage/VKYWPWPM/Kim et al_2018_Impact Activation Improves Rapid Button Pressing.pdf:application/pdf},
}

@inproceedings{oulasvirta2018,
	address = {Montreal QC Canada},
	title = {Neuromechanics of a {Button} {Press}},
	isbn = {978-1-4503-5620-6},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174082},
	doi = {10.1145/3173574.3174082},
	abstract = {To press a button, a ﬁnger must push down and pull up with the right force and timing. How the motor system succeeds in button-pressing, in spite of neural noise and lacking direct access to the mechanism of the button, is poorly understood. This paper investigates a unifying account based on neuromechanics. Mechanics is used to model muscles controlling the ﬁnger that contacts the button. Neurocognitive principles are used to model how the motor system learns appropriate muscle activations over repeated strokes though relying on degraded sensory feedback. Neuromechanical simulations yield a rich set of predictions for kinematics, dynamics, and user performance and may aid in understanding and improving input devices. We present a computational implementation and evaluate predictions for common button types.},
	language = {en},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Oulasvirta, Antti and Kim, Sunjun and Lee, Byungjoo},
	month = apr,
	year = {2018},
	pages = {1--13},
	file = {Oulasvirta et al_2018_Neuromechanics of a Button Press.pdf:/Users/Vincent/Zotero/storage/JCF6MGC9/Oulasvirta et al_2018_Neuromechanics of a Button Press.pdf:application/pdf},
}

@inproceedings{pohl2015,
	address = {New York, NY, USA},
	series = {{UbiComp} '15},
	title = {One-button recognizer: exploiting button pressing behavior for user differentiation},
	isbn = {978-1-4503-3574-4},
	shorttitle = {One-button recognizer},
	url = {https://doi.org/10.1145/2750858.2804270},
	doi = {10.1145/2750858.2804270},
	abstract = {We present a novel way to recognize users by the way they press a button. Our approach allows low-effort and fast interaction without the need for augmenting the user or controlling the environment. It eschews privacy concerns of methods such as fingerprint scanning. Button pressing behavior is sufficiently discriminative to allow distinguishing users within small groups. This approach combines recognition and action in a single step, e.g., getting and tallying a coffee can be done with one button press. We deployed our system for 5 users over a period of 4 weeks and achieved recognition rates of 95\% in the last week. We also ran a larger scale but short-term evaluation to investigate effects of group size and found that our method degrades gracefully for larger groups.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 2015 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Pohl, Henning and Krause, Markus and Rohs, Michael},
	month = sep,
	year = {2015},
	keywords = {adaptive user interfaces, context recognition, lightweight interaction, physical interaction, sensing, user recognition},
	pages = {403--407},
	file = {Pohl et al_2015_One-button recognizer.pdf:/Users/Vincent/Zotero/storage/GAXN5BJ3/Pohl et al_2015_One-button recognizer.pdf:application/pdf},
}

@inproceedings{sykes2003,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '03},
	title = {Affective gaming: measuring emotion through the gamepad},
	isbn = {978-1-58113-637-1},
	shorttitle = {Affective gaming},
	url = {https://doi.org/10.1145/765891.765957},
	doi = {10.1145/765891.765957},
	abstract = {In search of suitable methods for measuring the affective state of video-game players, this study investigates the hypothesis that the player's state of arousal will correspond with the pressure used to depress buttons on a gamepad. A video game was created that would detect the force of each button press during play. It was found that as the difficulty level of the game increased, players would hit the gamepad buttons significantly harder.},
	urldate = {2021-04-05},
	booktitle = {{CHI} '03 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sykes, Jonathan and Brown, Simon},
	month = apr,
	year = {2003},
	keywords = {emotion and affective UI, empirical methods, entertainment, input and Interaction technologies, quantitative},
	pages = {732--733},
	file = {Sykes_Brown_2003_Affective gaming.pdf:/Users/Vincent/Zotero/storage/HMVEB97R/Sykes_Brown_2003_Affective gaming.pdf:application/pdf},
}

@inproceedings{ramos2004,
	address = {New York, NY, USA},
	series = {{CHI} '04},
	title = {Pressure widgets},
	isbn = {978-1-58113-702-6},
	url = {https://doi.org/10.1145/985692.985754},
	doi = {10.1145/985692.985754},
	abstract = {Current user interface widgets typically assume that the input device can only provide x-y position and binary button press information. Other inputs such as the continuous pressure data provided by styluses on tablets are rarely used. We explore the design space of using the continuous pressure sensing capabilities of styluses to operate multi-state widgets. We present the results of a controlled experiment that investigates human ability to perform discrete target selection tasks by varying a stylus' pressure, with full or partial visual feedback. The experiment also considers different techniques for confirming selection once the target is acquired. Based on the experimental results, we discuss implications for the design of pressure sensitive widgets. A taxonomy of pressure widgets is presented, along with a set of initial concept sketches of various pressure widget designs.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ramos, Gonzalo and Boulos, Matthew and Balakrishnan, Ravin},
	month = apr,
	year = {2004},
	keywords = {isometric input, pen-based interfaces, pressure input, pressure widgets},
	pages = {487--494},
	file = {Ramos et al_2004_Pressure widgets.pdf:/Users/Vincent/Zotero/storage/W49NUTM7/Ramos et al_2004_Pressure widgets.pdf:application/pdf},
}

@inproceedings{liao2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Button {Simulation} and {Design} via {FDVV} {Models}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376262},
	doi = {10.1145/3313831.3376262},
	abstract = {Designing a push-button with desired sensation and performance is challenging because the mechanical construction must have the right response characteristics. Physical simulation of a button's force-displacement (FD) response has been studied to facilitate prototyping; however, the simulations' scope and realism have been limited. In this paper, we extend FD modeling to include vibration (V) and velocity-dependence characteristics (V). The resulting FDVV models better capture tactility characteristics of buttons, including snap. They increase the range of simulated buttons and the perceived realism relative to FD models. The paper also demonstrates methods for obtaining these models, editing them, and simulating accordingly. This end-to-end approach enables the analysis, prototyping, and optimization of buttons, and supports exploring designs that would be hard to implement mechanically.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Yi-Chi and Kim, Sunjun and Lee, Byungjoo and Oulasvirta, Antti},
	month = apr,
	year = {2020},
	keywords = {button, fd model, fdvv model, force feedback, haptic, haptic rendering, input device, modeling, simulation, tactility, vibration},
	pages = {1--14},
	file = {Liao et al_2020_Button Simulation and Design via FDVV Models.pdf:/Users/Vincent/Zotero/storage/B6FFTGHN/Liao et al_2020_Button Simulation and Design via FDVV Models.pdf:application/pdf},
}

@inproceedings{suh2017,
	address = {New York, NY, USA},
	series = {{TEI} '17},
	title = {Button+: {Supporting} {User} and {Context} {Aware} {Interaction} through {Shape}-{Changing} {Interfaces}},
	isbn = {978-1-4503-4676-4},
	shorttitle = {Button+},
	url = {https://doi.org/10.1145/3024969.3024980},
	doi = {10.1145/3024969.3024980},
	abstract = {Shape-changing interfaces are an emerging topic in HCI research: they merge the simplicity of tangible interfaces with the expressiveness of dynamic physical affordances. However, while prior work largely focused on technical aspects and proposed classifications of shape-changing interfaces based on the physical properties of the actuators and the user's levels of control, this work presents a classification of shape-changing interfaces based on the context and identity of the users. After introducing a new prototype for a shape-changing pushbutton, we conducted a series of workshop studies with designers and engineers to explore the design space and potential applications for this interface. We used the result of our workshops to propose a generalized taxonomy of interactions, and built two applications that reflect the proposed model. The paper concludes by highlighting future possible research directions for context and user aware shape-changing interfaces.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Suh, Jihoon and Kim, Wooshik and Bianchi, Andrea},
	month = mar,
	year = {2017},
	keywords = {button interface, context-aware, design, personalization, shape-changing interface},
	pages = {261--268},
	file = {Suh et al_2017_Button+.pdf:/Users/Vincent/Zotero/storage/BNPYTKPK/Suh et al_2017_Button+.pdf:application/pdf},
}

@inproceedings{harrison2009,
	address = {New York, NY, USA},
	series = {{CHI} '09},
	title = {Providing dynamically changeable physical buttons on a visual display},
	isbn = {978-1-60558-246-7},
	url = {https://doi.org/10.1145/1518701.1518749},
	doi = {10.1145/1518701.1518749},
	abstract = {Physical buttons have the unique ability to provide low-attention and vision-free interactions through their intuitive tactile clues. Unfortunately, the physicality of these interfaces makes them static, limiting the number and types of user interfaces they can support. On the other hand, touch screen technologies provide the ultimate interface flexibility, but offer no inherent tactile qualities. In this paper, we describe a technique that seeks to occupy the space between these two extremes - offering some of the flexibility of touch screens, while retaining the beneficial tactile properties of physical interfaces. The outcome of our investigations is a visual display that contains deformable areas, able to produce physical buttons and other interface elements. These tactile features can be dynamically brought into and out of the interface, and otherwise manipulated under program control. The surfaces we describe provide the full dynamics of a visual display (through rear projection) as well as allowing for multitouch input (though an infrared lighting and camera setup behind the display). To illustrate the tactile capabilities of the surfaces, we describe a number of variations we uncovered in our exploration and prototyping. These go beyond simple on/off actuation and can be combined to provide a range of different possible tactile expressions. A preliminary user study indicates that our dynamic buttons perform much like physical buttons in tactile search tasks.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Harrison, Chris and Hudson, Scott E.},
	month = apr,
	year = {2009},
	keywords = {haptic, barometric, dashboard, dynamic buttons, eyes-free, input, multitouch, physical interfaces, pneumatic, pressure, programmatically controlled, rear projection, shape displays, tactile},
	pages = {299--308},
	file = {Harrison_Hudson_2009_Providing dynamically changeable physical buttons on a visual display.pdf:/Users/Vincent/Zotero/storage/7LQ7WTXP/Harrison_Hudson_2009_Providing dynamically changeable physical buttons on a visual display.pdf:application/pdf},
}

@inproceedings{bah2008,
	address = {New York, NY, USA},
	series = {{CHI} '08},
	title = {You can touch, but you can't look: interacting with in-vehicle systems},
	isbn = {978-1-60558-011-1},
	shorttitle = {You can touch, but you can't look},
	url = {https://doi.org/10.1145/1357054.1357233},
	doi = {10.1145/1357054.1357233},
	abstract = {Car drivers are nowadays offered a wide array of in-vehicle systems i.e. route guidance systems, climate controls, music players. Such in-vehicle systems often require the driver's visual attention, but visual workload has shown significant less eyes-on-the-road time and affects driving performance. In this paper, we illustrate and compare three different interaction techniques for in-vehicle systems. We refer to them as tactile, touch, and gesture interaction. The focus of the techniques is the effects on drivers while driving cars. We evaluated the interaction techniques with 16 subjects in two settings. Our results showed that gesture interaction has a significant effect on the number of driver eye glances especially eye fixations of more seconds. However, gesture interaction still required rapid eye glances for hand/eye coordination. On the other hand, touch interaction leads to fast and efficient task completion while tactile interaction seemed inferior to the two other interaction techniques.},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ba h, Kenneth Majlund and Jæger, Mads Gregers and Skov, Mikael B. and Thomassen, Nils Gram},
	month = apr,
	year = {2008},
	keywords = {driving, eye glances, gesture interaction, in-vehicle systems, tactile interaction, touch interaction, visual attention},
	pages = {1139--1148},
	file = {Ba h et al_2008_You can touch, but you can't look.pdf:/Users/Vincent/Zotero/storage/AL8MSKJI/Ba h et al_2008_You can touch, but you can't look.pdf:application/pdf},
}

@article{mackinlay1990,
	title = {A semantic analysis of the design space of input devices},
	volume = {5},
	issn = {0737-0024},
	url = {https://doi.org/10.1207/s15327051hci0502%263_2},
	doi = {10.1207/s15327051hci0502%263_2},
	abstract = {A bewildering variety of devices for communication from humans to computers now exists on the market. In this article, we propose a descriptive framework for analyzing the design space of these input devices. We begin with Buxton's (1983) idea that input devices are transducers of physical properties in one, two, or three dimensions. Following Mackinlay's semantic analysis of the design space for graphical presentations, we extend this idea to more comprehensive descriptions of physical properties, space, and transducer mappings. In our reformulation, input devices are transducers of any combination of linear and rotary, absolute and relative, position and force, in any of the six spatial degrees of freedom. Simple input devices are described in terms of semantic mappings from the transducers of physical properties into the parameters of the applications. One of these mappings, the resolution function, allows us to describe the range of possibilities from continuous devices to discrete devices, including possibilities in between. Complex input controls are described in terms of hierarchical families of generic devices and in terms of composition operators on simpler devices. The description that emerges is used to produce a new taxonomy of input devices. The taxonomy is compared with previous taxonomies of Foley, Wallace, and Chan (1984) and of Buxton (1983) by reclassifying the devices previously analyzed by these authors. The descriptive techniques are further applied to the design of complex mouse-based virtual input controls for simulated three-dimensional (3D) egocentric motion. One result is the design of a new virtual egocentric motion control.},
	number = {2},
	urldate = {2021-04-05},
	journal = {Human-Computer Interaction},
	author = {Mackinlay, Jock and Card, Stuart K. and Robertson, George G.},
	month = jun,
	year = {1990},
	pages = {145--190},
}

@inproceedings{liao2018,
	address = {New York, NY, USA},
	series = {{UIST} '18 {Adjunct}},
	title = {One {Button} to {Rule} {Them} {All}: {Rendering} {Arbitrary} {Force}-{Displacement} {Curves}},
	isbn = {978-1-4503-5949-8},
	shorttitle = {One {Button} to {Rule} {Them} {All}},
	url = {https://doi.org/10.1145/3266037.3266118},
	doi = {10.1145/3266037.3266118},
	abstract = {Physical buttons provide rich force characteristics during the travel range, which are commonly described in the form of force-displacement curves. These force characteristics play an important role in the users' experiences while pressing a button. However, due to lack of proper tools to dynamically render various force-displacement curves, little literature has tried iterative button design improvement. This paper presents Button Simulator, a low-cost 3D printed physical button capable of displaying any force-displacement curves, with limited average error offset around .034 N. By reading the force-displacement curves of existing push-buttons, we can easily replicate the force characteristics from any buttons onto our Button Simulator. One can even go beyond existing buttons and design non-existent ones as the form of arbitrary force-displacement curves; then use Button Simulator to render the sensation. This project will be open-sourced and the implementation details will be released. Our system can be a useful tool for future researchers, designers, and makers to investigate rich and dynamic button"s force design.},
	urldate = {2021-04-05},
	booktitle = {The 31st {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology} {Adjunct} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Yi-Chi and Kim, Sunjun and Oulasvirta, Antti},
	month = oct,
	year = {2018},
	keywords = {button, force-displacement curve, input devices, input engineering},
	pages = {111--113},
	file = {Liao et al_2018_One Button to Rule Them All.pdf:/Users/Vincent/Zotero/storage/GWM9WMRG/Liao et al_2018_One Button to Rule Them All.pdf:application/pdf},
}

@inproceedings{zeleznik2001,
	address = {New York, NY, USA},
	series = {{UIST} '01},
	title = {Pop through mouse button interactions},
	isbn = {978-1-58113-438-4},
	url = {https://doi.org/10.1145/502348.502384},
	doi = {10.1145/502348.502384},
	abstract = {We present a range of novel interactions enabled by a simple modification in the design of a computer mouse. By converting each mouse button to pop through tactile push-buttons, similar to the focus/shutter-release buttons used in many cameras, users can feel, and the computer can sense, two distinct "clicks" corresponding to pressing lightly and pressing firmly to pop through. Despite the prototypical status of our hardware and software implementations, our current pop through mouse interactions are compelling and warrant further investigation. In particular, we demonstrate that pop through buttons not only yield an additional button activation state that is composable with, or even preferable to, techniques such as double-clicking, but also can endow a qualitatively novel user experience when meaningfully and consistently applied. We propose a number of software guidelines that may provide a consistent, systemic benefit; for example, light pressure may invoke default interaction (short menu), and firm pressure may supply more detail (long menu).},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the 14th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Zeleznik, Robert and Miller, Timothy and Forsberg, Andrew},
	month = nov,
	year = {2001},
	keywords = {input devices, buttons, click through, double-action, gesture, haptics, interaction, mouse, pop through},
	pages = {195--196},
	file = {Zeleznik et al_2001_Pop through mouse button interactions.pdf:/Users/Vincent/Zotero/storage/Z7NQWCRV/Zeleznik et al_2001_Pop through mouse button interactions.pdf:application/pdf},
}

@article{radwin1997,
	title = {Activation {Force} and {Travel} effects on {Overexertion} in {Repetitive} {Key} {Tapping}},
	volume = {39},
	issn = {0018-7208},
	url = {https://doi.org/10.1518/001872097778940605},
	doi = {10.1518/001872097778940605},
	abstract = {Key switch design parameters, including make force, make travel, and over travel, were investigated for minimizing operator-exerted force while maximizing key-tapping speed. A mechanical apparatus was designed, constructed, and used for independently controlling key switch parameters and for directly measuring finger exertions during repetitive key tapping using strain gauge load cells. The task for the 25 participants involved using the index finger of the dominant hand to repeatedly depress a single key as rapidly as possible. Participants received visual and auditory feedback upon a successful keystroke. Peak force exerted decreased 24\% and key-tapping rate increased 2\% when over travel was distended from 0.0 to 3.0 mm. Although peak force exerted was not significantly affected by make point travel, key-tapping rate increased 2\% when make point travel was reduced from 4.0 to 1.0 mm. These results indicate that key switch mechanisms that provide adequate over travel might enable operators to exert less force during repetitive key tapping without inhibiting performance},
	language = {en},
	number = {1},
	urldate = {2021-04-05},
	journal = {Human Factors},
	author = {Radwin, Robert G. and Jeng, One-Jang},
	month = mar,
	year = {1997},
	note = {Publisher: SAGE Publications Inc},
	pages = {130--140},
}

@article{rempel1994,
	title = {A method of measuring fingertip loading during keyboard use},
	volume = {27},
	issn = {0021-9290},
	url = {https://www.sciencedirect.com/science/article/pii/0021929094902275},
	doi = {10.1016/0021-9290(94)90227-5},
	abstract = {A single keycap on a standard alphanumeric computer keyboard was instrumented with a piezoelectric load cell and the fingertip motion was recorded with a high-speed video motion analysis system. Contact force histories between the fingertip and the keycap were recorded while four subjects typed a standard text for five minutes. Each keystroke force history is characterized by three distinct phases: (I) keyswitch compression, (II) finger impact and (III) fingertip pulp compression and release. Each keystroke force history contained two relative maxima, one in phase II and one in phase III. The subject mean peak forces ranged from 1.6 to 5.3 N and the subject mean peak fingertip velocities ranged from 0.3 to 0.7 m/s. Motion analyses and force measurements suggest a ballistic model of finger motion during typing.},
	language = {en},
	number = {8},
	urldate = {2021-04-05},
	journal = {Journal of Biomechanics},
	author = {Rempel, David and Dennerlein, Jack and Mote, C. D. and Armstrong, Thomas},
	month = aug,
	year = {1994},
	pages = {1101--1104},
	file = {Rempel et al_1994_A method of measuring fingertip loading during keyboard use.pdf:/Users/Vincent/Zotero/storage/V98BZK8R/Rempel et al_1994_A method of measuring fingertip loading during keyboard use.pdf:application/pdf},
}

@article{nagurka2005,
	title = {Measurement of {Stiffness} and {Damping} {Characteristics} of {Computer} {Keyboard} {Keys}},
	volume = {127},
	issn = {0022-0434},
	url = {https://doi.org/10.1115/1.1902823},
	doi = {10.1115/1.1902823},
	abstract = {To determine the stiffness and damping of computer keyboard keys, a computer-controlled test rig that can measure computer key displacement, velocity, and contact force has been designed. The test rig, consisting of a single-axis stage carrying a probe for contacting keys, has been used to collect contact force and motion data as computer keys are depressed and released at constant velocities up to 80mm∕s. Keys that employ a rubber-dome under their caps to achieve the necessary compliance and toggling action were tested. The results demonstrate a nonlinear stiffness force versus displacement characteristic at a given speed and the presence of damping-type forces that increase with key depression speed at a given displacement. In particular, the results indicate that the peak force at the 80mm∕s rate of depression increases relative to the quasistatic (0.5mm∕s) force level by over 12\% for the “Enter,” “K,” and “Spacebar” keys. This paper describes the hardware and software configuration, and presents sample results of the stiffness and damping characteristics of keys during depression-return stroke tests.},
	number = {2},
	urldate = {2021-04-05},
	journal = {Journal of Dynamic Systems, Measurement, and Control},
	author = {Nagurka, Mark and Marklin, Richard},
	month = jun,
	year = {2005},
	pages = {283--288},
	file = {Nagurka_Marklin_2005_Measurement of Stiffness and Damping Characteristics of Computer Keyboard Keys.pdf:/Users/Vincent/Zotero/storage/B52JQPXB/Nagurka_Marklin_2005_Measurement of Stiffness and Damping Characteristics of Computer Keyboard Keys.pdf:application/pdf;Snapshot:/Users/Vincent/Zotero/storage/WI2JSFBP/474365.html:text/html},
}

@incollection{lewis1997,
	address = {Amsterdam},
	title = {Chapter 54 - {Keys} and {Keyboards}},
	isbn = {978-0-444-81862-1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444818621501205},
	abstract = {Keyboards are in widespread use both on typewriters and as input devices to computers. Early refinements of the typewriter keyboard aimed at improving its mechanical action so that it would operate more smoothly with fewer malfunctions. Later, the work focused on improving typing speed and accuracy. This chapter describes keyboard design factors that affect skilled typing and data entry. The information presented should apply equally well to typewriter and computer keyboards. Some data also apply to telephones and other specialized keypads used for data entry tasks. Proponents of the best-publicized alternatives to the standard keyboard have generally failed to provide convincing empirical cases for their wholesale replacement of the standard, although they might see reasonable application in certain special settings. A well-designed standard keyboard is an extremely effective data-entry device and will probably remain a key component in human-computer interaction for the foreseeable future.},
	language = {en},
	urldate = {2021-04-05},
	booktitle = {Handbook of {Human}-{Computer} {Interaction} ({Second} {Edition})},
	publisher = {North-Holland},
	author = {Lewis, James R. and Potosnak, Kathleen M. and Magyar, Regis L.},
	editor = {Helander, Marting G. and Landauer, Thomas K. and Prabhu, Prasad V.},
	month = jan,
	year = {1997},
	doi = {10.1016/B978-044481862-1.50120-5},
	pages = {1285--1315},
	file = {ScienceDirect Snapshot:/Users/Vincent/Zotero/storage/63AF46N8/B9780444818621501205.html:text/html;Lewis et al_1997_Chapter 54 - Keys and Keyboards.pdf:/Users/Vincent/Zotero/storage/G3BT93GV/Lewis et al_1997_Chapter 54 - Keys and Keyboards.pdf:application/pdf},
}

@inproceedings{bailly2013,
	address = {Paris France},
	title = {Métamorphe: augmenting hotkey usage with actuated keys},
	isbn = {978-1-4503-1899-0},
	shorttitle = {Métamorphe},
	url = {https://dl.acm.org/doi/10.1145/2470654.2470734},
	doi = {10.1145/2470654.2470734},
	abstract = {Hotkeys are an efficient method of selecting commands on a keyboard. However, these shortcuts are often underused by users. We present Métamorphe, a novel keyboard with keys that can be individually raised and lowered to promote hotkeys usage. Métamorphe augments the output of traditional keyboards with haptic and visual feedback, and offers a novel design space for user input on raised keys (e.g., gestures such as squeezing or pushing the sides of a key). We detail the implementation of Métamorphe and discuss design factors. We also report two user studies. The first is a user-defined interface study that shows that the new input vocabulary is usable and useful, and provides insights into the mental models that users associate with raised keys. The second user study shows improved eyesfree selection performance for raised keys as well as the surrounding unraised keys.},
	language = {en},
	urldate = {2021-04-05},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Bailly, Gilles and Pietrzak, Thomas and Deber, Jonathan and Wigdor, Daniel J.},
	month = apr,
	year = {2013},
	pages = {563--572},
	file = {Bailly et al_2013_Métamorphe.pdf:/Users/Vincent/Zotero/storage/N7MG2NKG/Bailly et al_2013_Métamorphe.pdf:application/pdf},
}

@article{jindrich2004,
	title = {Effects of keyswitch design and finger posture on finger joint kinematics and dynamics during tapping on computer keyswitches},
	volume = {19},
	issn = {0268-0033},
	url = {https://www.sciencedirect.com/science/article/pii/S026800330400052X},
	doi = {10.1016/j.clinbiomech.2004.03.003},
	abstract = {Objective. To examine the effects of postural and keyswitch characteristics on musculoskeletal tissue loading during tapping on computer keyswitches. Design. We hypothesized that joint torques, stiffness and work parameters differ across keyswitch designs and finger postures typical of those observed during computer keyboard typing. We experimentally measured joint kinematics and calculated joint torques while tapping on different keyswitches in different postures, and analyzed the data using mechanical impedance models. Methods. Sixteen human subjects tapped with the index finger on computer keyswitches mounted on a sensor which measured vertical and horizontal forces. Miniature electro-optical goniometers mounted dorsally across each finger joint measured joint kinematics. Joint torques were calculated from endpoint forces and joint kinematics using an inverse dynamics algorithm. A linear spring-damper impedance model was fitted to joint torque, position, and velocity during the contact period of each tap. Subjects tapped in three postures approximating those employed during tapping on three rows of a computer keyboard, on four different keyswitches, resulting in 12 conditions. Results. More extended finger posture was associated with greater joint torques, energies, and stiffnesses, despite minimal differences in endpoint forces across posture. Greater keyswitch make forces were associated with increased forces, joint torques and joint stiffnesses, however this relationship was not monotonic. Conclusions. Joint torques and stiffness parameters differed across keyswitch designs and finger postures. Estimates of joint impedance and work provided a unique perspective into finger dynamics. Relevance Determining the causes of work-related musculoskeletal disorders is facilitated by characterizing workplace task biomechanics, which can be linked to specific injury mechanisms.},
	language = {en},
	number = {6},
	urldate = {2021-04-05},
	journal = {Clinical Biomechanics},
	author = {Jindrich, Devin L and Balakrishnan, Aruna D and Dennerlein, Jack T},
	month = jul,
	year = {2004},
	keywords = {Finger, Impact, Joint stiffness, Keyswitch, Mechanical properties, Posture, Typing},
	pages = {600--608},
	file = {Jindrich et al_2004_Effects of keyswitch design and finger posture on finger joint kinematics and.pdf:/Users/Vincent/Zotero/storage/TAMTJGTL/Jindrich et al_2004_Effects of keyswitch design and finger posture on finger joint kinematics and.pdf:application/pdf},
}

@article{akagi1992,
	title = {A {Computer} {Keyboard} {Key} {Feel} {Study} in {Performance} and {Preference}},
	volume = {36},
	issn = {0163-5182},
	url = {https://doi.org/10.1177/154193129203600511},
	doi = {10.1177/154193129203600511},
	abstract = {A study was conducted to compare user preference and performance of four keyboards having different key force and travel characteristics. Two keyboards had linear spring key action, one with low (key force) resistance (42.5 grams) and one with high resistance (70.9 grams). The other two keyboards had tactile (snap) action, one with low resistance (35.5 grams) and one with high resistance (70.9 grams). All four keyboards were manufactured by the same company, and were visually identical in size, layout, color, etc. There was no difference in typing sound and traveling distance among the four keyboards. Twenty four touch typists typed material taken from a college psychology textbook for seven to eight minutes on each keyboard. Between changing keyboards, the participants rested one minute. The two low resistance (35.5 grams tactile, 42.5 grams linear spring action) keyboards produced 23.3\% more errors (57\% of total errors) than the two high resistance keyboards (43\% of total errors). There was a 10.28\% difference in errors between the low resistance spring and the low tactile action keyboards, and there was only a 2.95\% difference in error between the high resistance spring and the high resistance tactile action keyboards. The lighter the key resistance, the more errors were produced. The average typing speed of all of the participants indicated that there was no significant typing speed difference among the four keyboards. The keyboards preferred by the participants were almost evenly distributed among the low resistance linear (42.5 grams, 29\% of participants), the low resistance tactile (35.5 grams, 29\% of participants) and the high resistance tactile (70.9 grams, 25\% of participants) keyboards. The high resistance linear (70.9 grams, 17\% of participants) keyboard was chosen least.},
	language = {en},
	number = {5},
	urldate = {2021-04-05},
	journal = {Proceedings of the Human Factors Society Annual Meeting},
	author = {Akagi, Kenichi},
	month = oct,
	year = {1992},
	note = {Publisher: SAGE Publications},
	pages = {523--527},
	file = {Akagi_1992_A Computer Keyboard Key Feel Study in Performance and Preference.pdf:/Users/Vincent/Zotero/storage/5JU74HNL/Akagi_1992_A Computer Keyboard Key Feel Study in Performance and Preference.pdf:application/pdf},
}

@inproceedings{assayag2006,
	address = {Santa Barbara, United States},
	title = {{OMAX} {Brothers}: {A} {Dynamic} {Topology} of {Agents} for {Improvisation} {Learning}},
	shorttitle = {{OMAX} {Brothers}},
	url = {https://hal.inria.fr/hal-00839075},
	abstract = {no abstract},
	urldate = {2021-04-05},
	booktitle = {{ACM} {Multimedia} {Workshop} on {Audio} and {Music} {Computing} for {Multimedia}},
	publisher = {Santa Barbara},
	author = {Assayag, Gerard and Bloch, George and Chemillier, Marc and Cont, Arshia and Dubnov, Shlomo},
	year = {2006},
	file = {HAL PDF Full Text:/Users/Vincent/Zotero/storage/TB2DXWUP/Assayag et al. - 2006 - OMAX Brothers A Dynamic Topology of Agents for Im.pdf:application/pdf},
}

@article{nika2017a,
	title = {{ImproteK}: introducing scenarios into human-computer music improvisation},
	shorttitle = {{ImproteK}},
	url = {https://hal.archives-ouvertes.fr/hal-01380163},
	doi = {10.1145/3022635},
	abstract = {This article focuses on the introduction of control, authoring, and composition in human-computer music improvisation through the description of a guided music generation model and a reactive architecture, both implemented in the software ImproteK. This interactive music system is used with expert improvisers in work sessions and performances of idiomatic and pulsed music, and more broadly in situations of structured or composed improvisation. The article deals with the integration of specifications in the music generation process by means of a fixed or dynamic "scenario", and addresses the issue of the dialectic between reactivity and planning in interactive music improvisation. It covers the different levels involved in the machine improvisation: the integration of anticipation relative to a predefined structure in a guided generation process at a symbolic level, an architecture combining this anticipation with reactivity using mixed static/dynamic scheduling techniques, and an audio rendering module performing live re-injection of captured material in synchrony with a non-metronomic beat. Finally, it sketches a framework to compose improvisation sessions at the scenario level, extending the initial musical scope of the system. All these points are illustrated by videos of performances or work sessions with musicians.},
	urldate = {2021-04-05},
	journal = {ACM Computers in Entertainment},
	author = {Nika, Jérôme and Chemillier, Marc and Assayag, Gérard},
	month = jan,
	year = {2017},
	keywords = {Abstraction, Combinatorics on words, Heuristic function construction, Intelligent agents, Markov processes, modeling and modularity, Modeling methodologies, Motif discovery, Music retrieval, Performing arts, Planning under uncertainty, Real-time system architecture, Sound and music computing, Symbolic and algebraic algorithms},
	file = {HAL PDF Full Text:/Users/Vincent/Zotero/storage/QZZY8PXL/Nika et al. - 2017 - ImproteK introducing scenarios into human-compute.pdf:application/pdf},
}

@misc{by2021,
	title = {{3D}-{Printed} {Macro} {Pad} {Ditches} {The} {PCB} {With} {Slick} {Wiring} {Guides}},
	url = {https://hackaday.com/2021/03/02/3d-printed-macro-pad-ditches-the-pcb-with-slick-wiring-guides/},
	abstract = {Reddit user [duzitbetter] showed off their design for a 3D-printed programmable macro keyboard that offers a different take on what can be thought of as a sort of 3D-printed PCB. The design is call…},
	language = {en-US},
	urldate = {2021-04-05},
	journal = {Hackaday},
	author = {By},
	month = mar,
	year = {2021},
	file = {Snapshot:/Users/Vincent/Zotero/storage/GXILIQHL/3d-printed-macro-pad-ditches-the-pcb-with-slick-wiring-guides.html:text/html},
}

@book{2007,
	title = {A {Novel} {Input} {Device} {For} {Thumb} {Control}},
	abstract = {This thesis project consists of developing a hand-held, one-thumb input device small enough to be placed on gestural controllers and providing performers with a simple way to select between multiple options with few movements of a thumb. Most commercial input devices for the thumb make the assumption that buttons are the de facto standard. However, buttons are not always the best solution especially where space is very limited. This project explores a new approach to input design by analyzing the thumb’s movements and designing a suitable input device that can track them. The design progression from the first to the last prototype is presented in great detail. The final prototype is compared to other existing one-thumb input devices used in gestural controllers and mobile computing. The shortcomings of each one-thumb input method is described with the final prototype presented as a possible solution in each case. ii Resumé Cette thèse présente le développement d’une interface de contrôle spécifique pour le pouce. Elle et destinée à être montée sur un controleur gestuel pour rendre possible une selection},
	year = {2007},
	file = {2007_A Novel Input Device For Thumb Control.pdf:/Users/Vincent/Zotero/storage/KGGYKK8M/2007_A Novel Input Device For Thumb Control.pdf:application/pdf},
}

@inproceedings{fornari2017,
	title = {An {Algorithm} for {Guiding} {Expectation} in {Free} {Improvisational} {Solo} {Performances}},
	abstract = {Free improvisation lets a performer to openly explore musical outcomes unbounded by any structure or notation. However, the human mind is naturally constrained by its own built-in habits. As such, musicians usually develop, during years of practice and aesthetic predilections, a repertoire of known musical patterns which are intentionally or even unconsciously used by them during musical improvisation. 
This work presents an algorithm that aims to retrieve in real-time similarities during sessions of free improvisations, This is done in order to inform the musician during performance which ones might be the most expected musical outcomes by the listeners.},
	author = {Fornari, Jose and Schaub, Stéphan},
	month = nov,
	year = {2017},
	file = {Full Text PDF:/Users/Vincent/Zotero/storage/X98CDFG3/Fornari and Schaub - 2017 - An Algorithm for Guiding Expectation in Free Impro.pdf:application/pdf},
}

@inproceedings{nakagaki2019,
	address = {Tempe Arizona USA},
	title = {{inFORCE}: {Bi}-directional `{Force}' {Shape} {Display} for {Haptic} {Interaction}},
	isbn = {978-1-4503-6196-5},
	shorttitle = {{inFORCE}},
	url = {https://dl.acm.org/doi/10.1145/3294109.3295621},
	doi = {10.1145/3294109.3295621},
	abstract = {While previously proposed hardware on pin-based shape display has improved various technical aspects, there has been a clear limitation on the haptic quality of variable ‘force’ feedback. In this paper, we explore a novel haptic interaction design space with ‘force’ controlled shape display. Utilizing high performance linear actuators with current reading functionality, we built a 10 x 5 ‘force’ shape display, named inFORCE, that can both detect and exert variable force on individual pins. By integrating closed-loop force control, our system can provide real-time variable haptic feedback in response to the way users press the pins. Our haptic interaction design space includes volumetric haptic feedback, material emulation, layer snapping, and friction. Our proposed interaction methods, for example, enables people to “press through” computationally rendered dynamic shapes to understand the internal structure of 3D volumetric information. We also demonstrate a material property capturing functionality. Our technical evaluation and user study assesses the hardware capability and haptic perception through interaction with inFORCE. We also discuss application spaces that ‘force’ shape display can be used for.},
	language = {en},
	urldate = {2021-04-08},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Tangible}, {Embedded}, and {Embodied} {Interaction}},
	publisher = {ACM},
	author = {Nakagaki, Ken and Fitzgerald, Daniel and Ma, Zhiyao (John) and Vink, Luke and Levine, Daniel and Ishii, Hiroshi},
	month = mar,
	year = {2019},
	pages = {615--623},
	file = {Nakagaki et al_2019_inFORCE.pdf:/Users/Vincent/Zotero/storage/TFQD7523/Nakagaki et al_2019_inFORCE.pdf:application/pdf},
}

@article{hoffding2021,
	title = {Interactive expertise in solo and joint musical performance},
	volume = {198},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-019-02339-x},
	doi = {10.1007/s11229-019-02339-x},
	abstract = {The paper presents two empirical cases of expert musicians—a classical string quartet and a solo, free improvisation saxophonist—to analyze the explanatory power and reach of theories in the field of expertise studies and joint action. We argue that neither the positions stressing top-down capacities of prediction, planning or perspective-taking, nor those emphasizing bottom-up embodied processes of entrainment, motor-responses and emotional sharing can do justice to the empirical material. We then turn to hybrid theories in the expertise debate and interactionist accounts of cognition. Attempting to strengthen and extend them, we offer ‘Arch’: an overarching conception of musical interaction as an externalized, cognitive scaffold that encompasses high and low-level cognition, internal and external processes, as well as the shared normative space including the musical materials in which the musicians perform. In other words, ‘Arch’ proposes interaction as a multivariate multimodal overarching scaffold necessary to explain not only cases of joint performance, but equally of solo improvisation.},
	language = {en},
	number = {1},
	urldate = {2021-04-09},
	journal = {Synthese},
	author = {Høffding, Simon and Satne, Glenda},
	month = jan,
	year = {2021},
	pages = {427--445},
	file = {Submitted Version:/Users/Vincent/Zotero/storage/XQ68JXJ5/Høffding and Satne - 2021 - Interactive expertise in solo and joint musical pe.pdf:application/pdf},
}

@inproceedings{fujii2012,
	title = {Autonomously {Acquiring} a {Video} {Game} {Agent}’s {Behavior}: {Letting} {Players} {Feel} {Like} {Playing} with a {Human} {Player}},
	volume = {7624},
	isbn = {978-3-642-34291-2},
	shorttitle = {Autonomously {Acquiring} a {Video} {Game} {Agent}’s {Behavior}},
	doi = {10.1007/978-3-642-34292-9_42},
	abstract = {Designing behavior patterns of video game agents (COM players) is a crucial aspect of video game development. While various systems aiming to automatically acquire behavior patterns has been proposed and some have successfully obtained stronger patterns than human players, the obtained behavior patterns looks mechanical. We present herein an autonomous acquisition of video game agent behavior, which emulates the behavior of a human player. Instead of implementing straightforward heuristics, the behavior is acquired using Q-learning, a reinforcement learning, where, biological constraints are imposed. In the experiments using Infinite Mario Bros., we observe that behaviors that imply human behaviors are obtained by imposing sensory error, perceptual and motion delay, and fatigue as biological constraints.},
	author = {Fujii, Nobuto and Sato, Yuichi and Wakama, Hironori and Katayose, Haruhiro},
	month = nov,
	year = {2012},
	pages = {490--493},
}

@inproceedings{mccormack2019,
	title = {In a {Silent} {Way}: {Communication} {Between} {AI} and {Improvising} {Musicians} {Beyond} {Sound}},
	shorttitle = {In a {Silent} {Way}},
	doi = {10.1145/3290605.3300268},
	abstract = {Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.},
	author = {McCormack, Jon and Gifford, Toby and Hutchings, Patrick and Llano, Maria and Yee-King, Matthew and dʼInverno, Mark},
	month = apr,
	year = {2019},
	pages = {1--11},
	file = {Full Text PDF:/Users/Vincent/Zotero/storage/DBS4D5VK/McCormack et al. - 2019 - In a Silent Way Communication Between AI and Impr.pdf:application/pdf},
}

@book{pachet,
	title = {Chapter {N}+1: {Enhancing} {Individual} {Creativity} with {Interactive} {Musical} {Reflective} {Systems}},
	shorttitle = {Chapter {N}+1},
	author = {Pachet, François},
	file = {Citeseer - Full Text PDF:/Users/Vincent/Zotero/storage/JUYZVHSP/Pachet - Chapter N+1 Enhancing Individual Creativity with .pdf:application/pdf;Citeseer - Snapshot:/Users/Vincent/Zotero/storage/7LI8FBD6/download.html:text/html},
}

@article{gifford2018,
	title = {Computational systems for music improvisation},
	volume = {29},
	issn = {1462-6268},
	url = {https://doi.org/10.1080/14626268.2018.1426613},
	doi = {10.1080/14626268.2018.1426613},
	abstract = {Computational music systems that afford improvised creative interaction in real time are often designed for a specific improviser and performance style. As such the field is diverse, fragmented and lacks a coherent framework. Through analysis of examples in the field, we identify key areas of concern in the design of new systems, which we use as categories in the construction of a taxonomy. From our broad overview of the field, we select significant examples to analyse in greater depth. This analysis serves to derive principles that may aid designers scaffold their work on existing innovation. We explore successful evaluation techniques from other fields and describe how they may be applied to iterative design processes for improvisational systems. We hope that by developing a more coherent design and evaluation process, we can support the next generation of improvisational music systems.},
	number = {1},
	urldate = {2021-04-09},
	journal = {Digital Creativity},
	author = {Gifford, Toby and Knotts, Shelly and McCormack, Jon and Kalonaris, Stefano and Yee-King, Matthew and d’Inverno, Mark},
	month = jan,
	year = {2018},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/14626268.2018.1426613},
	keywords = {computational creativity, creative agency, evaluation, generative music, Improvisational interfaces},
	pages = {19--36},
	file = {Snapshot:/Users/Vincent/Zotero/storage/8HIYA4D3/14626268.2018.html:text/html;Accepted Version:/Users/Vincent/Zotero/storage/4IIE4GBG/Gifford et al. - 2018 - Computational systems for music improvisation.pdf:application/pdf},
}

@inproceedings{mccormack2016a,
	title = {Designing {Improvisational} {Interfaces}},
	url = {https://research.monash.edu/en/publications/designing-improvisational-interfaces},
	language = {English},
	urldate = {2021-04-09},
	booktitle = {Proceedings of the {Seventh} {International} {Conference} on {Computational} {Creativity}},
	publisher = {Sony CSL},
	author = {McCormack, Jon and d'Inverno, Mark},
	month = jul,
	year = {2016},
	pages = {98--105},
	file = {Snapshot:/Users/Vincent/Zotero/storage/V7QZZXV5/designing-improvisational-interfaces.html:text/html},
}

@article{mccormack2016,
	title = {Designing {Improvisational} {Interfaces}},
	abstract = {This paper examines the possibilities for creative interaction with computers, in particular modes of interaction based on improvisation and spontaneous creative discovery. We consider research ﬁndings from studies in psychology that investigate how humans improvise together to see what could be useful in helping us to design systems that provide new kinds of interactive opportunities. We draw on our personal experiences both as computer scientists working across art and music, and as practicing artists and musicians, to examine what artists and musicians would want in any system designed to support creative interaction with a computer. We bring together these different ﬁndings to propose a series of working principles which form a basis for designing systems that facilitate collaboration and improvisation with computers in creative domains.},
	language = {en},
	author = {McCormack, Jon},
	year = {2016},
	pages = {8},
	file = {McCormack - 2016 - Designing Improvisational Interfaces.pdf:/Users/Vincent/Zotero/storage/CE5MLJ7K/McCormack - 2016 - Designing Improvisational Interfaces.pdf:application/pdf},
}

@incollection{blackwell2012,
	address = {Berlin, Heidelberg},
	title = {Live {Algorithms}: {Towards} {Autonomous} {Computer} {Improvisers}},
	isbn = {978-3-642-31726-2 978-3-642-31727-9},
	shorttitle = {Live {Algorithms}},
	url = {http://link.springer.com/10.1007/978-3-642-31727-9_6},
	abstract = {A Live Algorithm is an autonomous machine that interacts with musicians in an improvised setting. This chapter outlines perspectives on Live Algorithm research, offering a high level view for the general reader, as well as more detailed and specialist analysis. The study of Live Algorithms is multi-disciplinary in nature, requiring insights from (at least) Music Technology, Artiﬁcial Intelligence, Cognitive Science, Musicology and Performance Studies. Some of the most important issues from these ﬁelds are considered. A modular decomposition and an associated set of wiring diagrams is offered as a practical and conceptual tool. Technical, behavioural, social and cultural contexts are considered, and some signposts for future Live Algorithm research are suggested.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {Computers and {Creativity}},
	publisher = {Springer Berlin Heidelberg},
	author = {Blackwell, Tim and Bown, Oliver and Young, Michael},
	editor = {McCormack, Jon and d’Inverno, Mark},
	year = {2012},
	doi = {10.1007/978-3-642-31727-9_6},
	pages = {147--174},
	file = {Blackwell et al. - 2012 - Live Algorithms Towards Autonomous Computer Impro.pdf:/Users/Vincent/Zotero/storage/KQ9N7AMW/Blackwell et al. - 2012 - Live Algorithms Towards Autonomous Computer Impro.pdf:application/pdf},
}

@inproceedings{pachet2013,
	address = {New York, NY, USA},
	series = {{CHI} '13},
	title = {Reflexive loopers for solo musical improvisation},
	isbn = {978-1-4503-1899-0},
	url = {https://doi.org/10.1145/2470654.2481303},
	doi = {10.1145/2470654.2481303},
	abstract = {Loop pedals are real-time samplers that playback audio played previously by a musician. Such pedals are routinely used for music practice or outdoor "busking". However, loop pedals always playback the same material, which can make performances monotonous and boring both to the musician and the audience, preventing their widespread uptake in professional concerts. In response, we propose a new approach to loop pedals that addresses this issue, which is based on an analytical multi-modal representation of the audio input. Instead of simply playing back prerecorded audio, our system enables real-time generation of an audio accompaniment reacting to what is currently being performed by the musician. By combining different modes of performance - e.g. bass line, chords, solo - from the musician and system automatically, solo musicians can perform duets or trios with themselves, without engendering the so-called canned (boringly repetitive and unresponsive) music effect of loop pedals. We describe the technology, based on supervised classification and concatenative synthesis, and then illustrate our approach on solo performances of jazz standards by guitar. We claim this approach opens up new avenues for concert performance.},
	urldate = {2021-04-08},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pachet, François and Roy, Pierre and Moreira, Julian and d'Inverno, Mark},
	month = apr,
	year = {2013},
	keywords = {classification, loop pedals, music interaction, synthesis},
	pages = {2205--2208},
	file = {Full Text:/Users/Vincent/Zotero/storage/ICHCTJSP/Pachet et al. - 2013 - Reflexive loopers for solo musical improvisation.pdf:application/pdf},
}

@inproceedings{hamanaka2003,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'03},
	title = {A learning-based jam session system that imitates a player's personality model},
	abstract = {This paper describes a jam session system that enables a human player to interplay with virtual players which can imitate the player personality models of various human players. Previous systems have parameters that allow some alteration in the way virtual players react, but these systems cannot imitate human personalities. Our system can obtain three kinds of player personality models from a MIDI recording of a session in which that player participated - a reaction model, a phrase model, and a groove model. The reaction model is the characteristic way that a player reacts to other players, and it can be statistically learned from the relationship between the MIDI data of music the player listens to and the MIDI data of music improvised by that player. The phrase model is a set of player's characteristic phrases; it can be acquired through musical segmentation of a MIDI session recording by using Voronoi diagrams on a piano-roll. The groove model is a model that generates onset time deviation; it can be acquired by using a hidden Markov model. Experimental results show that the personality models of any player participating in a guitar trio session can be derived from a MIDI recording of that session.},
	urldate = {2021-04-08},
	booktitle = {Proceedings of the 18th international joint conference on {Artificial} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hamanaka, Masatoshi and Goto, Masataka and Asoh, Hideki and Otsu, Nobuyuki},
	month = aug,
	year = {2003},
	pages = {51--58},
}

@book{stowell,
	title = {Evaluation of live human-computer music-making: quantitative and qualitative approaches},
	shorttitle = {Evaluation of live human-computer music-making},
	abstract = {Live music-making using interactive systems is not completely amenable to traditional HCI evaluation metrics such as taskcompletion rates. In this paper we discuss quantitative and qualitative approaches which provide opportunities to evaluate the music-making interaction, accounting for aspects which cannot be directly measured or expressed numerically, yet which may be important for participants. We present case studies in the application of a qualitative method based on Discourse Analysis, and a quantitative method based on the Turing Test. We compare and contrast these methods with each other, and with other evaluation approaches used in the literature, and discuss factors affecting which evaluation methods are appropriate in a given context. Key words: Music, qualitative, quantitative 1.},
	author = {Stowell, D. and Robertson, A. and Bryan-kinns, N. and Plumbley, M. D.},
	file = {Citeseer - Snapshot:/Users/Vincent/Zotero/storage/JFZSY4JY/download.html:text/html;Citeseer - Full Text PDF:/Users/Vincent/Zotero/storage/KXC7EJLM/Stowell et al. - Evaluation of live human-computer music-making qu.pdf:application/pdf},
}

@inproceedings{thom2001,
	title = {Machine {Learning} {Techniques} for {Real}-time {Improvisational} {Solo} {Trading}},
	abstract = {This paper introduces a new melody generation scheme that enables customized interaction between a live, improvising musician and the computer. This scheme provides a method for integrating a model that was learned to describe the user's tonal, melodic-interval and -contour trends into a stochastic process that, when sampled, produces sequences that exhibit similar trends while also seamlessly integrating with the local environment. This algorithm's musical performance is evaluated both quantitatively, using traditional machine learning techniques, and qualitatively, exploring its behavior in the context of Bebop saxophonist Charlie Parker.},
	booktitle = {In {Proceedings} of {IS} 2001},
	author = {Thom, Belinda},
	year = {2001},
	file = {Citeseer - Snapshot:/Users/Vincent/Zotero/storage/JQ4WBMB2/summary.html:text/html;Citeseer - Full Text PDF:/Users/Vincent/Zotero/storage/MNDLGX6P/Thom - 2001 - Machine Learning Techniques for Real-time Improvis.pdf:application/pdf},
}

@incollection{marshall2006,
	address = {Berlin, Heidelberg},
	title = {Evaluation of {Sensors} as {Input} {Devices} for {Computer} {Music} {Interfaces}},
	volume = {3902},
	isbn = {978-3-540-34027-0 978-3-540-34028-7},
	url = {http://link.springer.com/10.1007/11751069_12},
	abstract = {This paper presents ongoing research into the design and creation of interfaces for computer music. This work concentrates on the use of sensor as the primary means of interaction for computer music, and examines the relationships between types of sensors and musical functions. Experiments are described which aim to discover the particular suitability of certain sensors for speciﬁc musical tasks. The eﬀects of additional visual feedback on the perceived suitability of these sensors is also examined. Results are given, along with a discussion of their possible implications for computer music interface design and pointers for further work on this topic.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {Computer {Music} {Modeling} and {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Marshall, Mark T. and Wanderley, Marcelo M.},
	editor = {Kronland-Martinet, Richard and Voinier, Thierry and Ystad, Sølvi},
	year = {2006},
	doi = {10.1007/11751069_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {130--139},
	file = {Marshall_Wanderley_2006_Evaluation of Sensors as Input Devices for Computer Music Interfaces.pdf:/Users/Vincent/Zotero/storage/G6DLFTNE/Marshall_Wanderley_2006_Evaluation of Sensors as Input Devices for Computer Music Interfaces.pdf:application/pdf},
}

@article{vertegaal,
	title = {Towards a {Musician}’s {Cockpit}: {Transducers}, {Feedback} and {Musical} {Function}},
	abstract = {This paper describes our on-going theoretical research into the design of computer music instruments. Traditionally, an acoustical instrument is used as an extension of the body. We feel the importance of this tight relationship between musician and instrument has traditionally been underestimated in the design of computer music controllers. In this paper, we will show the benefits of a cybernetic approach to the man-instrument system, in which we specify relationships between body-parts, different types of transducers, feedback, and the musical function performed. Matching these different requirements is a first step into bringing computer music instruments into reach of the skilled musician.},
	language = {en},
	author = {Vertegaal, Roel and Ungvary, Tamas and Kieslinger, Michael},
	pages = {5},
	file = {Vertegaal et al_Towards a Musician’s Cockpit.pdf:/Users/Vincent/Zotero/storage/J2TV36VA/Vertegaal et al_Towards a Musician’s Cockpit.pdf:application/pdf},
}

@misc{zotero-364,
	title = {Touché: {Enhancing} {Touch} {Interaction} on {Humans}, {Screens}, {Liquids}, and {Everyday} {Objects} – {Disney} {Research}},
	url = {https://la.disneyresearch.com/publication/touche-enhancing-touch-interaction-on-humans-screens-liquids-and-everyday-objects/},
	urldate = {2021-04-09},
	file = {Touché\: Enhancing Touch Interaction on Humans, Screens, Liquids, and Everyday Objects – Disney Research:/Users/Vincent/Zotero/storage/PHI3AC4M/touche-enhancing-touch-interaction-on-humans-screens-liquids-and-everyday-objects.html:text/html},
}

@inproceedings{hoffmann2009a,
	address = {New York, NY, USA},
	series = {{CHI} '09},
	title = {{TypeRight}: a keyboard with tactile error prevention},
	isbn = {978-1-60558-246-7},
	shorttitle = {{TypeRight}},
	url = {https://doi.org/10.1145/1518701.1519048},
	doi = {10.1145/1518701.1519048},
	abstract = {TypeRight is a new tactile input device for text entry. It combines the advantages of tactile feedback with error prevention methods of word processors. TypeRight extends the standard keyboard so that the resistance to press each key becomes dynamically adjustable through software. Before each keystroke, the resistance of keys that would lead to a typing error according to dictionary and grammar rules is increased momentarily to make them harder to press, thus avoiding typing errors rather than indicating them after the fact. Two user studies showed that TypeRight decreases error correction rates by an average of 46\%.},
	urldate = {2021-04-09},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Hoffmann, Alexander and Spelmezan, Daniel and Borchers, Jan},
	month = apr,
	year = {2009},
	keywords = {haptic, adopting input device, error prevention, tactile feedback, text entry},
	pages = {2265--2268},
}

@inproceedings{hoffmann2009,
	address = {Boston, MA, USA},
	title = {{TypeRight}: a keyboard with tactile error prevention},
	isbn = {978-1-60558-246-7},
	shorttitle = {{TypeRight}},
	url = {http://dl.acm.org/citation.cfm?doid=1518701.1519048},
	doi = {10.1145/1518701.1519048},
	abstract = {TYPERIGHT is a new tactile input device for text entry. It combines the advantages of tactile feedback with error prevention methods of word processors. TYPERIGHT extends the standard keyboard so that the resistance to press each key becomes dynamically adjustable through software. Before each keystroke, the resistance of keys that would lead to a typing error according to dictionary and grammar rules is increased momentarily to make them harder to press, thus avoiding typing errors rather than indicating them after the fact. Two user studies showed that TYPERIGHT decreases error correction rates by an average of 46\%.},
	language = {en},
	urldate = {2021-04-09},
	booktitle = {Proceedings of the 27th international conference on {Human} factors in computing systems - {CHI} 09},
	publisher = {ACM Press},
	author = {Hoffmann, Alexander and Spelmezan, Daniel and Borchers, Jan},
	year = {2009},
	pages = {2265},
	file = {Hoffmann et al_2009_TypeRight.pdf:/Users/Vincent/Zotero/storage/UEUFLBUX/Hoffmann et al_2009_TypeRight.pdf:application/pdf},
}

@inproceedings{frisson2020,
	address = {Montreal, Canada},
	title = {Printgets: an {Open}-{Source} {Toolbox} for {Designing} {Vibrotactile} {Widgets} with {Industrial}-{Grade} {Printed} {Actuators} and {Sensors}},
	shorttitle = {Printgets},
	url = {https://hal.archives-ouvertes.fr/hal-02901202},
	abstract = {New technologies for printing sensors and actuators combine the flexibility of interface layouts of touchscreens with localized vibrotactile feedback, but their fabrication still requires industrial-grade facilities. Until these technologies become easily replicable, interaction designers need material for ideation. We propose an open-source hardware and software toolbox providing maker-grade tools for iterative design of vibrotactile widgets with industrial-grade printed sensors and actuators. Our hardware toolbox provides a mechanical structure to clamp and stretch printed sheets, and electronic boards to drive sensors and actuators. Our software toolbox expands the design space of haptic interaction techniques by reusing the wide palette of available audio processing algorithms to generate real-time vibrotactile signals. We validate our toolbox with the implementation of three exemplar interface elements with tactile feedback: buttons, sliders, touchpads.},
	urldate = {2021-04-10},
	booktitle = {{HAID} 2020 - {International} {Workshop} on {Haptic} and {Audio} {Interaction} {Design}},
	author = {Frisson, Christian and Decaudin, Julien and Sanz-Lopez, Mario and Pietrzak, Thomas},
	month = aug,
	year = {2020},
	file = {Frisson et al_2020_Printgets.pdf:/Users/Vincent/Zotero/storage/8JLYG9ML/Frisson et al_2020_Printgets.pdf:application/pdf},
}

@inproceedings{lylykangas2011,
	address = {New York, NY, USA},
	series = {{CHI} '11},
	title = {Designing tactile feedback for piezo buttons},
	isbn = {978-1-4503-0228-9},
	url = {https://doi.org/10.1145/1978942.1979428},
	doi = {10.1145/1978942.1979428},
	abstract = {The present aim was to study the preference of tactile feedback stimulations given by non-physical (i.e., solid) piezo-actuated buttons. Participants (n=16) ranked 16 different tactile feedback stimuli varied by 4 output delays and 4 vibration durations. The results showed that the mean ranks of the stimuli differed significantly from each other. The timing parameters of delay and duration interacted with each other, for example, so that preference of certain vibration duration fluctuated in response to different output delays. Using a very short time window (i.e., 10-453 ms) combining both delay and duration parameters of the feedback could result either in favorable or significantly less favorable subjective experience. The results suggest that a preferred perception of tactile feedback from non-physical buttons requires careful design and controlling of the timing parameters.},
	urldate = {2021-04-10},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lylykangas, Jani and Surakka, Veikko and Salminen, Katri and Raisamo, Jukka and Laitinen, Pauli and Rönning, Kasper and Raisamo, Roope},
	month = may,
	year = {2011},
	keywords = {haptics, tactile feedback, delay, interaction design, lag, non-physical buttons, piezo-electric},
	pages = {3281--3284},
}

@misc{zotero-403,
	title = {feedback killer - {MaxMSP} {Forum} {\textbar} {Cycling} '74},
	url = {https://cycling74.com/forums/feedback-killer},
	abstract = {Hello,
Did anyone try to program a feedback killer with max? Actually, I'm doing a piece with real-time processing with 5 simple delays ...},
	language = {en},
	urldate = {2021-04-14},
	file = {Snapshot:/Users/Vincent/Zotero/storage/LKVMGJ5F/feedback-killer.html:text/html},
}

@phdthesis{sullivan2021,
	type = {Ph.{D}. thesis},
	title = {Built to perform: {Designing} digital musical instruments for professional use},
	url = {https://johnnyvenom.com/files/sullivan_phd_thesis.pdf},
	school = {McGill University},
	author = {Sullivan, John},
	year = {2021},
	file = {Sullivan_2021_Built to perform.pdf:/Users/Vincent/Zotero/storage/FIU5KL9G/Sullivan_2021_Built to perform.pdf:application/pdf},
}

@incollection{fiebrink2017,
	address = {Singapore},
	title = {Machine {Learning} as {Meta}-{Instrument}: {Human}-{Machine} {Partnerships} {Shaping} {Expressive} {Instrumental} {Creation}},
	isbn = {978-981-10-2951-6},
	shorttitle = {Machine {Learning} as {Meta}-{Instrument}},
	url = {https://doi.org/10.1007/978-981-10-2951-6_10},
	abstract = {In this chapter, I describe how supervised learning algorithms can be used to build new digital musical instruments. Rather than merely serving as methods for inferring mathematical relationships from data, I show how these algorithms can be understood as valuable design tools that support embodied, real-time, creative practices. Through this discussion, I argue that the relationship between instrument builders and instrument creation tools warrants closer consideration: the affordances of a creation tool shape the musical potential of the instruments that are built, as well as the experiences and even the creative aims of the human builder. Understanding creation tools as “instruments” themselves invites us to examine them from perspectives informed by past work on performer-instrument interactions.},
	language = {en},
	urldate = {2021-05-10},
	booktitle = {Musical {Instruments} in the 21st {Century}: {Identities}, {Configurations}, {Practices}},
	publisher = {Springer},
	author = {Fiebrink, Rebecca},
	editor = {Bovermann, Till and de Campo, Alberto and Egermann, Hauke and Hardjowirogo, Sarah-Indriyati and Weinzierl, Stefan},
	year = {2017},
	doi = {10.1007/978-981-10-2951-6_10},
	keywords = {Creation Tool, Machine Learning, Musical Instrument, Sound Synthesis, Supervise Learning Algorithm},
	pages = {137--151},
	file = {Fiebrink_2017_Machine Learning as Meta-Instrument.pdf:/Users/Vincent/Zotero/storage/KW8NURTJ/Fiebrink_2017_Machine Learning as Meta-Instrument.pdf:application/pdf},
}

@article{bastien1992,
	title = {Cooperation as communicative accomplishment: {A} symbolic interaction analysis of an improvised {Jazz} {Concert}},
	volume = {43},
	doi = {10.1080/10510979209368363},
	abstract = {Weick (1983) suggests that understanding prototypical cases of organizational communication increases our understanding beyond the event itself. In Weick (1990) he further argues that the jazz orchestra is one such case. Following Weick's call, this is a study of a jazz concert performed by a zero-history group of seasoned professionals. The performance was videotaped, and one of the musicians acted as a participant-informant, explaining instant-by-instant interpretive processes through the course of the concert. The two forms of data, videotape and participant-informant, are analyzed in light of Couch's (1986) theory of elementary forms and Miller, Hintz, and Couch's (1975) theory of the elements of sociation.},
	language = {English},
	number = {2},
	journal = {Communication Studies},
	author = {Bastien, David T. and {Todd J. Hostager}},
	year = {1992},
	note = {Publisher: Routledge},
	pages = {92--104},
	file = {Bastien and Hostager - Cooperation as communicative accomplishment A sym:/Users/Vincent/Documents/zoteroCollections/littReview/files/9217/Bastien and Hostager - Cooperation as communicative accomplishment A sym.pdf:application/pdf},
}

@article{bown2015,
	title = {Player {Responses} to a {Live} {Algorithm}: {Conceptualising} computational creativity without recourse to human comparisons?},
	abstract = {Live algorithms are computational systems made to perform in an improvised manner with human improvising musicians, typically using only live audio or MIDI streams as the medium of interaction. They are designed to establish meaningful musical interaction with their musical partners, without necessarily being conceived of as “virtual musicians”. This paper investigates, with respect to a specific live algorithm designed by the author, how improvising musicians approach and discuss performing with that system.},
	language = {English},
	author = {Bown, Oliver},
	year = {2015},
	pages = {8},
	file = {Bown - 2015 - Player Responses to a Live Algorithm Conceptualis:/Users/Vincent/Documents/zoteroCollections/littReview/files/146/Bown - 2015 - Player Responses to a Live Algorithm Conceptualis.pdf:application/pdf},
}

@article{bown2018,
	title = {Performer interaction and expectation with live algorithms: {Experiences} with {Zamyatin}},
	volume = {29},
	issn = {1462-6268},
	shorttitle = {Performer interaction and expectation with live algorithms},
	doi = {10.1080/14626268.2018.1432663},
	abstract = {This paper builds on previous work into the development of a live improvising software system, Zamyatin, which is designed to perform in duets with improvising musicians. The research looks further into the issue of how performers conceptualise and approach working with the system. It takes the form of a detailed interview and analysis of the musical performance of one musician with the system, framed according to theoretical issues of the analysis and perception of agency, and the notion of how we approach different systems in the world according to Dennett's notion of `stances' and principles of embodied cognition. The results of this study further support the idea that musicians maintain an ambiguous stance towards improvising software systems that do include concepts of autonomy, but do not depend on comparisons to human behaviour.},
	number = {1},
	journal = {Digital Creativity},
	author = {Bown, Oliver},
	month = jan,
	year = {2018},
	keywords = {autonomous music systems, Live algorithms, musical agency, musical metacreation},
	pages = {37--50},
	file = {Bown - 2018 - Performer interaction and expectation with live al:/Users/Vincent/Documents/zoteroCollections/littReview/files/332/Bown - 2018 - Performer interaction and expectation with live al.pdf:application/pdf},
}

@article{brown2018a,
	title = {Creative improvisation with a reflexive musical bot},
	volume = {29},
	issn = {1462-6268, 1744-3806},
	doi = {10.1080/14626268.2017.1419979},
	abstract = {This paper discusses improvisatory musical interactions between a musician and a machine. The focus is on duet performances, in which a human pianist and the Controlling Interactive Music (CIM) software system both perform on mechanized pianos. It also discusses improvisatory behaviours, using reflexive strategies in machines, and describes interfaces for musical communication and control between human and machine performers. Results are derived from trials with six expert improvising musicians using CIM. Analysis reveals that creative partnerships are fostered by several factors. The reflexive generative system provides aesthetic cohesion by ensuring that generated material has a direct relationship to that played by the musician. The interaction design relies on musical communication through performance as the primary mechanism for feedback and control. It can be shown that his approach to musical human-machine improvisation allows technical concerns to fall away from the musician's awareness and attention to shift to the musical dialogue within the duet.},
	language = {English},
	number = {1},
	journal = {Digital Creativity},
	author = {Brown, Andrew R.},
	month = jan,
	year = {2018},
	keywords = {performance, interaction, generative, computer, improvization, Music},
	pages = {5--18},
	file = {Brown - 2018 - Creative improvisation with a reflexive musical bo:/Users/Vincent/Documents/zoteroCollections/littReview/files/57/Brown - 2018 - Creative improvisation with a reflexive musical bo.pdf:application/pdf},
}

@inproceedings{brown2020a,
	address = {New York, NY, USA},
	series = {{AM} '20},
	title = {Was that me? {Exploring} the effects of error in gestural digital musical instruments},
	isbn = {978-1-4503-7563-4},
	shorttitle = {Was that me?},
	doi = {10.1145/3411109.3411137},
	abstract = {Traditional Western musical instruments have evolved to be robust and predictable, responding consistently to the same player actions with the same musical response. Consequently, errors occurring in a performance scenario are typically attributed to the performer and thus a hallmark of musical accomplishment is a flawless musical rendition. Digital musical instruments often increase the potential for a second type of error as a result of technological failure within one or more components of the instrument. Gestural instruments using machine learning can be particularly susceptible to these types of error as recognition accuracy often falls short of 100\%, making errors a familiar feature of gestural music performances. In this paper we refer to these technology-related errors as system errors, which can be difficult for players and audiences to disambiguate from performer errors. We conduct a pilot study in which participants repeat a note selection task in the presence of simulated system errors. The results suggest that, for the gestural music system under study, controlled increases in system error correspond to an increase in the occurrence and severity of performer error. Furthermore, we find the system errors reduce a performer's sense of control and result in the instrument being perceived as less accurate and less responsive.},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Audio} {Mostly}},
	publisher = {Association for Computing Machinery},
	author = {Brown, Dom and Nash, Chris and Mitchell, Thomas J.},
	month = sep,
	year = {2020},
	keywords = {augmented reality, game audio, musicology, sonic interaction design, sonification, sound art, spatial audio, virtual reality},
	pages = {168--174},
	file = {Brown et al. - 2020 - Was that me exploring the effects of error in ges:/Users/Vincent/Documents/zoteroCollections/littReview/files/509/Brown et al. - 2020 - Was that me exploring the effects of error in ges.pdf:application/pdf},
}

@inproceedings{davis2017,
	title = {Creative {Sense}-{Making}: {Quantifying} {Interaction} {Dynamics} in {Co}-{Creation}},
	shorttitle = {Creative {Sense}-{Making}},
	doi = {10.1145/3059454.3059478},
	abstract = {This paper describes a new technique for quantifying interaction dynamics during open-ended co-creation, such as collaborative drawing or playing pretend. We present a cognitive framework called creative sense-making. This framework synthesizes existing cognitive science theories and empirical investigations into open-ended improvisation to develop a method of quantifying cognitive states and types of interactions through time. We apply this framework to empirical studies of human collaboration (in the domain of pretend play) and AI-based systems (in the domain of collaborative drawing) to establish its validity through cross-domain application and inter-rater reliability within each domain. The creative sense-making framework described includes a qualitative coding technique, interaction coding software, and the cognitive theory behind their application.},
	author = {Davis, Nicholas and Hsiao, Chih-Pin and Singh, Kunwar and Lin, Brenda and Magerko, Brian},
	month = jun,
	year = {2017},
	pages = {356--366},
	file = {Davis et al. - 2017 - Creative Sense-Making Quantifying Interaction Dyn:/Users/Vincent/Documents/zoteroCollections/littReview/files/257/Davis et al. - 2017 - Creative Sense-Making Quantifying Interaction Dyn.pdf:application/pdf},
}

@article{demos2017,
	title = {The unresponsive partner: {Roles} of social status, auditory feedback, and animacy in coordination of joint music performance},
	doi = {10.3389/fpsyg.2017.00149},
	journal = {Frontiers in Psychology},
	author = {Demos, Alexander P. and Carter, Daniel J. and Wanderley, Marcelo M. and Palmer, Caroline},
	year = {2017},
	file = {Demos et al. - 2017 - The unresponsive partner Roles of social status,:/Users/Vincent/Documents/zoteroCollections/littReview/files/536/Demos et al. - 2017 - The unresponsive partner Roles of social status, .pdf:application/pdf},
}

@inproceedings{weinberg2005,
	address = {Vancouver, BC, Canada},
	title = {Iltur – {Connecting} novices and experts through collaborative improvisation},
	doi = {10.5281/zenodo.1176840},
	abstract = {The iltur system features a novel method of interaction between expert and novice musicians through a set of musical controllers called Beatbugs. Beatbug players can record live musical input from MIDI and acoustic instruments and respond by transforming the recorded material in real-time, creating motif-and-variation call-and-response routines on the fly. A central computer system analyzes MIDI and audio played by expert players and allows novice Beatbug players to personalize the analyzed material using a variety of transformation algorithms. This paper presents the motivation for developing the iltur system, followed by a brief survey of pervious and related work that guided the definition of the project's goals. We then present the hardware and software approaches that were taken to address these goals, as well as a couple of compositions that were written for the system. The paper ends with a discussion based on observations of players using the iltur system and a number of suggestions for future work.},
	booktitle = {Proceedings of the international conference on new interfaces for musical expression},
	author = {Weinberg, Gil and Driscoll, Scott},
	year = {2005},
	note = {ISSN: 2220-4806},
	keywords = {improvisation, mapping, Collaboration, novices, gestrual handheld controllers},
	pages = {17--22},
	file = {Weinberg and Driscoll - 2005 - iltur – Connecting novices and experts through col:/Users/Vincent/Documents/zoteroCollections/littReview/files/739/Weinberg and Driscoll - 2005 - iltur – Connecting novices and experts through col.pdf:application/pdf},
}

@article{csikszentmihalyi2000,
	title = {{FLOW}: {The} {Psychology} of {Optimal} {Experience}},
	language = {en},
	author = {Csikszentmihalyi, Mihaly},
	year = {2000},
	pages = {6},
	file = {Csikszentmihalyi - 2000 - FLOW The Psychology of Optimal Experience.pdf:/Users/Vincent/Zotero/storage/TFJJDT5Y/Csikszentmihalyi - 2000 - FLOW The Psychology of Optimal Experience.pdf:application/pdf},
}

@article{eigenfeldt2013a,
	title = {Towards a {Taxonomy} of {Musical} {Metacreation}: {Reflections} on the {First} {Musical} {Metacreation} {Weekend}},
	volume = {9},
	copyright = {Copyright (c)},
	issn = {2334-0924},
	shorttitle = {Towards a {Taxonomy} of {Musical} {Metacreation}},
	url = {https://ojs.aaai.org/index.php/AIIDE/article/view/12647},
	language = {en},
	number = {1},
	urldate = {2021-05-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
	author = {Eigenfeldt, Arne and Bown, Oliver and Pasquier, Philippe and Martin, Aengus},
	month = nov,
	year = {2013},
	note = {Number: 1},
	keywords = {computer music},
	file = {Eigenfeldt et al_2013_Towards a Taxonomy of Musical Metacreation.pdf:/Users/Vincent/Zotero/storage/ZJF7PH47/Eigenfeldt et al_2013_Towards a Taxonomy of Musical Metacreation.pdf:application/pdf},
}

@book{yee-king2011,
	title = {An autonomous timbre matching improviser},
	abstract = {A new autonomous musical agent is described which is intended to improvise alongside a human instrumentalist. The system collects high level, symbolic information about the musical behaviour of the human player such as note sequences and rhythmic patters. It also extracts tim-bral trajectories in MFCC space. It produces permutations of this information using a sound synthesizer which it automatically programs to achieve tone matching. To match tones in real time a genetic algorithm and a data driven approach have been implemented. The genetic algorithm requires an offline phase to generate a bank of settings for real time tone matching but achieves closer tone matches. The data driven approach organises a large random set of sounds from the synthesizer into hierarchical clusters which can be searched in real time for matches. Evaluation of the system is ongoing.},
	author = {Yee-King, Matthew},
	month = jul,
	year = {2011},
}

@inproceedings{ravikumar2017,
	address = {Singapore Singapore},
	title = {Notational {Communication} with {Co}-creative {Systems}: {Studying} {Improvements} to {Musical} {Coordination}},
	isbn = {978-1-4503-4403-6},
	shorttitle = {Notational {Communication} with {Co}-creative {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3059454.3078702},
	doi = {10.1145/3059454.3078702},
	abstract = {This research examines the impact of differences in extramusical notational communication on a musician’s co-creative experience with a music improvisation system. The method involves the development of musical improvisation agent technologies that support mixed-initiative communication and the use of mixed methods for studying the musician’s co-creative engagement. The expected contributions are the ﬁndings about the impact of differences in extra-musical communication on the co-creative engagement with coimprovisation systems.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {Proceedings of the 2017 {ACM} {SIGCHI} {Conference} on {Creativity} and {Cognition}},
	publisher = {ACM},
	author = {Ravikumar, Prashanth Thattai},
	month = jun,
	year = {2017},
	pages = {518--523},
	file = {Ravikumar - 2017 - Notational Communication with Co-creative Systems.pdf:/Users/Vincent/Zotero/storage/ITI9XC8F/Ravikumar - 2017 - Notational Communication with Co-creative Systems.pdf:application/pdf},
}

@article{vanderheiden2020,
	title = {The influence of cognitive load on susceptibility to audio},
	volume = {205},
	doi = {10.1016/j.actpsy.2020.103058},
	abstract = {In this study we evaluate how cognitive load affects susceptibility to auditory signals. Previous research has used the frontal P3 (fP3) event related potential response to auditory novel stimuli as an index for susceptibility to auditory signals. This work demonstrated that tasks that induce cognitive load such as visual and manual tasks, reduced susceptibility. It is however unknown whether cognitive load without visual or manual components also reduces susceptibility. To investigate this, we induced cognitive load by means of the verb generation task, in which participants need to think about a verb that matches a noun. The susceptibility to auditory signals was measured by recording the event related potential in response to a successively presented oddball probe stimulus at 3 different inter-stimulus intervals, 0 ms, 200 ms or 400 ms after the offset of the noun from the verb generation task. An additional control baseline condition, in which oddball response was probed without a verb generation task, was also included. Results show that the cognitive load associated with the verb task reduces fP3 response (and associated auditory signal susceptibility) compared to baseline, independent of presentation interval. This suggests that not only visual and motor processing, but also cognitive load without visual or manual components, can reduce susceptibility to auditory signals and alerts.},
	journal = {Acta Psychologica},
	author = {van der Heiden, Remo and Janssen, Christian and Donker, Stella and Kenemans, J.},
	month = apr,
	year = {2020},
	pages = {103058},
	file = {Full Text:/Users/Vincent/Zotero/storage/5H3724T7/van der Heiden et al. - 2020 - The influence of cognitive load on susceptibility .pdf:application/pdf},
}

@article{tatar2019,
	title = {Musical agents: {A} typology and state of the art towards {Musical} {Metacreation}},
	volume = {48},
	issn = {0929-8215},
	shorttitle = {Musical agents},
	url = {https://doi.org/10.1080/09298215.2018.1511736},
	doi = {10.1080/09298215.2018.1511736},
	abstract = {Musical agents are artificial agents that tackle musical creative tasks, partially or completely. This review of musical agents combines the terminology of Generative Arts (artistic practice) and the scientific literature of Computational Creativity, Multi-Agent Systems (MAS), and Artificial Intelligence. We define Musical Metacreation as a field that studies the partial or complete automation of musical tasks. We survey seventy-eight musical agent systems, and present a typology of musical agents. After examining the evaluation methodologies of musical agents, we propose possible future steps while mentioning ongoing discussions in the field.},
	number = {1},
	urldate = {2021-05-27},
	journal = {Journal of New Music Research},
	author = {Tatar, Kıvanç and Pasquier, Philippe},
	month = jan,
	year = {2019},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09298215.2018.1511736},
	keywords = {Artificial Intelligence, Computational Creativity, Correction, Multi-Agent Systems, Musical agents, Musical Metacreation},
	pages = {56--105},
	file = {Full Text PDF:/Users/Vincent/Zotero/storage/EES8TYG6/Tatar and Pasquier - 2019 - Musical agents A typology and state of the art to.pdf:application/pdf;Snapshot:/Users/Vincent/Zotero/storage/H3TI9WL5/09298215.2018.html:text/html},
}

@article{winston2017,
	title = {Turn-{Taking} with {Improvisational} {Co}-{Creative} {Agents}},
	volume = {13},
	copyright = {Copyright (c)},
	issn = {2334-0924},
	url = {https://ojs.aaai.org/index.php/AIIDE/article/view/12931},
	language = {en},
	number = {1},
	urldate = {2021-05-27},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
	author = {Winston, Lauren and Magerko, Brian},
	month = sep,
	year = {2017},
	note = {Number: 1},
	keywords = {intelligent agents},
	file = {Full Text PDF:/Users/Vincent/Zotero/storage/B7QEEYVW/Winston and Magerko - 2017 - Turn-Taking with Improvisational Co-Creative Agent.pdf:application/pdf;Snapshot:/Users/Vincent/Zotero/storage/PKS9SFY7/12931.html:text/html},
}

@article{jensenius,
	title = {Chapter 2. {Musical} gestures: concepts and methods in research},
	language = {en},
	author = {Jensenius, Alexander Refsum and Wanderley, Marcelo M and Godøy, Rolf Inge and Leman, Marc},
	pages = {22},
	file = {Jensenius et al. - Chapter 2. Musical gestures concepts and methods .pdf:/Users/Vincent/Zotero/storage/LQPY87G3/Jensenius et al. - Chapter 2. Musical gestures concepts and methods .pdf:application/pdf},
}

@inproceedings{sullivan2018b,
	address = {Genoa Italy},
	title = {Gestural {Control} of {Augmented} {Instrumental} {Performance}: {A} {Case} {Study} of the {Concert} {Harp}},
	isbn = {978-1-4503-6504-8},
	shorttitle = {Gestural {Control} of {Augmented} {Instrumental} {Performance}},
	url = {https://dl.acm.org/doi/10.1145/3212721.3212814},
	doi = {10.1145/3212721.3212814},
	abstract = {We present a gestural control system to augment harp performance with real-time control of computer-based audio affects and processing. While the lightweight system was designed for use alongside any instrument, our choice of the concert harp represented a unique case study in gestural control of music. The instrument’s large size and physically demanding playing technique leaves the performer with little spare bandwidth to devote to other tasks. A motion capture study analyzed instrumental and ancillary gestures of natural harp performances that could be mapped effectively to control of additional signal processing parameters. The initial findings of the study helped to guide the design of custom gesture control devices and user software, and a new work for solo harpist and electronics was created. We discuss our findings, successes and challenges in the study and design of gesture control for augmented instrumental performance, with particular focus on the concert harp.},
	language = {en},
	urldate = {2021-07-28},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Movement} and {Computing}},
	publisher = {ACM},
	author = {Sullivan, John and Tibbitts, Alexandra and Gatinet, Brice and Wanderley, Marcelo M.},
	month = jun,
	year = {2018},
	pages = {1--8},
	file = {Sullivan et al. - 2018 - Gestural Control of Augmented Instrumental Perform.pdf:/Users/Vincent/Zotero/storage/AWXZ9IYX/Sullivan et al. - 2018 - Gestural Control of Augmented Instrumental Perform.pdf:application/pdf},
}

@article{bishop2018,
	title = {Collaborative {Musical} {Creativity}: {How} {Ensembles} {Coordinate} {Spontaneity}},
	volume = {9},
	issn = {1664-1078},
	shorttitle = {Collaborative {Musical} {Creativity}},
	doi = {10.3389/fpsyg.2018.01285},
	abstract = {Music performance is inherently social. Most music is performed in groups, and even soloists are subject to influence from a (real or imagined) audience. It is also inherently creative. Performers are called upon to interpret notated music, improvise new musical material, adapt to unexpected playing conditions, and accommodate technical errors. The focus of this paper is how creativity is distributed across members of a music ensemble as they perform these tasks. Some aspects of ensemble performance have been investigated extensively in recent years as part of the broader literature on joint action (e.g., the processes underlying sensorimotor synchronization). Much of this research has been done under highly controlled conditions, using tasks that generate reliable results, but capture only a small part of ensemble performance as it occurs naturalistically. Still missing from this literature is an explanation of how ensemble musicians perform in conditions that require creative interpretation, improvisation, and/or adaptation: how do they coordinate the production of something new? Current theories of creativity endorse the idea that dynamic interaction between individuals, their actions, and their social and material environments underlies creative performance. This framework is much in line with the embodied music cognition paradigm and the dynamical systems perspective on ensemble coordination. This review begins by situating the concept of collaborative musical creativity in the context of embodiment. Progress that has been made toward identifying the mechanisms that underlie collaborative creativity in music performance is then assessed. The focus is on the possible role of musical imagination in facilitating performer flexibility, and on the forms of communication that are likely to support the coordination of creative musical output. Next, emergence and group flow-constructs that seem to characterize ensemble performance at its peak-are considered, and some of the conditions that may encourage periods of emergence or flow are identified. Finally, it is argued that further research is needed to (1) demystify the constructs of emergence and group flow, clarifying their effects on performer experience and listener response, (2) determine how constrained musical imagination is by perceptual experience and understand people's capacity to depart from familiar frameworks and imagine new sounds and sound structures, and (3) assess the technological developments that are supposed to facilitate or enhance musical creativity, and determine what effect they have on the processes underlying creative collaboration.},
	language = {eng},
	journal = {Frontiers in Psychology},
	author = {Bishop, Laura},
	year = {2018},
	pmid = {30087645},
	pmcid = {PMC6066987},
	keywords = {creativity, embodiment, communication, emergence, ensemble performance, mental imagery},
	pages = {1285},
	file = {Bishop_2018_Collaborative Musical Creativity.pdf:/Users/Vincent/Zotero/storage/WW2KPY5D/Bishop_2018_Collaborative Musical Creativity.pdf:application/pdf},
}

@article{horwitz2020,
	title = {The {Interplay} {Between} {Chamber} {Musicians} {During} {Two} {Public} {Performances} of the {Same} {Piece}: {A} {Novel} {Methodology} {Using} the {Concept} of "{Flow}"},
	volume = {11},
	issn = {1664-1078},
	shorttitle = {The {Interplay} {Between} {Chamber} {Musicians} {During} {Two} {Public} {Performances} of the {Same} {Piece}},
	doi = {10.3389/fpsyg.2020.618227},
	abstract = {The purpose of the study is to explore a new research methodology that will improve our understanding of "flow" through indicators of physiological and qualitative state. We examine indicators of "flow" experienced by musicians of a youth string quartet, two women (25, 29) and two men (23, 24). Electrocardiogram (ECG) equipment was used to record heart rate variability (HRV) data throughout the four movements in one and the same quartet performed during two concerts. Individual physiological indicators of flow were supplemented by assessments of group "state flow" (means from standardized questionnaires) and a group interview in which the musicians provided qualitative data. A matrix was constructed for the characterization of different kinds of demands in the written music in each one of the four movements for each one of the musicians. HRV derived from ECG data showed non-significant trends for group state flow across the eight musical episodes. Individual-level analysis showed that compared to the other players the first violin player had the highest mean heart rate and the lowest increase in high frequency (HF) power in HRV during this particular movement, particularly during the second concert. The qualitative data illustrated how an interplay of synchronized social interactions between this player and their colleagues during the musical performance was associated with a feeling of group state flow and served to support the first violinist. The case illustrates that the proposed mixed methodology drawing on physiological and qualitative data, has the potential to provide meaningful information about experiences of a flow state, both at individual and group levels. Applications in future research are possible.},
	language = {eng},
	journal = {Frontiers in Psychology},
	author = {Horwitz, Eva Bojner and Harmat, László and Osika, Walter and Theorell, Töres},
	year = {2020},
	pmid = {33488486},
	pmcid = {PMC7815933},
	keywords = {flow, heart rate variability, interpersonal interaction, musical performance, synchronization},
	pages = {618227},
	file = {Horwitz et al_2020_The Interplay Between Chamber Musicians During Two Public Performances of the.pdf:/Users/Vincent/Zotero/storage/Q2JAQRMH/Horwitz et al_2020_The Interplay Between Chamber Musicians During Two Public Performances of the.pdf:application/pdf},
}

@article{bishop2019,
	title = {Eye gaze as a means of giving and seeking information during musical interaction},
	volume = {68},
	issn = {1090-2376},
	doi = {10.1016/j.concog.2019.01.002},
	abstract = {During skilled music ensemble performance, a multi-layered network of interaction processes allows musicians to negotiate common interpretations of ambiguously-notated music in real-time. This study investigated the conditions that encourage visual interaction during duo performance. Duos recorded performances of a new piece before and after a period of rehearsal. Mobile eye tracking and motion capture were used in combination to map uni- and bidirectional eye gaze patterns. Musicians watched each other more during temporally-unstable passages than during regularly-timed passages. They also watched each other more after rehearsal than before. Duo musicians may seek visual interaction with each other primarily, but not exclusively, when coordination is threatened by temporal instability. Visual interaction increases as musicians become familiar with the piece, suggesting that they visually monitor each other once a shared interpretation of the piece is established. Visual monitoring of co-performers' movements and attention may facilitate feelings of engagement and high-level creative collaboration.},
	language = {eng},
	journal = {Consciousness and Cognition},
	author = {Bishop, Laura and Cancino-Chacón, Carlos and Goebl, Werner},
	month = feb,
	year = {2019},
	pmid = {30660927},
	pmcid = {PMC6374286},
	keywords = {Music, Adult, Attention, Auditory Perception, Body gestures, Communication, Cooperative Behavior, Coordination, Eye gaze, Eye Movement Measurements, Female, Fixation, Ocular, Gestures, Humans, Interpersonal Relations, Joint action, Male, Visual interaction, Visual Perception},
	pages = {73--96},
	file = {Bishop et al_2019_Eye gaze as a means of giving and seeking information during musical interaction.pdf:/Users/Vincent/Zotero/storage/PAHKXDC9/Bishop et al_2019_Eye gaze as a means of giving and seeking information during musical interaction.pdf:application/pdf},
}

@article{damato2020,
	title = {Understanding {Violin} {Players}’ {Skill} {Level} {Based} on {Motion} {Capture}: a {Data}-{Driven} {Perspective}},
	volume = {12},
	issn = {1866-9964},
	shorttitle = {Understanding {Violin} {Players}’ {Skill} {Level} {Based} on {Motion} {Capture}},
	url = {https://doi.org/10.1007/s12559-020-09768-8},
	doi = {10.1007/s12559-020-09768-8},
	abstract = {Learning to play and perform a music instrument is a complex cognitive task, requiring high conscious control and coordination of an impressive number of cognitive and sensorimotor skills. For professional violinists, there exists a physical connection with the instrument allowing the player to continuously manage the sound through sophisticated bowing techniques and fine hand movements. Hence, it is not surprising that great importance in violin training is given to right hand techniques, responsible for most of the sound produced. In this paper, our aim is to understand which motion features can be used to efficiently and effectively distinguish a professional performance from that of a student without exploiting sound-based features. We collected and made freely available a dataset consisting of motion capture recordings of different violinists with different skills performing different exercises covering different pedagogical and technical aspects. We then engineered peculiar features and trained a data-driven classifier to distinguish among two different levels of violinist experience, namely beginners and experts. In accordance with the hierarchy present in the dataset, we study two different scenarios: extrapolation with respect to different exercises and violinists. Furthermore, we study which features are the most predictive ones of the quality of a violinist to corroborate the significance of the results. The results, both in terms of accuracy and insight on the cognitive problem, support the proposal and support the use of the proposed technique as a support tool for students to monitor and enhance their home study and practice.},
	language = {en},
	number = {6},
	urldate = {2021-07-28},
	journal = {Cognitive Computation},
	author = {D’Amato, Vincenzo and Volta, Erica and Oneto, Luca and Volpe, Gualtiero and Camurri, Antonio and Anguita, Davide},
	month = nov,
	year = {2020},
	pages = {1356--1369},
	file = {D’Amato et al_2020_Understanding Violin Players’ Skill Level Based on Motion Capture.pdf:/Users/Vincent/Zotero/storage/SJJVP7Q3/D’Amato et al_2020_Understanding Violin Players’ Skill Level Based on Motion Capture.pdf:application/pdf},
}

@article{mchugh2021,
	title = {Optical motion capture accuracy is task-dependent in assessing wrist motion},
	volume = {120},
	issn = {0021-9290},
	url = {https://www.sciencedirect.com/science/article/pii/S0021929021001421},
	doi = {10.1016/j.jbiomech.2021.110362},
	abstract = {Optical motion capture (OMC) systems are commonly used to capture in-vivo three-dimensional joint kinematics. However, the skin-based markers may not reflect the underlying bone movement, a source of error known as soft tissue artifact (STA). This study examined STA during wrist motion by evaluating the agreement between OMC and biplanar videoradiography (BVR). Nine subjects completed 7 different wrist motion tasks: doorknob rotation to capture supination and pronation, radial-ulnar deviation, flexion–extension, circumduction, hammering, and pitcher pouring. BVR and OMC captured the motion simultaneously. Wrist kinematics were quantified using helical motion parameters of rotation and translation, and Bland-Altman analysis quantified the mean difference (bias) and 95\% limit of agreement (LOA). The rotational bias of doorknob pronation, a median bias of −4.9°, was significantly larger than the flexion–extension (0.7°, p {\textless} 0.05) and radial-ulnar deviation (1.8°, p {\textless} 0.01) tasks. The rotational LOA range was significantly smaller in the flexion–extension task (5.9°) compared to pitcher (11.6°, p {\textless} 0.05) and doorknob pronation (17.9°, p {\textless} 0.05) tasks. The translation bias did not differ between tasks. The translation LOA range was significantly larger in circumduction (9.8°) compared to the radial-ulnar deviation (6.3°, p {\textless} 0.05) and pitcher (3.4°, p {\textless} 0.05) tasks. While OMC technology has a wide-range of successful applications, we demonstrated it has relatively poor agreement with BVR in tracking wrist motion, and that the agreement depends on the nature and direction of wrist motion.},
	language = {en},
	urldate = {2021-07-28},
	journal = {Journal of Biomechanics},
	author = {McHugh, Brian and Akhbari, Bardiya and Morton, Amy M. and Moore, Douglas C. and Crisco, Joseph J.},
	month = may,
	year = {2021},
	keywords = {Accuracy, Kinematics, Optical motion capture, Soft tissue artifact, Wrist},
	pages = {110362},
	file = {McHugh et al_2021_Optical motion capture accuracy is task-dependent in assessing wrist motion.pdf:/Users/Vincent/Zotero/storage/ZMGBN4UI/McHugh et al_2021_Optical motion capture accuracy is task-dependent in assessing wrist motion.pdf:application/pdf;ScienceDirect Snapshot:/Users/Vincent/Zotero/storage/QJ9BU2QX/S0021929021001421.html:text/html},
}

@article{bishop2020a,
	title = {Reliability of two infrared motion capture systems in a music performance setting},
	copyright = {https://creativecommons.org/licenses/by/3.0/},
	url = {https://www.duo.uio.no/handle/10852/78536},
	abstract = {This paper describes a comparative analysis of tracking quality in two infrared marker-based motion capture systems: one older but high-end (Qualisys, purchased in 2009) and the other newer and mid-range (OptiTrack, purchased in 2019). We recorded performances by a string quartet with both systems simultaneously, using the same frame rate. Our recording set-up included a combination of moving markers (affixed to musicians’ bodies) and stationary markers (affixed to music stands). Higher noise levels were observed in Qualisys recordings of stationary markers than in OptiTrack recordings, as well as a greater spatial range, though OptiTrack recordings had a higher rate of outliers (“spikes” in the signal). In moving markers, increased quantity of motion was associated with increased betweensystem error rates. Both systems showed minimal withintrial drift but a reduction in recording accuracy and precision over the duration of the experiment. Overall, our results show that the older/high-end system (Qualisys) produced slightly lower-quality recordings than the newer/midrange system (OptiTrack). We discuss how our findings may inform researchers’ interpretations of motion capture data, particularly when capturing the types of motion that are important for performing music.},
	language = {EN},
	urldate = {2021-07-28},
	journal = {978-88-945415-0-2},
	author = {Bishop, Laura and Jensenius, Alexander Refsum},
	year = {2020},
	note = {Accepted: 2020-08-18T19:44:42Z
Publisher: Axea sas/SMC Network},
	file = {Snapshot:/Users/Vincent/Zotero/storage/2W3SSH7V/78536.html:text/html;Bishop_Jensenius_2020_Reliability of two infrared motion capture systems in a music performance.pdf:/Users/Vincent/Zotero/storage/236TII7J/Bishop_Jensenius_2020_Reliability of two infrared motion capture systems in a music performance.pdf:application/pdf},
}

@incollection{jensenius2018,
	address = {Berlin, Heidelberg},
	series = {Springer {Handbooks}},
	title = {Methods for {Studying} {Music}-{Related} {Body} {Motion}},
	isbn = {978-3-662-55004-5},
	url = {https://doi.org/10.1007/978-3-662-55004-5_38},
	abstract = {This chapter presents an overview of some methodological approaches and technologies that can be used in the study of music-related body motion. The aim is not to cover all possible approaches, but rather to highlight some of the ones that are more relevant from a musicological point of view. This includes methods for video-based and sensor-based motion analyses, both qualitative and quantitative. It also includes discussions of the strengths and weaknesses of the different methods, and reflections on how the methods can be used in connection to other data in question, such as physiological or neurological data, symbolic notation, sound recordings and contextual data.},
	language = {en},
	urldate = {2021-07-28},
	booktitle = {Springer {Handbook} of {Systematic} {Musicology}},
	publisher = {Springer},
	author = {Jensenius, Alexander Refsum},
	editor = {Bader, Rolf},
	year = {2018},
	doi = {10.1007/978-3-662-55004-5_38},
	keywords = {Laban Movement Analysis, Marker-based Systems, Motion Capture, Motion History Image, Multimodal Presentation Markup Language},
	pages = {805--818},
	file = {Jensenius_2018_Methods for Studying Music-Related Body Motion.pdf:/Users/Vincent/Zotero/storage/2MJS65IB/Jensenius_2018_Methods for Studying Music-Related Body Motion.pdf:application/pdf},
}

@article{chang2017,
	title = {Body sway reflects leadership in joint music performance},
	volume = {114},
	copyright = {©  . http://www.pnas.org/site/misc/userlicense.xhtml},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/114/21/E4134},
	doi = {10.1073/pnas.1617657114},
	abstract = {The cultural and technological achievements of the human species depend on complex social interactions. Nonverbal interpersonal coordination, or joint action, is a crucial element of social interaction, but the dynamics of nonverbal information flow among people are not well understood. We used joint music making in string quartets, a complex, naturalistic nonverbal behavior, as a model system. Using motion capture, we recorded body sway simultaneously in four musicians, which reflected real-time interpersonal information sharing. We used Granger causality to analyze predictive relationships among the motion time series of the players to determine the magnitude and direction of information flow among the players. We experimentally manipulated which musician was the leader (followers were not informed who was leading) and whether they could see each other, to investigate how these variables affect information flow. We found that assigned leaders exerted significantly greater influence on others and were less influenced by others compared with followers. This effect was present, whether or not they could see each other, but was enhanced with visual information, indicating that visual as well as auditory information is used in musical coordination. Importantly, performers’ ratings of the “goodness” of their performances were positively correlated with the overall degree of body sway coupling, indicating that communication through body sway reflects perceived performance success. These results confirm that information sharing in a nonverbal joint action task occurs through both auditory and visual cues and that the dynamics of information flow are affected by changing group relationships.},
	language = {en},
	number = {21},
	urldate = {2021-07-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Chang, Andrew and Livingstone, Steven R. and Bosnyak, Dan J. and Trainor, Laurel J.},
	month = may,
	year = {2017},
	pmid = {28484007},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {body sway, Granger causality, joint action, leadership, music performance},
	pages = {E4134--E4141},
	file = {Chang et al_2017_Body sway reflects leadership in joint music performance.pdf:/Users/Vincent/Zotero/storage/BUU8EQ46/Chang et al_2017_Body sway reflects leadership in joint music performance.pdf:application/pdf;Snapshot:/Users/Vincent/Zotero/storage/V89GL9K5/E4134.html:text/html},
}

@article{merel2017,
	title = {Learning human behaviors from motion capture by adversarial imitation},
	url = {http://arxiv.org/abs/1707.02201},
	abstract = {Rapid progress in deep reinforcement learning has made it increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to produce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of generic neural network policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show that they can be reused to solve tasks when controlled by a higher level controller.},
	urldate = {2021-07-28},
	journal = {arXiv:1707.02201 [cs]},
	author = {Merel, Josh and Tassa, Yuval and TB, Dhruva and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.02201},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {Merel et al_2017_Learning human behaviors from motion capture by adversarial imitation.pdf:/Users/Vincent/Zotero/storage/GHKUPV2R/Merel et al_2017_Learning human behaviors from motion capture by adversarial imitation.pdf:application/pdf;arXiv.org Snapshot:/Users/Vincent/Zotero/storage/S4FME3VL/1707.html:text/html},
}

@incollection{perez-carrillo2016,
	address = {Cham},
	title = {Estimation of {Guitar} {Fingering} and {Plucking} {Controls} {Based} on {Multimodal} {Analysis} of {Motion}, {Audio} and {Musical} {Score}},
	volume = {9617},
	isbn = {978-3-319-46281-3 978-3-319-46282-0},
	url = {http://link.springer.com/10.1007/978-3-319-46282-0_5},
	abstract = {This work presents a method for the extraction of instrumental controls during guitar performances. The method is based on the analysis of multimodal data consisting of a combination of motion capture, audio analysis and musical score. High speed video cameras based on marker identiﬁcation are used to track the position of ﬁnger bones and articulations and audio is recorded with a transducer measuring vibration on the guitar body. The extracted parameters are divided into left hand controls, i.e. ﬁngering (which string and fret is pressed with a left hand ﬁnger) and right hand controls, i.e. the plucked string, the plucking ﬁnger and the characteristics of the pluck (position, velocity and angles with respect to the string). Controls are estimated based on probability functions of low level features, namely, the plucking instants (i.e. note onsets), the pitch and the distances of the ﬁngers (both hands) to strings and frets. Note onsets are detected via audio analysis, the pitch is extracted from the score and distances are computed from 3D Euclidean Geometry. Results show that by combination of multimodal information, it is possible to estimate such a comprehensive set of control features, with special high performance for the ﬁngering and plucked string estimation. Regarding the plucking ﬁnger and the pluck characteristics, their accuracy gets lower but improvements are foreseen including a hand model and the use of high-speed cameras for calibration and evaluation.},
	language = {en},
	urldate = {2021-07-28},
	booktitle = {Music, {Mind}, and {Embodiment}},
	publisher = {Springer International Publishing},
	author = {Perez-Carrillo, Alfonso and Arcos, Josep-Lluis and Wanderley, Marcelo},
	editor = {Kronland-Martinet, Richard and Aramaki, Mitsuko and Ystad, Sølvi},
	year = {2016},
	doi = {10.1007/978-3-319-46282-0_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {71--87},
	file = {Perez-Carrillo et al. - 2016 - Estimation of Guitar Fingering and Plucking Contro.pdf:/Users/Vincent/Zotero/storage/I5RSKW4N/Perez-Carrillo et al. - 2016 - Estimation of Guitar Fingering and Plucking Contro.pdf:application/pdf},
}

@inproceedings{savioz2011,
	title = {Towards multi-finger haptic devices: {A} computer keyboard with adjustable force feedback},
	shorttitle = {Towards multi-finger haptic devices},
	doi = {10.1109/ICEMS.2011.6073855},
	abstract = {This paper addresses the design of a novel computer keyboard embedding 64 miniature actuators providing a programmable force feedback in response to human strokes. A specific electromechanical actuator is designed using a global methodology. Then, a parallel strategy is set to efficiently control all the actuators simultaneously. Finally, two prototypes are presented: the haptic pad and the haptic keyboard. In addition, a graphic interface allows the user to define his preferable force profile: whether he prefers tough or soft keys, smooth of rough sensation or even a buckling-spring like feeling.},
	booktitle = {2011 {International} {Conference} on {Electrical} {Machines} and {Systems}},
	author = {Savioz, Grégory and Markovic, Miroslav and Perriard, Yves},
	month = aug,
	year = {2011},
	keywords = {force feedback, Actuators, Computers, design for experiments, Force, Force feedback, Haptic interface, keyboard, Keyboards, Microcontrollers, miniature actuator, optimization method},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/Vincent/Zotero/storage/3W2HM9HT/6073855.html:text/html},
}

@misc{zotero-1409,
	title = {Tactile {Echoes}: {A} {Wearable} {System} for {Tactile} {Augmentation} of {Objects}},
	shorttitle = {Tactile {Echoes}},
	url = {https://research.fb.com/publications/tactile-echoes-a-wearable-system-for-tactile-augmentation-of-objects/},
	abstract = {We present Tactile Echoes, a wearable system for augmenting tactile interactions with any object. This system senses vibrations in the fingertip that are produced by interactions of the finger with a touched object. It processes the vibration signals in real-time via a parametric signal network and returns them to the finger as “Tactile Echoes” of the touch interaction.},
	language = {en-US},
	urldate = {2021-09-08},
	journal = {Facebook Research},
	file = {Snapshot:/Users/Vincent/Zotero/storage/EG5GA8PE/tactile-echoes-a-wearable-system-for-tactile-augmentation-of-objects.html:text/html},
}

@inproceedings{kawazoe2019,
	address = {Tokyo, Japan},
	title = {Tactile {Echoes}: {A} {Wearable} {System} for {Tactile} {Augmentation} of {Objects}},
	isbn = {978-1-5386-9461-9},
	shorttitle = {Tactile {Echoes}},
	url = {https://ieeexplore.ieee.org/document/8816099/},
	doi = {10.1109/WHC.2019.8816099},
	abstract = {We present Tactile Echoes, a wearable system for augmenting tactile interactions with any object. This system senses vibrations in the ﬁngertip that are produced by interactions of the ﬁnger with a touched object. It processes the vibration signals in real-time via a parametric signal network and returns them to the ﬁnger as “Tactile Echoes” of the touch interaction. Just as acoustic echoes continuously respond to sound, Tactile Echoes are continuously generated in response to the sensed tactile contacts. A short ﬁnger tap produces discrete Echoes, while a slide can yield continuous feedback. We also render the signals as sound, yielding multisensory feedback. Many different effects can be designed using ten signal processing parameters. Distinct effects may be assigned to different touched objects or surface regions by sensing the hand location in a mapped environment. We investigated how Tactile Echoes are perceived in a behavioral study using semantic differential scaling and multidimensional scaling methods. This yielded low-dimensional, semantically grounded representations of the perceptual similarities between different Echoes. This system holds promise for enabling evocative haptic effects during a wide range of free-hand tactile interactions.},
	language = {en},
	urldate = {2021-09-08},
	booktitle = {2019 {IEEE} {World} {Haptics} {Conference} ({WHC})},
	publisher = {IEEE},
	author = {Kawazoe, Anzu and Luca, Massimiliano Di and Visell, Yon},
	month = jul,
	year = {2019},
	pages = {359--364},
	file = {Kawazoe et al. - 2019 - Tactile Echoes A Wearable System for Tactile Augm.pdf:/Users/Vincent/Zotero/storage/HBCWY2NP/Kawazoe et al. - 2019 - Tactile Echoes A Wearable System for Tactile Augm.pdf:application/pdf},
}

@misc{titmuss2021,
	title = {Torn},
	url = {https://github.com/rtitmuss/torn},
	abstract = {Torn keyboard},
	urldate = {2021-09-09},
	author = {Titmuss, Richard},
	month = sep,
	year = {2021},
	note = {original-date: 2020-08-02T10:07:44Z},
}

@misc{by2021a,
	title = {Where {We}’re {Going}, {We} {Don}’t {Need} {Keycaps}},
	url = {https://hackaday.com/2021/04/26/where-were-going-we-dont-need-keycaps/},
	abstract = {Just when we thought we’d seen the peak of ergonomic, split keyboards, along comes [Peter Lyons] with the Squeezebox — an adjustable, column-staggered, streamlined beauty with 21 keys p…},
	language = {en-US},
	urldate = {2021-09-09},
	journal = {Hackaday},
	author = {By},
	month = apr,
	year = {2021},
	file = {Snapshot:/Users/Vincent/Zotero/storage/3MGG934X/where-were-going-we-dont-need-keycaps.html:text/html},
}

@misc{richmond2021,
	type = {Substack newsletter},
	title = {Beginner’s {Guide} to {Mechanical} {Keyboards}},
	url = {https://coolgadget.substack.com/p/beginners-guide-to-mechanical-keyboards},
	abstract = {As we all know, the performance of a mechanical keyboard is overall better than that of a membrane keyboard. But how to pick your Mr/Mrs. Right? The first thing you should learn about is the differences between a mechanical keyboard and a membrane keyboard.},
	urldate = {2021-09-09},
	journal = {Jenny’s Newsletter},
	author = {Richmond, Jenny},
	month = apr,
	year = {2021},
	file = {Snapshot:/Users/Vincent/Zotero/storage/V5CNNS8C/beginners-guide-to-mechanical-keyboards.html:text/html},
}

@misc{2018,
	title = {What influences keyboard input speed},
	url = {https://blog.wooting.nl/what-influences-keyboard-speed/},
	abstract = {While everybody is focused on mouse lag, there’s little attention to keyboard lag. I’m going to explain to you why keyboard input latency matters, what influences it, and what you can do about it.},
	language = {en},
	urldate = {2021-09-09},
	journal = {Wooting developer blog},
	month = may,
	year = {2018},
	file = {Snapshot:/Users/Vincent/Zotero/storage/X5W9VB62/what-influences-keyboard-speed.html:text/html},
}

@misc{by2021b,
	title = {Gesture-{Detecting} {Macro} {Keyboard} {Knows} {What} {You} {Want}},
	url = {https://hackaday.com/2021/06/13/gesture-detecting-macro-keyboard-knows-what-you-want/},
	abstract = {[jakkra] bought a couple of capacitive touchpads from a Kickstarter a few years ago and recently got around to using them in a project. And what a project it is: this super macro pad combines two t…},
	language = {en-US},
	urldate = {2021-09-09},
	journal = {Hackaday},
	author = {By},
	month = jun,
	year = {2021},
	file = {Snapshot:/Users/Vincent/Zotero/storage/I8ZSDHVQ/gesture-detecting-macro-keyboard-knows-what-you-want.html:text/html},
}

@misc{zotero-1422,
	title = {Wooting two {HE} - {Lekker} switch - {Full}-size {Analog} {Mechanical} {Keyboard}},
	url = {https://wooting.io/wooting_two_he},
	abstract = {A full-size gaming keyboard with magnet Lekker switches for analog input on each key. Adjust you actuation point from 0.1 to 4.0mm, experience analog movement in games, achieve the fastest possible input speed, and active developers to back it all up. This is the future of pc keyboards for gaming.},
	language = {en-US},
	urldate = {2021-09-09},
	file = {Snapshot:/Users/Vincent/Zotero/storage/5MWUFXJV/wooting_two_he.html:text/html},
}

@misc{by2021c,
	title = {Mag-{Lev} {Switches} {Are} {The} {Future} {Of} {Clacking}},
	url = {https://hackaday.com/2021/08/02/mag-lev-switches-are-the-future-of-clacking/},
	abstract = {While there’s probably a Cherry MX clone born every year or so, it’s not often that such a radically different type of switch comes along. These “Void” switches are Hall-eff…},
	language = {en-US},
	urldate = {2021-09-09},
	journal = {Hackaday},
	author = {By},
	month = aug,
	year = {2021},
	file = {Snapshot:/Users/Vincent/Zotero/storage/N2HL6GRQ/mag-lev-switches-are-the-future-of-clacking.html:text/html},
}

@misc{zotero-1426,
	title = {Keyboard latency},
	url = {https://danluu.com/keyboard-latency/},
	urldate = {2021-09-09},
	file = {Keyboard latency:/Users/Vincent/Zotero/storage/R58YTMC4/keyboard-latency.html:text/html},
}

@article{kirkegaard,
	title = {{TorqueTuner}: {A} self contained module for designing rotary haptic force feedback for digital musical instruments},
	abstract = {TorqueTuner is an embedded module that allows Digital Musical Instrument (DMI) designers to map sensors to parameters of haptic eﬀects and dynamically modify rotary force feedback in real-time. We embedded inside TorqueTuner a collection of haptic eﬀects (Wall, Magnet, Detents, Spring, Friction, Spin, Free) and a bi-directional interface through libmapper, a software library for making connections between data signals on a shared network. To increase aﬀordability and portability of force-feedback implementations in DMI design, we designed our platform to be wireless, self-contained and built from commercially available components. To provide examples of modularity and portability, we integrated TorqueTuner into a standalone haptic knob and into an existing DMI, the T-Stick. We implemented 3 musical applications (Pitch wheel, Turntable and Exciter), by mapping sensors to sound synthesis in audio programming environment SuperCollider. While the original goal was to simulate the haptic feedback associated with turning a knob, we found that the platform allows for further expanding interaction possibilities in application scenarios where rotary control is familiar.},
	language = {en},
	author = {Kirkegaard, Mathias and Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M},
	pages = {6},
	file = {Kirkegaard et al. - TorqueTuner A self contained module for designing.pdf:/Users/Vincent/Zotero/storage/J38GFH3R/Kirkegaard et al. - TorqueTuner A self contained module for designing.pdf:application/pdf},
}

@misc{kirkegaarda,
	title = {Integrating 1-{DoF} force feedback interactions in self-contained {DMIs}},
	url = {https://escholarship.mcgill.ca/concern/theses/b5644x49k},
	language = {http://id.loc.gov/vocabulary/iso639-2/eng},
	urldate = {2021-09-09},
	author = {Kirkegaard, Mathias},
	collaborator = {Pietrzak (Supervisor2), Thomas and Wanderley (Supervisor1), Marcelo},
	note = {Publisher: McGill University},
	keywords = {Music},
	file = {Thesis | Integrating 1-DoF force feedback interactions in self-contained DMIs | ID\: b5644x49k | eScholarship@McGill:/Users/Vincent/Zotero/storage/6V76YPLX/b5644x49k.html:text/html;Kirkegaard_Integrating 1-DoF force feedback interactions in self-contained DMIs.pdf:/Users/Vincent/Zotero/storage/LHNVNQVA/Kirkegaard_Integrating 1-DoF force feedback interactions in self-contained DMIs.pdf:application/pdf},
}

@misc{zotero-1434,
	title = {moteus r4.5 controller},
	url = {https://mjbots.com/products/moteus-r4-5},
	abstract = {OUT OF STOCK: Expect a new revision (fully compatible with the r4.5) in early to mid September 2021!  There are still a few development kits remaining. ---- The moteus brushless controller turns a hobby brushless motor into a high performance servo actuator.  It integrates the necessary drive electronics, a high perfor},
	language = {en},
	urldate = {2021-09-09},
	journal = {mjbots Robotic Systems},
	file = {Snapshot:/Users/Vincent/Zotero/storage/39MDM78H/moteus-r4-5.html:text/html},
}

@inproceedings{kim2013,
	address = {New York, NY, USA},
	series = {{UIST} '13},
	title = {Haptic feedback design for a virtual button along force-displacement curves},
	isbn = {978-1-4503-2268-3},
	url = {https://doi.org/10.1145/2501988.2502041},
	doi = {10.1145/2501988.2502041},
	abstract = {In this paper, we present a haptic feedback method for a virtual button based on the force-displacement curves of a physical button. The original feature of the proposed method is that it provides haptic feedback, not only for the "click" sensation but also for the moving sensation before and after transition points in a force-displacement curve. The haptic feedback is by vibrotactile stimulations only and does not require a force feedback mechanism. We conducted user experiments to show that the resultant haptic feedback is realistic and distinctive. Participants were able to distinguish among six different virtual buttons, with 94.1\% accuracy even in a noisy environment. In addition, participants were able to associate four virtual buttons with their physical counterparts, with a correct answer rate of 79.2\%.},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 26th annual {ACM} symposium on {User} interface software and technology},
	publisher = {Association for Computing Machinery},
	author = {Kim, Sunjun and Lee, Geehyuk},
	month = oct,
	year = {2013},
	keywords = {haptic, force-displacement curve, tactile feedback, rigid surface, virtual button},
	pages = {91--96},
	file = {Kim_Lee_2013_Haptic feedback design for a virtual button along force-displacement curves.pdf:/Users/Vincent/Zotero/storage/4LCI68IU/Kim_Lee_2013_Haptic feedback design for a virtual button along force-displacement curves.pdf:application/pdf},
}

@incollection{liao2020b,
	address = {New York, NY, USA},
	title = {Button {Simulation} and {Design} via {FDVV} {Models}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376262},
	abstract = {Designing a push-button with desired sensation and performance is challenging because the mechanical construction must have the right response characteristics. Physical simulation of a button's force-displacement (FD) response has been studied to facilitate prototyping; however, the simulations' scope and realism have been limited. In this paper, we extend FD modeling to include vibration (V) and velocity-dependence characteristics (V). The resulting FDVV models better capture tactility characteristics of buttons, including snap. They increase the range of simulated buttons and the perceived realism relative to FD models. The paper also demonstrates methods for obtaining these models, editing them, and simulating accordingly. This end-to-end approach enables the analysis, prototyping, and optimization of buttons, and supports exploring designs that would be hard to implement mechanically.},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Yi-Chi and Kim, Sunjun and Lee, Byungjoo and Oulasvirta, Antti},
	month = apr,
	year = {2020},
	keywords = {button, fd model, fdvv model, force feedback, haptic, haptic rendering, input device, modeling, simulation, tactility, vibration},
	pages = {1--14},
	file = {Liao et al_2020_Button Simulation and Design via FDVV Models.pdf:/Users/Vincent/Zotero/storage/NJJWED35/Liao et al_2020_Button Simulation and Design via FDVV Models.pdf:application/pdf},
}

@inproceedings{depra2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Endless {Knob} with {Programmable} {Resistive} {Force} {Feedback}},
	isbn = {978-3-030-85610-6},
	doi = {10.1007/978-3-030-85610-6_32},
	abstract = {Touchscreens today represent the most versatile solution for configuring user interfaces. A toll for this versatility is the intangibility of the displayed virtual controls. The addition of haptic feedback can improve their manipulation by reinforcing or even substituting visual information. While haptic rendering of virtual buttons is advancing, tangible knobs seem yet to come, possibly due to the inherent difficulty to conceptualize rotation as an interaction primitive in absence of a physical control. To address this issue, we propose a hybrid solution consisting in an endless knob with programmable resistance to rotation. Compared to existing related devices, it minimizes costs, encumbrance and power consumption, making its installation also possible on portable equipment. After describing its design and main features, we present a test which assessed how haptic feedback rendered by the knob affects performance in a visual target-matching task: users had to select targets placed on a horizontal slider by dragging its cursor through knob rotation. Results show that haptic augmentation significantly improved target acquisition.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2021},
	publisher = {Springer International Publishing},
	author = {De Pra, Yuri and Fontana, Federico and Papetti, Stefano},
	editor = {Ardito, Carmelo and Lanzilotti, Rosa and Malizia, Alessio and Petrie, Helen and Piccinno, Antonio and Desolda, Giuseppe and Inkpen, Kori},
	year = {2021},
	pages = {580--589},
}

@incollection{liao2020c,
	address = {New York, NY, USA},
	title = {Button {Simulation} and {Design} via {FDVV} {Models}},
	isbn = {978-1-4503-6708-0},
	url = {https://doi.org/10.1145/3313831.3376262},
	abstract = {Designing a push-button with desired sensation and performance is challenging because the mechanical construction must have the right response characteristics. Physical simulation of a button's force-displacement (FD) response has been studied to facilitate prototyping; however, the simulations' scope and realism have been limited. In this paper, we extend FD modeling to include vibration (V) and velocity-dependence characteristics (V). The resulting FDVV models better capture tactility characteristics of buttons, including snap. They increase the range of simulated buttons and the perceived realism relative to FD models. The paper also demonstrates methods for obtaining these models, editing them, and simulating accordingly. This end-to-end approach enables the analysis, prototyping, and optimization of buttons, and supports exploring designs that would be hard to implement mechanically.},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Yi-Chi and Kim, Sunjun and Lee, Byungjoo and Oulasvirta, Antti},
	month = apr,
	year = {2020},
	keywords = {button, fd model, fdvv model, force feedback, haptic, haptic rendering, input device, modeling, simulation, tactility, vibration},
	pages = {1--14},
	file = {Liao et al_2020_Button Simulation and Design via FDVV Models.pdf:/Users/Vincent/Zotero/storage/NRBH48YK/Liao et al_2020_Button Simulation and Design via FDVV Models.pdf:application/pdf},
}

@incollection{chang2020,
	address = {New York, NY, USA},
	title = {Kirigami {Haptic} {Swatches}: {Design} {Methods} for {Cut}-and-{Fold} {Haptic} {Feedback} {Mechanisms}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Kirigami {Haptic} {Swatches}},
	url = {https://doi.org/10.1145/3313831.3376655},
	abstract = {Kirigami Haptic Swatches demonstrate how kirigami and origami based structures enable sophisticated haptic feedback through simple cut-and-fold fabrication techniques. We leverage four types of geometric patterns: rotational erection system (RES), split-fold waterbomb (SFWB), the overlaid structure of SFWB and RES (SFWB+RES), and cylindrical origami, to render different sets of haptic feedback (i.e. linear, bistable, bouncing snap-through, and rotational force behaviors, respectively). In each structure, not only the form factor but also the force feedback properties can be tuned through geometric parameters. We experimentally analyzed and modeled the structures, and implemented software to automatically generate 2D patterns for desired haptic properties. We also demonstrate five example applications including an assistive custom keyboard, rotational switch, multi-sensory toy, task checklist, and phone accessories. We believe the Kirigami Haptic Swatches helps tinkerers, designers, and even researchers to create interactions that enrich our haptic experience.},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chang, Zekun and Ta, Tung D. and Narumi, Koya and Kim, Heeju and Okuya, Fuminori and Li, Dongchi and Kato, Kunihiro and Qi, Jie and Miyamoto, Yoshinobu and Saito, Kazuya and Kawahara, Yoshihiro},
	month = apr,
	year = {2020},
	keywords = {haptics, computational fabrication, design methods, Kirigami structure, paper button},
	pages = {1--12},
	file = {Chang et al_2020_Kirigami Haptic Swatches.pdf:/Users/Vincent/Zotero/storage/624JTFJZ/Chang et al_2020_Kirigami Haptic Swatches.pdf:application/pdf},
}

@inproceedings{park2020b,
	address = {New York, NY, USA},
	series = {{UIST} '20},
	title = {Augmenting {Physical} {Buttons} with {Vibrotactile} {Feedback} for {Programmable} {Feels}},
	isbn = {978-1-4503-7514-6},
	url = {https://doi.org/10.1145/3379337.3415837},
	doi = {10.1145/3379337.3415837},
	abstract = {Physical buttons provide clear haptic feedback when pressed and released, but their responses are unvarying. Physical buttons can be powered by force actuators to produce unlimited click sensations, but the cost is substantial. An alternative can be augmenting physical buttons with simple and inexpensive vibration actuators. When pushed, an augmented button generates a vibration overlayed on the button's original kinesthetic response, under the general framework of haptic augmented reality. We explore the design space of augmented buttons while changing vibration frequency, amplitude, duration, and envelope. We then visualize the perceptual structure of augmented buttons by estimating a perceptual space for 7 physical buttons and 40 augmented buttons. Their sensations are also assessed against adjectives, and results are mapped into the perceptual space to identify meaningful perceptual dimensions. Our results contribute to understanding the benefits and limitations of programmable vibration-augmented physical buttons with emphasis on their feels.},
	urldate = {2021-09-09},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Park, Chaeyong and Yoon, Jinhyuk and Oh, Seungjae and Choi, Seungmoon},
	month = oct,
	year = {2020},
	keywords = {button, haptics, augmented reality, multimodal, vibrotactile},
	pages = {924--937},
	file = {Park et al_2020_Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels.pdf:/Users/Vincent/Zotero/storage/ZCQEJHUJ/Park et al_2020_Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels.pdf:application/pdf},
}

@misc{cardona2021,
	title = {jcard0na/haxo-hw},
	copyright = {MIT},
	url = {https://github.com/jcard0na/haxo-hw},
	abstract = {Haxophone, an electronic musical instrument that resembles a saxophone},
	urldate = {2021-09-10},
	author = {Cardona, Javier},
	month = sep,
	year = {2021},
	note = {original-date: 2021-05-05T10:42:57Z},
}

@inproceedings{orio2001,
	title = {Input {Devices} for {Musical} {Expression}: {Borrowing} {Tools} from {HCI}},
	shorttitle = {Input {Devices} for {Musical} {Expression}},
	abstract = {This paper reviews the existing literature on input device evaluation and design in human-computer interaction (HCI) and discusses possible applications of this knowledge to the design and evaluation of new interfaces for musical expression. Specifically, a set of musical tasks is suggested to allow the evaluation of different existing controllers.},
	author = {Orio, Nicola and Schnell, Norbert and Wanderley, Marcelo},
	month = jan,
	year = {2001},
	file = {Orio et al_2001_Input Devices for Musical Expression.pdf:/Users/Vincent/Zotero/storage/V5YC66CU/Orio et al_2001_Input Devices for Musical Expression.pdf:application/pdf},
}

@inproceedings{hunt2000a,
	title = {Towards a {Model} for {Instrumental} {Mapping} in {Expert} {Musical} {Interaction}},
	abstract = {cote interne IRCAM: Hunt00a},
	author = {Hunt, Andy and Wanderley, Marcelo and Kirk, Ross},
	month = sep,
	year = {2000},
	file = {Hunt et al_2000_Towards a Model for Instrumental Mapping in Expert Musical Interaction.pdf:/Users/Vincent/Zotero/storage/SLCF5N67/Hunt et al_2000_Towards a Model for Instrumental Mapping in Expert Musical Interaction.pdf:application/pdf},
}

@article{sullivan,
	title = {Reinventing the {Noisebox}: {Designing} {Embedded} {Instruments} for {Active} {Musicians}},
	abstract = {This paper reports on the user-driven redesign of an embedded digital musical instrument that has yielded a trio of new instruments, informed by early user feedback and co-design workshops organized with active musicians. Collectively, they share a stand-alone design, digitally fabricated enclosures, and a common sensor acquisition and sound synthesis architecture, yet each is unique in its playing technique and sonic output. We focus on the technical design of the instruments and provide examples of key design speciﬁcations that were derived from user input, while reﬂecting on the challenges to, and opportunities for, creating instruments that support active practices of performing musicians.},
	language = {en},
	author = {Sullivan, John and Vanasse, Julian and Wanderley, Marcelo M and Guastavino, Catherine},
	pages = {6},
	file = {Sullivan et al. - Reinventing the Noisebox Designing Embedded Instr.pdf:/Users/Vincent/Zotero/storage/CCFMI4FC/Sullivan et al. - Reinventing the Noisebox Designing Embedded Instr.pdf:application/pdf},
}

@article{harrison,
	title = {When is a {Guitar} not a {Guitar}? {Cultural} {Form}, {Input} {Modality} and {Expertise}},
	abstract = {The design of traditional musical instruments is a process of incremental reﬁnement over many centuries of innovation. As a result, the shape and form of instruments are well established and recognised across cultures. Conversely, digital musical instruments (DMIs), being unconstrained by requirements of eﬃcient acoustic sound production and ergonomics, can take on forms which are more abstract in their relation to the mechanism of control and sound production. In this paper we consider the case of designing DMIs that resemble traditional instruments, and pose questions around the social and technical acceptability of certain design choices relating to physical form and input modality (sensing strategy and the input gestures that it aﬀords). We designed four guitar-derivative DMIs to be suitable for performing strummed harmonic accompaniments to a folk tune. Each instrument possesses a combination of one of two global forms (guitar-like body and a smaller tabletop enclosure) and one of two control mechanisms (physical strings and touch sensors). We conducted a study where both non-musicians and guitarists played two versions of the instruments and completed musical tasks with each instrument. This study highlights the complex interaction between global form and input modality when designing for existing musical cultures and varying levels of expertise.},
	language = {en},
	author = {Harrison, Jacob and Jack, Robert H and Morreale, Fabio and McPherson, Andrew},
	pages = {6},
	file = {Harrison et al. - When is a Guitar not a Guitar Cultural Form, Inpu.pdf:/Users/Vincent/Zotero/storage/PLFPV96G/Harrison et al. - When is a Guitar not a Guitar Cultural Form, Inpu.pdf:application/pdf},
}

@article{jense2015,
	title = {Awakening the {Synthesizer} {Knob}: {Gestural} {Perspectives}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Awakening the {Synthesizer} {Knob}},
	url = {https://www.mdpi.com/2075-1702/3/4/317},
	doi = {10.3390/machines3040317},
	abstract = {While being the primary mode of interaction with mainstream digital musical instruments, the knob has been greatly overlooked in its potential for innovation. In this paper, we aim to open up the thinking about new possibilities for the knob. Based on an analysis of the background of the knob and the relevant theory on interaction, three directives are formulated to guide the design of a new breed of knobs. Six prototypes are tested through an AttrakDiff questionnaire and Discourse Analysis. It is shown that the proposed new breed of knobs has stronger hedonic qualities than knobs of mainstream digital musical instruments, though the pragmatic quality appears lower. The strongest improvement is seen in stimulation, an important factor in enticing investment to play, and as such, more expressive control. Through using three directives, a new generation of knobs can be made that would improve the expressive affordances of digital musical instruments.},
	language = {en},
	number = {4},
	urldate = {2021-09-12},
	journal = {Machines},
	author = {Jense, Arvid and Eggen, Berry},
	month = dec,
	year = {2015},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {interaction design, interfaces, digital musical instruments, AttrakDiff, discourse analysis, electronic musical instruments, knobs},
	pages = {317--338},
	file = {Jense_Eggen_2015_Awakening the Synthesizer Knob.pdf:/Users/Vincent/Zotero/storage/SHF7MFZC/Jense_Eggen_2015_Awakening the Synthesizer Knob.pdf:application/pdf;Snapshot:/Users/Vincent/Zotero/storage/K39SUDJR/htm.html:text/html},
}

@article{zappi,
	title = {Dimensionality and {Appropriation} in {Digital} {Musical} {Instrument} {Design}},
	abstract = {This paper investigates the process of appropriation in digital musical instrument performance, examining the eﬀect of instrument complexity on the emergence of personal playing styles. Ten musicians of varying background were given a deliberately constrained musical instrument, a wooden cube containing a touch/force sensor, speaker and embedded computer. Each cube was identical in construction, but half the instruments were conﬁgured for two degrees of freedom while the other half allowed only a single degree. Each musician practiced at home and presented two performances, in which their techniques and reactions were assessed through video, sensor data logs, questionnaires and interviews. Results show that the addition of a second degree of freedom had the counterintuitive eﬀect of reducing the exploration of the instrument’s aﬀordances; this suggested the presence of a dominant constraint in one of the two conﬁgurations which strongly diﬀerentiated the process of appropriation across the two groups of participants.},
	language = {en},
	author = {Zappi, Victor and McPherson, Andrew P},
	pages = {6},
	file = {Zappi and McPherson - Dimensionality and Appropriation in Digital Musica.pdf:/Users/Vincent/Zotero/storage/XM26PQBZ/Zappi and McPherson - Dimensionality and Appropriation in Digital Musica.pdf:application/pdf},
}

@article{sullivana,
	title = {Built for {Performance}: {Designing} {Digital} {Musical} {Instruments} for {Professional} {Use}},
	abstract = {The ﬁeld of digital musical instrument (DMI) design is highly interdisciplinary and comprises a variety of diﬀerent approaches to developing new instruments and putting them into artistic use. While these vibrant ecosystems of design and creative practice thrive in certain communities, they tend to be concentrated within the context of contemporary experimental musical practice and academic research. In more widespread professional performance communities, while digital technology is ubiquitous, the use of truly novel DMIs is uncommon.},
	language = {en},
	author = {Sullivan, John D},
	pages = {249},
	file = {Sullivan - Built for Performance Designing Digital Musical I.pdf:/Users/Vincent/Zotero/storage/4UUIHPCF/Sullivan - Built for Performance Designing Digital Musical I.pdf:application/pdf},
}

@inproceedings{vanoosterhout2019,
	address = {New York, NY, USA},
	series = {{DIS} '19},
	title = {{DynaKnob}: {Combining} {Haptic} {Force} {Feedback} and {Shape} {Change}},
	isbn = {978-1-4503-5850-7},
	shorttitle = {{DynaKnob}},
	url = {https://doi.org/10.1145/3322276.3322321},
	doi = {10.1145/3322276.3322321},
	abstract = {Despite the advantages of tangible interaction, physical controls like knobs seem to be disappearing from a wide range of products in our everyday life. The work presented in this paper explores how physical controls can become dynamic, in terms of both shape, and haptic force feedback. The paper contains two strands of work: First, we present a study that explores the relationship between haptic force feedback and different knob shapes, evaluating twelve distinct haptic stimuli in relation to six widely used knob shapes. Second, based on the insights collected in the study, we present the design of DynaKnob, a shape changing knob that can change between four different knob shapes. DynaKnob illustrates how dynamic content control, can be combined with dynamic shape and force feedback. Both the study and the design of DynaKnob contribute to understanding how adaptive physical interface controls could be designed in the future.},
	urldate = {2021-09-13},
	booktitle = {Proceedings of the 2019 on {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {van Oosterhout, Anke and Hoggan, Eve and Rasmussen, Majken Kirkegaard and Bruns, Miguel},
	month = jun,
	year = {2019},
	keywords = {affordance, haptic force feedback, mechanical metamaterials, shape-changing interfaces},
	pages = {963--974},
}

@inproceedings{burger2013,
	address = {Stockholm, Sweden},
	title = {{MoCap} {Toolbox} – {A} {Matlab} toolbox for computational analysis of movement data},
	booktitle = {Proceedings of the 10th sound and music computing conference},
	publisher = {KTH Royal Institute of Technology},
	author = {Burger, Birgitta and Toiviainen, Petri},
	editor = {Bresin, Roberto},
	year = {2013},
	pages = {172--178},
}

@inproceedings{bouenard2008,
	address = {Genova, Italy},
	title = {Enhancing the {Visualization} of {Percussion} {Gestures} by {Virtual} {Character} {Animation}},
	url = {https://hal.archives-ouvertes.fr/hal-00369237},
	abstract = {A new interface for visualizing and analyzing percussion gestures is presented, proposing enhancements of existing motion capture analysis tools. This is achieved by offering a percussion gesture analysis protocol using motion capture. A virtual character dynamic model is then designed in order to take advantage of gesture characteristics, yielding to improve gesture analysis with visualization and interaction cues of different types.},
	urldate = {2021-10-04},
	booktitle = {{NIME}},
	author = {Bouënard, Alexandre and Gibet, Sylvie and Wanderley, Marcelo M.},
	month = jun,
	year = {2008},
	keywords = {Percussion Performance, Physics Modeling, Virtual Character Animation},
	pages = {38--43},
	file = {Bouënard et al_2008_Enhancing the Visualization of Percussion Gestures by Virtual Character.pdf:/Users/Vincent/Zotero/storage/ZF89SYEL/Bouënard et al_2008_Enhancing the Visualization of Percussion Gestures by Virtual Character.pdf:application/pdf},
}

@inproceedings{reid2016,
	address = {Brisbane, Australia},
	title = {Minimally invasive gesture sensing interface ({MIGSI}) for trumpet},
	volume = {16},
	isbn = {978-1-925455-13-7},
	url = {http://www.nime.org/proceedings/2016/nime2016_paper0082.pdf},
	doi = {10.5281/zenodo.1176106},
	booktitle = {Proceedings of the international conference on new interfaces for musical expression},
	publisher = {Queensland Conservatorium Griffith University},
	author = {Reid, Sarah and Gaston, Ryan and Honigman, Colin and Kapur, Ajay},
	year = {2016},
	note = {ISSN: 2220-4806
tex.track: Papers},
	pages = {419--424},
	file = {Reid et al_2016_Minimally invasive gesture sensing interface (MIGSI) for trumpet.pdf:/Users/Vincent/Zotero/storage/DRVDN3R7/Reid et al_2016_Minimally invasive gesture sensing interface (MIGSI) for trumpet.pdf:application/pdf},
}

@inproceedings{trail2012,
	address = {Ann Arbor, Michigan},
	title = {Non-invasive sensing and gesture control for pitched percussion hyper-instruments using the {Kinect}},
	url = {http://www.nime.org/proceedings/2012/nime2012_297.pdf},
	doi = {10.5281/zenodo.1178435},
	booktitle = {Proceedings of the international conference on new interfaces for musical expression},
	publisher = {University of Michigan},
	author = {Trail, Shawn and Dean, Michael and Odowichuk, Gabrielle and Tavares, Tiago Fernandes and Driessen, Peter and Schloss, W. Andrew and Tzanetakis, George},
	year = {2012},
	note = {ISSN: 2220-4806},
	file = {Trail et al. - Non-invasive sensing and gesture control for pitch.pdf:/Users/Vincent/Zotero/storage/9GZRTUVK/Trail et al. - Non-invasive sensing and gesture control for pitch.pdf:application/pdf},
}

@inproceedings{melo2012,
	address = {Ann Arbor, Michigan},
	title = {Gest-{O}: {Performer} gestures used to expand the sounds of the saxophone},
	url = {http://www.nime.org/proceedings/2012/nime2012_262.pdf},
	doi = {10.5281/zenodo.1180535},
	booktitle = {Proceedings of the international conference on new interfaces for musical expression},
	publisher = {University of Michigan},
	author = {Melo, Jonh and Gómez, Daniel and Vargas, Miguel},
	year = {2012},
	note = {ISSN: 2220-4806},
	keywords = {Electroacoustic music, expanded instrument, gesture., saxophone},
	file = {Melo et al_2012_Gest-O.pdf:/Users/Vincent/Zotero/storage/4Y2X9MG4/Melo et al_2012_Gest-O.pdf:application/pdf},
}

@inproceedings{schiesser2006,
	address = {Paris, France},
	title = {On making and playing an electronically-augmented saxophone},
	url = {http://www.nime.org/proceedings/2006/nime2006_308.pdf},
	doi = {10.5281/zenodo.1177001},
	booktitle = {Proceedings of the international conference on new interfaces for musical expression},
	author = {Schiesser, Sébastien and Traube, Caroline},
	year = {2006},
	note = {ISSN: 2220-4806},
	keywords = {saxophone, augmented instrument, gestural control, live electronics, perfor- mance},
	pages = {308--313},
	file = {Schiesser_Traube_2006_On making and playing an electronically-augmented saxophone.pdf:/Users/Vincent/Zotero/storage/4Q9MW5L2/Schiesser_Traube_2006_On making and playing an electronically-augmented saxophone.pdf:application/pdf},
}

@techreport{senior2004,
	address = {St. John's, NL},
	title = {Qualisys {Track} {Manager}: {User} {Manual}},
	shorttitle = {Qualisys {Track} {Manager}},
	url = {https://nrc-publications.canada.ca/eng/view/object/?id=de61d2e8-121c-49f7-868e-2ec8ac7175ba},
	abstract = {QTM software is a Windows based application, which takes information provided by ProReflex cameras and generates 2D, 3D, and/or 6 degree of freedom (DOF) data. If 3D or 6DOF data is obtained it can be viewed through a 3D viewing window. At least two cameras must be used in order to obtain 3D or 6DOF data. ProReflex cameras emit IR flashes of light, which are reflected by retro-reflective markers back to the cameras. The cameras recognize markers and relay information to QTM where it is processed and output appropriately.This report is intended as a guide through linearization, calibration, and capturing measurement processes. Error analysis concerning linearization and calibration are discussed within the report, while the set-up procedures are included in appendices. Also, the report describes QTMDAC, which is a modified version of QualiDAC. QTMDAC communicates with QTM and generates an analog output.},
	institution = {National Research Council of Canada. Institute for Ocean Technology},
	author = {Senior, D.},
	year = {2004},
	doi = {10.4224/8896115},
	keywords = {Calibration, Linearization, QTM},
	file = {QTM-usermanual.pdf:/Users/Vincent/Zotero/storage/JPDNW6LG/QTM-usermanual.pdf:application/pdf},
}

@article{freire2020,
	title = {Evaluation of {Inertial} {Sensor} {Data} by a {Comparison} with {Optical} {Motion} {Capture} {Data} of {Guitar} {Strumming} {Gestures}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/19/5722},
	doi = {10.3390/s20195722},
	abstract = {Computing technologies have opened up a myriad of possibilities for expanding the sonic capabilities of acoustic musical instruments. Musicians nowadays employ a variety of rather inexpensive, wireless sensor-based systems to obtain refined control of interactive musical performances in actual musical situations like live music concerts. It is essential though to clearly understand the capabilities and limitations of such acquisition systems and their potential influence on high-level control of musical processes. In this study, we evaluate one such system composed of an inertial sensor (MetaMotionR) and a hexaphonic nylon guitar for capturing strumming gestures. To characterize this system, we compared it with a high-end commercial motion capture system (Qualisys) typically used in the controlled environments of research laboratories, in two complementary tasks: comparisons of rotational and translational data. For the rotations, we were able to compare our results with those that are found in the literature, obtaining RMSE below 10° for 88\% of the curves. The translations were compared in two ways: by double derivation of positional data from the mocap and by double integration of IMU acceleration data. For the task of estimating displacements from acceleration data, we developed a compensative-integration method to deal with the oscillatory character of the strumming, whose approximative results are very dependent on the type of gestures and segmentation; a value of 0.77 was obtained for the average of the normalized covariance coefficients of the displacement magnitudes. Although not in the ideal range, these results point to a clearly acceptable trade-off between the flexibility, portability and low cost of the proposed system when compared to the limited use and cost of the high-end motion capture standard in interactive music setups.},
	language = {en},
	number = {19},
	urldate = {2021-10-04},
	journal = {Sensors},
	author = {Freire, Sérgio and Santos, Geise and Armondes, Augusto and Meneses, Eduardo A. L. and Wanderley, Marcelo M.},
	month = oct,
	year = {2020},
	pages = {5722},
	file = {Freire et al_2020_Evaluation of Inertial Sensor Data by a Comparison with Optical Motion Capture.pdf:/Users/Vincent/Zotero/storage/NN6RZUHX/Freire et al_2020_Evaluation of Inertial Sensor Data by a Comparison with Optical Motion Capture.pdf:application/pdf},
}

@article{caramiaux2012,
	title = {Segmenting and {Parsing} {Instrumentalists}' {Gestures}},
	volume = {41},
	issn = {0929-8215, 1744-5027},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2011.643314},
	doi = {10.1080/09298215.2011.643314},
	language = {en},
	number = {1},
	urldate = {2021-10-04},
	journal = {Journal of New Music Research},
	author = {Caramiaux, Baptiste and Wanderley, Marcelo M. and Bevilacqua, Frédéric},
	month = mar,
	year = {2012},
	pages = {13--29},
	file = {Caramiaux et al_2012_Segmenting and Parsing Instrumentalists' Gestures.pdf:/Users/Vincent/Zotero/storage/HURZXL22/Caramiaux et al_2012_Segmenting and Parsing Instrumentalists' Gestures.pdf:application/pdf},
}

@inproceedings{visi2014a,
	address = {Paris, France},
	title = {Gesture in performance with traditional musical instruments and electronics: {Use} of embodied music cognition and multimodal motion capture to design gestural mapping strategies},
	isbn = {978-1-4503-2814-2},
	shorttitle = {Gesture in performance with traditional musical instruments and electronics},
	url = {http://dl.acm.org/citation.cfm?doid=2617995.2618013},
	doi = {10.1145/2617995.2618013},
	abstract = {This paper describes the implementation of gestural mapping strategies for performance with a traditional musical instrument and electronics. The approach adopted is informed by embodied music cognition and functional categories of musical gestures. Within this framework, gestures are not seen as means of control subordinated to the resulting musical sounds but rather as signiﬁcant elements contributing to the formation of musical meaning similarly to auditory features. Moreover, the ecological knowledge of the gestural repertoire of the instrument is taken into account as it deﬁnes the action-sound relationships between the instrument and the performer and contributes to form expectations in the listeners. Subsequently, mapping strategies from a case study of electric guitar performance will be illustrated describing what motivated the choice of a multimodal motion capture system and how diﬀerent solutions have been adopted considering both gestural meaning formation and technical constraints.},
	language = {en},
	urldate = {2021-10-08},
	booktitle = {Proceedings of the 2014 {International} {Workshop} on {Movement} and {Computing} - {MOCO} '14},
	publisher = {ACM Press},
	author = {Visi, Federico and Schramm, Rodrigo and Miranda, Eduardo},
	year = {2014},
	pages = {100--105},
	file = {Visi et al. - 2014 - Gesture in performance with traditional musical in.pdf:/Users/Vincent/Zotero/storage/98H5IUR5/Visi et al. - 2014 - Gesture in performance with traditional musical in.pdf:application/pdf},
}
